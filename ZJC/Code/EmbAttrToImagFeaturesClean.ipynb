{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import Model, Sequential\n",
    "from keras.applications import VGG16, VGG19\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "import keras\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "path = '../Data/'\n",
    "\n",
    "class_emb = pd.read_csv(path + '/DatasetA_train_20180813/class_wordembeddings.txt', \n",
    "                        index_col = 0, sep = ' ', header = None)\n",
    "class_emb.index.name = 'class_name'\n",
    "# class_emb_vec = pd.DataFrame(index = class_emb.index)\n",
    "class_emb = class_emb.apply(lambda s: np.array([float(x) for x in s]), axis = 1)\n",
    "\n",
    "class_id_to_name = pd.read_csv(path + '/DatasetA_train_20180813/label_list.txt', \n",
    "                               index_col = 'class_name', sep = '\\t', header = None, names = ['class_id', 'class_name'])\n",
    "\n",
    "attr_list = pd.read_csv(path + '/DatasetA_train_20180813/attribute_list.txt', index_col = 0, sep = '\\t', header = None)\n",
    "\n",
    "attributes_per_class = pd.read_csv(path + '/DatasetA_train_20180813/attributes_per_class.txt', \n",
    "                                   index_col = 0, sep = '\\t', header = None)\n",
    "attributes_per_class.index.name = 'class_id'\n",
    "attributes_per_class = attributes_per_class.apply(lambda s: np.array([float(x) for x in s]), axis = 1)\n",
    "\n",
    "train_data = pd.read_csv(path + '/DatasetA_train_20180813/train.txt', index_col = 'class_id', \n",
    "                         sep = '\\t', header = None, names = ['image_id', 'class_id'])\n",
    "\n",
    "class_id_emb_attr = class_id_to_name.copy()\n",
    "class_id_emb_attr['emb'] = class_emb\n",
    "class_id_emb_attr.reset_index(inplace = True)\n",
    "class_id_emb_attr.set_index('class_id', inplace = True)\n",
    "class_id_emb_attr['attr'] = attributes_per_class\n",
    "train_data = train_data.merge(class_id_emb, how = 'left', on = 'class_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "imag_path = path + r'/DatasetA_train_20180813/train/'\n",
    "def read_image(image_id):\n",
    "    img = image.load_img(imag_path + image_id)\n",
    "    img= image.img_to_array(img) / 255\n",
    "    return img\n",
    "train_data['img'] = train_data['image_id'].apply(lambda id: read_image(id))\n",
    "\n",
    "train_data.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4ccc430550>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# np.asarray(list(train_data['img'])).shape\n",
    "# train_data[train_data['class_id'] == 'ZJL1']\n",
    "train_data['class_id'].value_counts().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "from keras import Model, Sequential\n",
    "from keras.applications import VGG16, VGG19\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "\n",
    "vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(64, 64, 3))\n",
    " \n",
    "# classes = {\n",
    "#     'cla' : 6+1,\n",
    "#   #  'clo' : 8, #暂且不用\n",
    "#     'has' : 4,\n",
    "#     'for' : 6\n",
    "#   #  'is' : 6 #暂且不用\n",
    "# }\n",
    " \n",
    "# x = base_model.output\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# # predictions = [Dense(n, activation='softmax', name=m)(x) for m,n in classes.items()]\n",
    "# predictions = [Dense(1, activation='sigmoid', name=m.replace(' ', '_'))(x) for m in head[1:]]\n",
    " \n",
    "# model = Model(inputs=base_model.input, outputs= predictions)\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "# model.summary()\n",
    "\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         31744       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1324)         0           input_6[0][0]                    \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2048)         2713600     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 2048)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1024)         2098176     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1024)         0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 512)          524800      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 512)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 2048)         1050624     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,418,944\n",
      "Trainable params: 6,418,944\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class RmseEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1, batch_interval = 1000000, verbose = 2, \\\n",
    "            scores = []):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        # print(\"y_val shape:{0}\".format(self.y_val.shape))\n",
    "        self.batch_interval = batch_interval\n",
    "        self.verbose = verbose\n",
    "        self.scores = scores\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0, batch_size=10240)\n",
    "            score = np.sqrt(metrics.mean_squared_error(self.y_val, y_pred))\n",
    "            self.scores.append(\"epoch:{0} {1}\".format(epoch + 1, score))\n",
    "            print(\"\\n RMSE - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "        if(self.verbose >= 2) and (batch % self.batch_interval == 0):\n",
    "            # y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            # loss = metrics.log_loss(self.y_val, y_pred)\n",
    "            print(\"Hi! on_batch_end() , batch=\",batch,\",logs:\",logs)\n",
    "            # print(\"Valide size=\",y_pred.shape[0], \"  Valide loss=\",loss)\n",
    "\n",
    "attr_input = Input(shape = (30,))\n",
    "word_emb = Input(shape = (300,))\n",
    "imag_classifier = Input(shape = (1024,))\n",
    "\n",
    "# attr_dense = Dense(2048, activation=\"relu\")(attr_input)\n",
    "# attr_dense = Dense(2048, activation=\"relu\")(attr_input)\n",
    "attr_dense = Dense(1024, activation=\"relu\")(attr_input)\n",
    "# attr_dense = Dense(512, activation=\"relu\")(attr_dense)\n",
    "\n",
    "attr_word_emb = Concatenate()([word_emb, attr_dense])\n",
    "\n",
    "attr_word_emb_dense = Dense(2048, activation=\"relu\")(attr_word_emb)\n",
    "attr_word_emb_dense = Dropout(0.2)(attr_word_emb_dense)\n",
    "# attr_word_emb_dense = Dense(2048, activation=\"relu\")(attr_word_emb_dense)\n",
    "attr_word_emb_dense = Dense(1024, activation=\"relu\")(attr_word_emb_dense)\n",
    "attr_word_emb_dense = Dropout(0.2)(attr_word_emb_dense)\n",
    "attr_word_emb_dense = Dense(512, activation=\"relu\")(attr_word_emb_dense)\n",
    "attr_word_emb_dense = Dropout(0.2)(attr_word_emb_dense)\n",
    "# attr_word_emb_dense = Dense(2048, activation=\"sigmoid\", kernel_regularizer=keras.regularizers.l2(0.01))(attr_word_emb_dense)\n",
    "attr_word_emb_dense = Dense(2048, activation=\"relu\")(attr_word_emb_dense)\n",
    "\n",
    "# mse = Dense(6, activation=\"softmax\")(dense_output)\n",
    "vgg_input = vgg_model.input\n",
    "vgg_output = Flatten()(vgg_model.output)\n",
    "vgg_output = Dense(1024, activation=\"sigmoid\")(vgg_output)\n",
    "\n",
    "# mse_loss = keras.losses.mean_squared_error(vgg_output, attr_word_emb_dense)\n",
    "\n",
    "model = Model([attr_input, word_emb], outputs = attr_word_emb_dense) #, vgg_output])\n",
    "# model.add_loss(mse_loss)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length:  38221\n",
      "38221/38221 [==============================] - 2415s 63ms/step\n",
      "Train on 30576 samples, validate on 7645 samples\n",
      "Epoch 1/100\n",
      "30576/30576 [==============================] - 18s 584us/step - loss: 0.0876 - val_loss: 0.0990\n",
      "Epoch 2/100\n",
      "30576/30576 [==============================] - 18s 575us/step - loss: 0.0836 - val_loss: 0.0963\n",
      "Epoch 3/100\n",
      "30576/30576 [==============================] - 17s 570us/step - loss: 0.0821 - val_loss: 0.0933\n",
      "Epoch 4/100\n",
      "30576/30576 [==============================] - 17s 569us/step - loss: 0.0811 - val_loss: 0.0917\n",
      "Epoch 5/100\n",
      "30576/30576 [==============================] - 22s 711us/step - loss: 0.0804 - val_loss: 0.0917\n",
      "Epoch 6/100\n",
      "30576/30576 [==============================] - 19s 606us/step - loss: 0.0801 - val_loss: 0.0915\n",
      "Epoch 7/100\n",
      "30576/30576 [==============================] - 18s 588us/step - loss: 0.0798 - val_loss: 0.0924\n",
      "Epoch 8/100\n",
      "30576/30576 [==============================] - 18s 582us/step - loss: 0.0795 - val_loss: 0.0929\n",
      "Epoch 9/100\n",
      "30576/30576 [==============================] - 18s 578us/step - loss: 0.0794 - val_loss: 0.0940\n",
      "Epoch 10/100\n",
      "30576/30576 [==============================] - 18s 579us/step - loss: 0.0792 - val_loss: 0.0947\n",
      "Epoch 11/100\n",
      "30576/30576 [==============================] - 18s 574us/step - loss: 0.0790 - val_loss: 0.0956\n",
      "Epoch 12/100\n",
      "30576/30576 [==============================] - 18s 589us/step - loss: 0.0789 - val_loss: 0.0970\n",
      "Epoch 13/100\n",
      "30576/30576 [==============================] - 18s 603us/step - loss: 0.0788 - val_loss: 0.0963\n",
      "Epoch 14/100\n",
      "30576/30576 [==============================] - 18s 588us/step - loss: 0.0787 - val_loss: 0.0977\n",
      "Epoch 15/100\n",
      "30576/30576 [==============================] - 18s 579us/step - loss: 0.0786 - val_loss: 0.0972\n",
      "Epoch 16/100\n",
      "30576/30576 [==============================] - 18s 578us/step - loss: 0.0784 - val_loss: 0.0981\n",
      "Epoch 17/100\n",
      "30576/30576 [==============================] - 18s 583us/step - loss: 0.0783 - val_loss: 0.1000\n",
      "Epoch 18/100\n",
      "30576/30576 [==============================] - 18s 587us/step - loss: 0.0782 - val_loss: 0.0997\n",
      "Epoch 19/100\n",
      "30576/30576 [==============================] - 18s 580us/step - loss: 0.0780 - val_loss: 0.0976\n",
      "Epoch 20/100\n",
      "30576/30576 [==============================] - 18s 577us/step - loss: 0.0780 - val_loss: 0.0989\n",
      "Epoch 21/100\n",
      "30576/30576 [==============================] - 18s 580us/step - loss: 0.0779 - val_loss: 0.0995\n",
      "Epoch 22/100\n",
      "30576/30576 [==============================] - 18s 582us/step - loss: 0.0778 - val_loss: 0.1019\n",
      "Epoch 23/100\n",
      "30576/30576 [==============================] - 18s 579us/step - loss: 0.0777 - val_loss: 0.1011\n",
      "Epoch 24/100\n",
      "30576/30576 [==============================] - 18s 581us/step - loss: 0.0776 - val_loss: 0.0991\n",
      "Epoch 25/100\n",
      "30576/30576 [==============================] - 18s 583us/step - loss: 0.0776 - val_loss: 0.0988\n",
      "Epoch 26/100\n",
      "30576/30576 [==============================] - 18s 583us/step - loss: 0.0775 - val_loss: 0.0994\n",
      "Epoch 27/100\n",
      "30576/30576 [==============================] - 18s 583us/step - loss: 0.0774 - val_loss: 0.0999\n",
      "Epoch 28/100\n",
      "30576/30576 [==============================] - 18s 583us/step - loss: 0.0773 - val_loss: 0.0987\n",
      "Epoch 29/100\n",
      "30576/30576 [==============================] - 18s 581us/step - loss: 0.0773 - val_loss: 0.0979\n",
      "Epoch 30/100\n",
      "30576/30576 [==============================] - 18s 584us/step - loss: 0.0773 - val_loss: 0.0987\n",
      "Epoch 31/100\n",
      "30576/30576 [==============================] - 18s 580us/step - loss: 0.0772 - val_loss: 0.0995\n",
      "Epoch 32/100\n",
      "30576/30576 [==============================] - 18s 579us/step - loss: 0.0771 - val_loss: 0.0975\n",
      "Epoch 33/100\n",
      "30576/30576 [==============================] - 18s 584us/step - loss: 0.0771 - val_loss: 0.0983\n",
      "Epoch 34/100\n",
      "30576/30576 [==============================] - 18s 584us/step - loss: 0.0770 - val_loss: 0.0975\n",
      "Epoch 35/100\n",
      "30576/30576 [==============================] - 18s 584us/step - loss: 0.0769 - val_loss: 0.0979\n",
      "Epoch 36/100\n",
      "30576/30576 [==============================] - 18s 585us/step - loss: 0.0768 - val_loss: 0.0961\n",
      "Epoch 37/100\n",
      "30576/30576 [==============================] - 18s 586us/step - loss: 0.0767 - val_loss: 0.0971\n",
      "Epoch 38/100\n",
      "30576/30576 [==============================] - 18s 587us/step - loss: 0.0767 - val_loss: 0.0965\n",
      "Epoch 39/100\n",
      "30576/30576 [==============================] - 18s 585us/step - loss: 0.0766 - val_loss: 0.0941\n",
      "Epoch 40/100\n",
      "30576/30576 [==============================] - 18s 581us/step - loss: 0.0765 - val_loss: 0.0960\n",
      "Epoch 41/100\n",
      "30576/30576 [==============================] - 18s 587us/step - loss: 0.0765 - val_loss: 0.0960\n",
      "Epoch 42/100\n",
      "30576/30576 [==============================] - 18s 584us/step - loss: 0.0764 - val_loss: 0.0964\n",
      "Epoch 43/100\n",
      "30576/30576 [==============================] - 18s 602us/step - loss: 0.0763 - val_loss: 0.0953\n",
      "Epoch 44/100\n",
      "30576/30576 [==============================] - 18s 599us/step - loss: 0.0762 - val_loss: 0.0957\n",
      "Epoch 45/100\n",
      "30576/30576 [==============================] - 18s 583us/step - loss: 0.0762 - val_loss: 0.0969\n",
      "Epoch 46/100\n",
      "30576/30576 [==============================] - 18s 583us/step - loss: 0.0761 - val_loss: 0.0958\n",
      "Epoch 47/100\n",
      "30576/30576 [==============================] - 18s 601us/step - loss: 0.0761 - val_loss: 0.0962\n",
      "Epoch 48/100\n",
      "30576/30576 [==============================] - 18s 591us/step - loss: 0.0760 - val_loss: 0.0965\n",
      "Epoch 49/100\n",
      "30576/30576 [==============================] - 17s 571us/step - loss: 0.0759 - val_loss: 0.0956\n",
      "Epoch 50/100\n",
      "30576/30576 [==============================] - 18s 580us/step - loss: 0.0758 - val_loss: 0.0953\n",
      "Epoch 51/100\n",
      "30576/30576 [==============================] - 18s 590us/step - loss: 0.0757 - val_loss: 0.0956\n",
      "Epoch 52/100\n",
      "30576/30576 [==============================] - 17s 567us/step - loss: 0.0756 - val_loss: 0.0958\n",
      "Epoch 53/100\n",
      "30576/30576 [==============================] - 18s 573us/step - loss: 0.0756 - val_loss: 0.0964\n",
      "Epoch 54/100\n",
      "30576/30576 [==============================] - 18s 573us/step - loss: 0.0756 - val_loss: 0.0957\n",
      "Epoch 55/100\n",
      "30576/30576 [==============================] - 18s 573us/step - loss: 0.0754 - val_loss: 0.0945\n",
      "Epoch 56/100\n",
      "30576/30576 [==============================] - 18s 589us/step - loss: 0.0754 - val_loss: 0.0957\n",
      "Epoch 57/100\n",
      "30576/30576 [==============================] - 18s 600us/step - loss: 0.0755 - val_loss: 0.0953\n",
      "Epoch 58/100\n",
      "30576/30576 [==============================] - 18s 578us/step - loss: 0.0755 - val_loss: 0.0969\n",
      "Epoch 59/100\n",
      "30576/30576 [==============================] - 18s 577us/step - loss: 0.0755 - val_loss: 0.0955\n",
      "Epoch 60/100\n",
      "30576/30576 [==============================] - 18s 578us/step - loss: 0.0754 - val_loss: 0.0954\n",
      "Epoch 61/100\n",
      "30576/30576 [==============================] - 18s 573us/step - loss: 0.0753 - val_loss: 0.0954\n",
      "Epoch 62/100\n",
      "30576/30576 [==============================] - 18s 585us/step - loss: 0.0753 - val_loss: 0.0952\n",
      "Epoch 63/100\n",
      "30576/30576 [==============================] - 18s 578us/step - loss: 0.0752 - val_loss: 0.0953\n",
      "Epoch 64/100\n",
      "30576/30576 [==============================] - 18s 581us/step - loss: 0.0750 - val_loss: 0.0953\n",
      "Epoch 65/100\n",
      "30576/30576 [==============================] - 18s 604us/step - loss: 0.0749 - val_loss: 0.0958\n",
      "Epoch 66/100\n",
      "30576/30576 [==============================] - 19s 609us/step - loss: 0.0749 - val_loss: 0.0951\n",
      "Epoch 67/100\n",
      "30576/30576 [==============================] - 18s 583us/step - loss: 0.0749 - val_loss: 0.0941\n",
      "Epoch 68/100\n",
      "30576/30576 [==============================] - 17s 568us/step - loss: 0.0749 - val_loss: 0.0940\n",
      "Epoch 69/100\n",
      "30576/30576 [==============================] - 17s 570us/step - loss: 0.0750 - val_loss: 0.0943\n",
      "Epoch 70/100\n",
      "30576/30576 [==============================] - 17s 568us/step - loss: 0.0750 - val_loss: 0.0943\n",
      "Epoch 71/100\n",
      "30576/30576 [==============================] - 17s 570us/step - loss: 0.0751 - val_loss: 0.0944\n",
      "Epoch 72/100\n",
      "30576/30576 [==============================] - 17s 568us/step - loss: 0.0750 - val_loss: 0.0951\n",
      "Epoch 73/100\n",
      "30576/30576 [==============================] - 17s 571us/step - loss: 0.0750 - val_loss: 0.0960\n",
      "Epoch 74/100\n",
      "30576/30576 [==============================] - 17s 570us/step - loss: 0.0750 - val_loss: 0.0947\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30576/30576 [==============================] - 17s 570us/step - loss: 0.0751 - val_loss: 0.0944\n",
      "Epoch 76/100\n",
      "30576/30576 [==============================] - 17s 570us/step - loss: 0.0751 - val_loss: 0.0940\n",
      "Epoch 77/100\n",
      "30576/30576 [==============================] - 17s 570us/step - loss: 0.0750 - val_loss: 0.0949\n",
      "Epoch 78/100\n",
      "30576/30576 [==============================] - 17s 571us/step - loss: 0.0750 - val_loss: 0.0953\n",
      "Epoch 79/100\n",
      "30576/30576 [==============================] - 18s 578us/step - loss: 0.0750 - val_loss: 0.0942\n",
      "Epoch 80/100\n",
      "30576/30576 [==============================] - 18s 573us/step - loss: 0.0749 - val_loss: 0.0945\n",
      "Epoch 81/100\n",
      "30576/30576 [==============================] - 18s 578us/step - loss: 0.0749 - val_loss: 0.0943\n",
      "Epoch 82/100\n",
      "30576/30576 [==============================] - 18s 574us/step - loss: 0.0749 - val_loss: 0.0939\n",
      "Epoch 83/100\n",
      "30576/30576 [==============================] - 18s 590us/step - loss: 0.0749 - val_loss: 0.0953\n",
      "Epoch 84/100\n",
      "30576/30576 [==============================] - 18s 575us/step - loss: 0.0748 - val_loss: 0.0931\n",
      "Epoch 85/100\n",
      "30576/30576 [==============================] - 17s 569us/step - loss: 0.0748 - val_loss: 0.0947\n",
      "Epoch 86/100\n",
      "30576/30576 [==============================] - 17s 568us/step - loss: 0.0748 - val_loss: 0.0952\n",
      "Epoch 87/100\n",
      "30576/30576 [==============================] - 17s 568us/step - loss: 0.0748 - val_loss: 0.0952\n",
      "Epoch 88/100\n",
      "30576/30576 [==============================] - 17s 567us/step - loss: 0.0747 - val_loss: 0.0939\n",
      "Epoch 89/100\n",
      "30576/30576 [==============================] - 17s 569us/step - loss: 0.0748 - val_loss: 0.0955\n",
      "Epoch 90/100\n",
      "30576/30576 [==============================] - 17s 570us/step - loss: 0.0748 - val_loss: 0.0949\n",
      "Epoch 91/100\n",
      "30576/30576 [==============================] - 18s 582us/step - loss: 0.0748 - val_loss: 0.0950\n",
      "Epoch 92/100\n",
      "30576/30576 [==============================] - 18s 578us/step - loss: 0.0748 - val_loss: 0.0943\n",
      "Epoch 93/100\n",
      "30576/30576 [==============================] - 18s 577us/step - loss: 0.0749 - val_loss: 0.0955\n",
      "Epoch 94/100\n",
      "30576/30576 [==============================] - 18s 573us/step - loss: 0.0749 - val_loss: 0.0949\n",
      "Epoch 95/100\n",
      "30576/30576 [==============================] - 17s 567us/step - loss: 0.0750 - val_loss: 0.0945\n",
      "Epoch 96/100\n",
      "30576/30576 [==============================] - 18s 575us/step - loss: 0.0750 - val_loss: 0.0943\n",
      "Epoch 97/100\n",
      "30576/30576 [==============================] - 17s 570us/step - loss: 0.0750 - val_loss: 0.0948\n",
      "Epoch 98/100\n",
      "30576/30576 [==============================] - 18s 574us/step - loss: 0.0750 - val_loss: 0.0947\n",
      "Epoch 99/100\n",
      "30576/30576 [==============================] - 17s 572us/step - loss: 0.0751 - val_loss: 0.0937\n",
      "Epoch 100/100\n",
      "30576/30576 [==============================] - 17s 570us/step - loss: 0.0750 - val_loss: 0.0932\n"
     ]
    }
   ],
   "source": [
    "train_data_part = train_data #[:1000]\n",
    "train_data_part_length = train_data_part.shape[0]\n",
    "print ('Train length: ', train_data_part.shape[0])\n",
    "\n",
    "train_data_part_img = np.asarray(list(train_data_part['img']))\n",
    "train_y = np.reshape(vgg_model.predict(train_data_part_img, verbose=  1), (train_data_part_length, -1)) #data.loc[class_id] #data_atten.loc[n]\n",
    "    \n",
    "h = model.fit([np.asarray(list(train_data_part['attr'])), np.asarray(list(train_data_part['emb']))], train_y, \n",
    "              epochs=100,validation_split=0.2, batch_size = 64, shuffle=True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(path + 'train_vgg16_img_div255.pickle', 'wb+') as handle:\n",
    "    pickle.dump(train_y, handle)\n",
    "# with open(path + 'train_vgg16.pickle', 'rb') as handle:\n",
    "#     train_y = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "data_test = open(\"D:\\\\data\\\\pic\\\\DatasetA_test_20180813\\\\DatasetA_test\\\\image.txt\")\n",
    "data_test = data_test.readlines()\n",
    "path_test = r'D:\\data\\pic\\DatasetA_test_20180813\\DatasetA_test\\test\\\\'\n",
    "length = len(data_test)\n",
    "data_test_x = np.zeros((length, 64, 64, 3))\n",
    "for i in range(length):\n",
    "    m = data_test[i][:-1]\n",
    "    img = image.load_img(path_test + m)\n",
    "    data_test_x[i] = image.img_to_array(img)\n",
    " \n",
    "res = model.predict(data_test_x)\n",
    " \n",
    "#欧氏距离：\n",
    "data_atten_res = []\n",
    "for i in range(160,241):\n",
    "    key = 'ZJL' + str(i)\n",
    "    data_atten_res.append(np.hstack((data_atten.loc[key][1:8], data_atten.loc[key][16:20], data_atten.loc[key][20:26])))\n",
    " \n",
    "vec_result = []#np.zores((len(res[0]), 18))\n",
    "for i in range(len(res[0])):\n",
    "    vec_result.append(np.hstack((res[0][i], res[1][i], res[2][i])))\n",
    "dis_list = []\n",
    " \n",
    "for j in range(len(data_atten_res)):\n",
    "    vec1 = vec_result[0]\n",
    "    vec2 = data_atten_res[j]\n",
    "    dist = np.linalg.norm(vec1 - vec2)\n",
    "    dis_list.append(dist)\n",
    "print(\"witch is : \"+ str(dis_list.index(min(dis_list))) + \" and min dis = \" + str(min(dis_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
