{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications import resnet50, vgg16\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from keras.regularizers import l1, l2\n",
    "# from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "path = '../Data/'\n",
    "\n",
    "# class_emb = pd.read_csv(path + '/DatasetA_train_20180813/class_wordembeddings.txt', \n",
    "#                         index_col = 0, sep = ' ', header = None)\n",
    "# class_emb.index.name = 'class_name'\n",
    "# # class_emb_vec = pd.DataFrame(index = class_emb.index)\n",
    "# class_emb = class_emb.apply(lambda s: np.array([float(x) for x in s]), axis = 1)\n",
    "\n",
    "# class_id_to_name = pd.read_csv(path + '/DatasetA_train_20180813/label_list.txt', \n",
    "#                                index_col = 'class_name', sep = '\\t', header = None, names = ['class_id', 'class_name'])\n",
    "\n",
    "# attr_list = pd.read_csv(path + '/DatasetA_train_20180813/attribute_list.txt', index_col = 0, sep = '\\t', header = None)\n",
    "\n",
    "# attributes_per_class = pd.read_csv(path + '/DatasetA_train_20180813/attributes_per_class.txt', \n",
    "#                                    index_col = 0, sep = '\\t', header = None)\n",
    "# attributes_per_class.index.name = 'class_id'\n",
    "# attributes_per_class = attributes_per_class.apply(lambda s: np.array([float(x) for x in s]), axis = 1)\n",
    "\n",
    "# class_id_emb_attr = class_id_to_name.copy()\n",
    "# class_id_emb_attr['emb'] = class_emb\n",
    "# class_id_emb_attr.reset_index(inplace = True)\n",
    "# class_id_emb_attr.set_index('class_id', inplace = True)\n",
    "# class_id_emb_attr['attr'] = attributes_per_class\n",
    "\n",
    "# with open(path + 'class_id_emb_attr.pickle', 'wb+') as handle:\n",
    "#     pickle.dump(class_id_emb_attr, handle)\n",
    "def read_image(imag_path, image_id):\n",
    "    img = image.load_img(imag_path + image_id)\n",
    "    img = image.img_to_array(img)\n",
    "#     img= vgg16.preprocess_input(image.img_to_array(img))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attr_list = pd.read_csv(path + '/DatasetA_train_20180813/attribute_list.txt', index_col = 0, sep = '\\t', header = None)\n",
    "# attr_list = attr_list.apply(lambda s: s[1].replace(' ', '_'), axis = 1)\n",
    "# attributes_per_class = pd.read_csv(path + '/DatasetA_train_20180813/attributes_per_class.txt', \n",
    "#                                    index_col = 0, sep = '\\t', header = None)\n",
    "# adj_graph\n",
    "# attr = np.array(list(class_id_emb_attr['attr']))\n",
    "# adj_graph = sklearn.metrics.pairwise.pairwise_distances(attr, metric = 'l2')\n",
    "# np.argsort(adj_graph[0])\n",
    "# adj_graph\n",
    "# class_id_emb_attr['attr'].values\n",
    "with open(path + '../zero-shot-gcn/data/imagenet_graph.pkl', 'rb') as handle:\n",
    "    imagenet_graph = pickle.load(handle)\n",
    "with open(path + '../zero-shot-gcn/data/list/invdict_wordntext.json', 'r') as fp:\n",
    "    words = json.load(fp)\n",
    "class_names = class_id_emb_attr['class_name'].values\n",
    "class_name_to_id = dict([(c, i) for i, c in enumerate(class_names)])\n",
    "class_wns = [[]] * class_names.shape[0]\n",
    "class_neighbor_wns = [[]] * class_names.shape[0]\n",
    "words_array = np.asarray(words)\n",
    "for i, word in enumerate(words):\n",
    "    for w in word.split(', '):\n",
    "        if w in class_name_to_id:\n",
    "#             print (class_name_to_id[w], i)\n",
    "            if len(class_wns[class_name_to_id[w]]) == 0:\n",
    "                class_wns[class_name_to_id[w]] = []\n",
    "            class_wns[class_name_to_id[w]].append(i)\n",
    "#             print (class_wns)\n",
    "            if len(class_neighbor_wns[class_name_to_id[w]]) == 0:\n",
    "                class_neighbor_wns[class_name_to_id[w]] = []\n",
    "            class_neighbor_wns[class_name_to_id[w]].extend(imagenet_graph[i])\n",
    "#             print (word, words_array[imagenet_graph[i]])\n",
    "#             print (i, w)\n",
    "#             if w not in class_dict:\n",
    "#                 class_dict[w] = 0\n",
    "#             class_dict[w] += 1\n",
    "# print (class_wns)\n",
    "adj_graph = scipy.eye(class_names.shape[0])\n",
    "for i, neighbor_wns in enumerate(class_neighbor_wns):\n",
    "    for j, wns in enumerate(class_wns):\n",
    "        if i == j:\n",
    "            continue\n",
    "        for neighbor in neighbor_wns:\n",
    "            if neighbor in wns:\n",
    "                adj_graph[i, j] = 1\n",
    "                adj_graph[j, i] = 1\n",
    "#                 print(class_names[i], class_names[j])\n",
    "                break\n",
    "adj_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 1, 1, 1, 7, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1,\n",
       "       1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 3, 1, 1,\n",
       "       1, 1, 1, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1,\n",
       "       2, 1, 1, 2, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
       "       8, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(adj_graph == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv(path + '/DatasetA_train_20180813/train.txt', index_col = 'class_id', \n",
    "#                          sep = '\\t', header = None, names = ['image_id', 'class_id'])\n",
    "# train_data = train_data.merge(class_id_emb_attr, how = 'left', on = 'class_id')\n",
    "\n",
    "# imag_path = path + r'/DatasetA_train_20180813/train/'\n",
    "\n",
    "# train_data['img'] = train_data['image_id'].apply(lambda id: read_image(id))\n",
    "\n",
    "# train_data.reset_index(inplace = True)\n",
    "with open(path + 'class_id_emb_attr.pickle', 'rb') as handle:\n",
    "    class_id_emb_attr = pickle.load(handle)\n",
    "class_id_emb_attr.reset_index(inplace = True)    \n",
    "# with open(path + '/train_img.pickle', 'rb') as handle:\n",
    "with open(path + '/model_sub/6_12_24_16_ini64_growth16_02962/train_data_2018_09_13_13_42_21.pickle', 'rb') as handle:\n",
    "    train_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + '/model_sub/6_12_24_16_ini64_growth16_02962/train_data_2018_09_13_13_42_21.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_data, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text =  pd.read_csv(path + '/FoundInFastText', \n",
    "                        index_col = 0, sep = ' ', header = None)\n",
    "fast_text.index.name = 'class_name'\n",
    "fast_text_df = pd.DataFrame(index = fast_text.index)\n",
    "fast_text_df['emb'] = fast_text.apply(lambda s: np.array([float(x) for x in s])[:300], axis = 1)\n",
    "\n",
    "train_data = train_data.merge(fast_text_df, how = 'left', on = 'class_name')\n",
    "\n",
    "train_data['emb'] = train_data.apply(lambda s: np.hstack([s['emb_x'], s['emb_y']]), axis = 1)\n",
    "\n",
    "class_id_emb_attr = class_id_emb_attr.merge(fast_text_df, how = 'left', on = 'class_name')\n",
    "class_id_emb_attr['emb'] = class_id_emb_attr.apply(lambda s: np.hstack([s['emb_x'], s['emb_y']]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_name</th>\n",
       "      <th>emb_x</th>\n",
       "      <th>attr</th>\n",
       "      <th>img</th>\n",
       "      <th>target</th>\n",
       "      <th>preds</th>\n",
       "      <th>emb_y</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>a6394b0f513290f4651cc46792e5ac86.jpeg</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>[-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...</td>\n",
       "      <td>[[[20.0, 33.0, 7.0], [19.0, 32.0, 6.0], [22.0,...</td>\n",
       "      <td>[0.31291956, 0.06956485, 0.34872296, 0.0408261...</td>\n",
       "      <td>[0.00025839225, 1.3706162e-09, 4.1264666e-06, ...</td>\n",
       "      <td>[0.10102, 0.1569, -0.56942, 0.2555300000000000...</td>\n",
       "      <td>[0.10102, 0.1569, -0.56942, 0.2555300000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>2fb89ef2ace869d3eb3bdd3afe184e1c.jpeg</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>[-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...</td>\n",
       "      <td>[[[140.0, 45.0, 51.0], [144.0, 47.0, 54.0], [1...</td>\n",
       "      <td>[0.41405687, 0.0, 0.427736, 0.26566926, 0.0959...</td>\n",
       "      <td>[8.5285705e-05, 7.096223e-13, 2.6975644e-07, 0...</td>\n",
       "      <td>[0.10102, 0.1569, -0.56942, 0.2555300000000000...</td>\n",
       "      <td>[0.10102, 0.1569, -0.56942, 0.2555300000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>eda9f3bef2bd8da038f6acbc8355fc25.jpeg</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>[-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...</td>\n",
       "      <td>[[[81.0, 69.0, 21.0], [86.0, 74.0, 26.0], [85....</td>\n",
       "      <td>[0.47680357, 0.040604617, 0.39211324, 0.050536...</td>\n",
       "      <td>[4.0051538e-07, 2.7406824e-10, 0.0068722973, 1...</td>\n",
       "      <td>[0.10102, 0.1569, -0.56942, 0.2555300000000000...</td>\n",
       "      <td>[0.10102, 0.1569, -0.56942, 0.2555300000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>7d93ef45972154aae150b4f9980a79c0.jpeg</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>[-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...</td>\n",
       "      <td>[[[16.0, 14.0, 15.0], [18.0, 16.0, 17.0], [19....</td>\n",
       "      <td>[0.43787622, 0.2263094, 0.91177565, 0.37130383...</td>\n",
       "      <td>[4.4784274e-06, 6.633557e-15, 4.5222623e-07, 5...</td>\n",
       "      <td>[0.10102, 0.1569, -0.56942, 0.2555300000000000...</td>\n",
       "      <td>[0.10102, 0.1569, -0.56942, 0.2555300000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>fb901b4f9a8e396c1d0155bccc5e5671.jpeg</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>[-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...</td>\n",
       "      <td>[[[120.0, 124.0, 127.0], [89.0, 93.0, 96.0], [...</td>\n",
       "      <td>[0.09672487, 0.020393904, 0.0, 0.063967496, 0....</td>\n",
       "      <td>[1.1269757e-05, 1.5967199e-12, 0.00011251625, ...</td>\n",
       "      <td>[0.10102, 0.1569, -0.56942, 0.2555300000000000...</td>\n",
       "      <td>[0.10102, 0.1569, -0.56942, 0.2555300000000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_id                               image_id class_name  \\\n",
       "0     ZJL1  a6394b0f513290f4651cc46792e5ac86.jpeg   goldfish   \n",
       "1     ZJL1  2fb89ef2ace869d3eb3bdd3afe184e1c.jpeg   goldfish   \n",
       "2     ZJL1  eda9f3bef2bd8da038f6acbc8355fc25.jpeg   goldfish   \n",
       "3     ZJL1  7d93ef45972154aae150b4f9980a79c0.jpeg   goldfish   \n",
       "4     ZJL1  fb901b4f9a8e396c1d0155bccc5e5671.jpeg   goldfish   \n",
       "\n",
       "                                               emb_x  \\\n",
       "0  [-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...   \n",
       "1  [-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...   \n",
       "2  [-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...   \n",
       "3  [-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...   \n",
       "4  [-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...   \n",
       "\n",
       "                                                attr  \\\n",
       "0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...   \n",
       "1  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...   \n",
       "2  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...   \n",
       "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...   \n",
       "4  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...   \n",
       "\n",
       "                                                 img  \\\n",
       "0  [[[20.0, 33.0, 7.0], [19.0, 32.0, 6.0], [22.0,...   \n",
       "1  [[[140.0, 45.0, 51.0], [144.0, 47.0, 54.0], [1...   \n",
       "2  [[[81.0, 69.0, 21.0], [86.0, 74.0, 26.0], [85....   \n",
       "3  [[[16.0, 14.0, 15.0], [18.0, 16.0, 17.0], [19....   \n",
       "4  [[[120.0, 124.0, 127.0], [89.0, 93.0, 96.0], [...   \n",
       "\n",
       "                                              target  \\\n",
       "0  [0.31291956, 0.06956485, 0.34872296, 0.0408261...   \n",
       "1  [0.41405687, 0.0, 0.427736, 0.26566926, 0.0959...   \n",
       "2  [0.47680357, 0.040604617, 0.39211324, 0.050536...   \n",
       "3  [0.43787622, 0.2263094, 0.91177565, 0.37130383...   \n",
       "4  [0.09672487, 0.020393904, 0.0, 0.063967496, 0....   \n",
       "\n",
       "                                               preds  \\\n",
       "0  [0.00025839225, 1.3706162e-09, 4.1264666e-06, ...   \n",
       "1  [8.5285705e-05, 7.096223e-13, 2.6975644e-07, 0...   \n",
       "2  [4.0051538e-07, 2.7406824e-10, 0.0068722973, 1...   \n",
       "3  [4.4784274e-06, 6.633557e-15, 4.5222623e-07, 5...   \n",
       "4  [1.1269757e-05, 1.5967199e-12, 0.00011251625, ...   \n",
       "\n",
       "                                               emb_y  \\\n",
       "0  [0.10102, 0.1569, -0.56942, 0.2555300000000000...   \n",
       "1  [0.10102, 0.1569, -0.56942, 0.2555300000000000...   \n",
       "2  [0.10102, 0.1569, -0.56942, 0.2555300000000000...   \n",
       "3  [0.10102, 0.1569, -0.56942, 0.2555300000000000...   \n",
       "4  [0.10102, 0.1569, -0.56942, 0.2555300000000000...   \n",
       "\n",
       "                                                 emb  \n",
       "0  [0.10102, 0.1569, -0.56942, 0.2555300000000000...  \n",
       "1  [0.10102, 0.1569, -0.56942, 0.2555300000000000...  \n",
       "2  [0.10102, 0.1569, -0.56942, 0.2555300000000000...  \n",
       "3  [0.10102, 0.1569, -0.56942, 0.2555300000000000...  \n",
       "4  [0.10102, 0.1569, -0.56942, 0.2555300000000000...  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['emb'] = train_data['emb_y']\n",
    "class_id_emb_attr['emb'] = class_id_emb_attr['emb_y']\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = train_data['class_id'].unique()\n",
    "category_dict = dict((category[i], i) for i in range(category.shape[0]))\n",
    "\n",
    "# vgg_model = vgg16.VGG16(include_top=True, weights = None, input_shape=(64, 64, 3), classes = train_data['class_id'].unique().shape[0])\n",
    "\n",
    "\n",
    "# with open(path + 'train_vgg16_img_div255.pickle', 'rb') as handle:\n",
    "#     train_y = pickle.load(handle)\n",
    "\n",
    "# input = Input(shape = (2048, ))\n",
    "# dense = Dense(2048, activation=\"relu\")(input)\n",
    "# dense = Dropout(0.5)(dense)\n",
    "# dense = Dense(2048, activation=\"relu\")(input)\n",
    "# dense = Dense(1024, activation=\"relu\")(input)\n",
    "# dense = Dense(category.shape[0], activation=\"softmax\")(input)\n",
    "\n",
    "# vgg_model = Model(input, dense)\n",
    "\n",
    "# vgg_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
    "\n",
    "# print (vgg_model.summary())\n",
    "\n",
    "# train_image_feature_map = np.reshape(vgg_model.predict(train_data_part_img, verbose=  1), (train_data_part_length, -1))\n",
    "# with open(path + 'train_vgg16_img_div255.pickle', 'wb+') as handle:\n",
    "#     pickle.dump(train_y, handle)\n",
    "# with open(path + 'train_vgg16_img_div255.pickle', 'rb') as handle:\n",
    "#     train_image_feature_map = pickle.load(handle)\n",
    "# train_data['target'] = list(train_image_feature_map)\n",
    "\n",
    "def extract_array_from_series(s):\n",
    "    return np.asarray(list(s))\n",
    "\n",
    "class DenseNet:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, cat_max, blocks, weight_decay, kernel_initializer, reduction, init_filters, growth_rate):\n",
    "        self.cat_max = cat_max\n",
    "        self.blocks = blocks\n",
    "        self.weight_decay = weight_decay\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.model = self.small_densenet(\n",
    "                classes = self.cat_max,\n",
    "                blocks = self.blocks, \n",
    "                weight_decay = self.weight_decay, \n",
    "                kernel_initializer = self.kernel_initializer,\n",
    "                init_filters = init_filters,\n",
    "                reduction = reduction,\n",
    "                growth_rate = growth_rate)\n",
    "\n",
    "    def dense_block(self, x, blocks, name, weight_decay = 1e-4, kernel_initializer = 'he_normal', growth_rate = None):\n",
    "        \"\"\"A dense block.\n",
    "        # Arguments\n",
    "            x: input tensor.\n",
    "            blocks: integer, the number of building blocks.\n",
    "            name: string, block label.\n",
    "        # Returns\n",
    "            output tensor for the block.\n",
    "        \"\"\"\n",
    "        for i in range(blocks):\n",
    "            x = self.conv_block(x, growth_rate, name=name + '_block' + str(i + 1), \n",
    "                weight_decay = weight_decay,\n",
    "                kernel_initializer = kernel_initializer)\n",
    "        return x\n",
    "\n",
    "    def transition_block(self, x, reduction, name, weight_decay = 1e-4, kernel_initializer = 'he_normal'):\n",
    "        \"\"\"A transition block.\n",
    "        # Arguments\n",
    "            x: input tensor.\n",
    "            reduction: float, compression rate at transition layers.\n",
    "            name: string, block label.\n",
    "        # Returns\n",
    "            output tensor for the block.\n",
    "        \"\"\"\n",
    "        bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "        x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                    name=name + '_bn')(x)\n",
    "        x = layers.Activation('relu', name=name + '_relu')(x)\n",
    "        x = layers.Conv2D(int(K.int_shape(x)[bn_axis] * reduction), 1,\n",
    "                        use_bias=False,\n",
    "                        kernel_initializer = kernel_initializer,\n",
    "                        kernel_regularizer = l2(weight_decay),\n",
    "                        name=name + '_conv')(x)\n",
    "        x = layers.AveragePooling2D(2, strides=2, name=name + '_pool')(x)\n",
    "        return x\n",
    "\n",
    "    def conv_block(self, x, growth_rate, name, weight_decay = 1e-4, kernel_initializer = 'he_normal'):\n",
    "        \"\"\"A building block for a dense block.\n",
    "        # Arguments\n",
    "            x: input tensor.\n",
    "            growth_rate: float, growth rate at dense layers.\n",
    "            name: string, block label.\n",
    "        # Returns\n",
    "            Output tensor for the block.\n",
    "        \"\"\"\n",
    "        bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "        x1 = layers.BatchNormalization(axis=bn_axis,\n",
    "                                    epsilon=1.001e-5,\n",
    "                                    name=name + '_0_bn')(x)\n",
    "        x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
    "        x1 = layers.Conv2D(4 * growth_rate, 1,\n",
    "                        use_bias=False,\n",
    "                        kernel_initializer = kernel_initializer,\n",
    "                        kernel_regularizer = l2(weight_decay),\n",
    "                        name=name + '_1_conv')(x1)\n",
    "        x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                    name=name + '_1_bn')(x1)\n",
    "        x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
    "        x1 = layers.Conv2D(growth_rate, 3,\n",
    "                        padding='same',\n",
    "                        use_bias=False,\n",
    "                        kernel_initializer = kernel_initializer,\n",
    "                        kernel_regularizer = l2(weight_decay),\n",
    "                        name=name + '_2_conv')(x1)\n",
    "        x = layers.Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
    "        return x\n",
    "\n",
    "    def small_densenet(self, classes = None, \n",
    "                       img_input_shape = (64, 64, 3), \n",
    "                       blocks = [6, 12, 24, 16], \n",
    "                       weight_decay = 1e-4, \n",
    "                       kernel_initializer = 'he_normal',\n",
    "                       reduction = 0.5, \n",
    "                       init_filters = None, \n",
    "                       growth_rate = None):\n",
    "        img_input = Input(shape = (img_input_shape))\n",
    "#         x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n",
    "#         x = layers.Conv2D(64, 3, strides=1, use_bias=False, \n",
    "#             kernel_initializer = kernel_initializer, \n",
    "#             kernel_regularizer = l2(weight_decay),\n",
    "#             name='conv1/conv')(img_input)\n",
    "#         x = layers.BatchNormalization(\n",
    "#             axis=3, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "#         x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "#         x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
    "#         x = layers.MaxPooling2D(3, strides=2, name='pool1')(x)\n",
    "        \n",
    "        x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n",
    "        x = layers.Conv2D(init_filters, 3, strides=1, use_bias=False, \n",
    "            kernel_initializer = kernel_initializer, \n",
    "            kernel_regularizer = l2(weight_decay),\n",
    "            name='conv1/conv')(x)\n",
    "        x = layers.BatchNormalization(\n",
    "            axis=3, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "        x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "        x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
    "        x = layers.AveragePooling2D(3, strides=2, name='pool1')(x)\n",
    "        \n",
    "        for i, block in enumerate(blocks):\n",
    "            scope_num_str = str(i + 2)\n",
    "            x = self.dense_block(x, block, name='conv' + scope_num_str, \n",
    "                                 weight_decay = weight_decay, kernel_initializer = kernel_initializer, growth_rate = growth_rate)\n",
    "            if i != len(blocks) - 1:\n",
    "                x = self.transition_block(x, reduction, name='pool' + scope_num_str, \n",
    "                                          weight_decay = weight_decay, kernel_initializer = kernel_initializer)\n",
    "        \n",
    "        x = layers.BatchNormalization(\n",
    "            axis=3, epsilon=1.001e-5, name='bn')(x)\n",
    "        x = layers.Activation('relu', name='relu')(x)\n",
    "\n",
    "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        # x = Lambda(lambda x: x, name = 'densenet_features')(x)\n",
    "        x = layers.Dense(classes, activation='softmax',\n",
    "            kernel_initializer = kernel_initializer, \n",
    "#             kernel_regularizer = l2(weight_decay), \n",
    "            name='fc')(x)\n",
    "        \n",
    "        model = Model(img_input, x)\n",
    "        print (model.summary())\n",
    "        model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
    "        \n",
    "        return model\n",
    "# train_data = train_data[:1000]\n",
    "train_img = extract_array_from_series(train_data['img'])\n",
    "train_img = vgg16.preprocess_input(train_img)\n",
    "OneHotEncoder = sklearn.preprocessing.OneHotEncoder()\n",
    "train_target = train_data['class_id'].apply(lambda id: category_dict[id]).values\n",
    "train_target = OneHotEncoder.fit_transform(np.reshape(train_target, (-1, 1))).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train target nunique:  190 Validate target nuique:  190\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 62, 62, 64)   1728        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 62, 62, 64)   256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 62, 62, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 62, 62, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 62, 62, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 62, 62, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 62, 62, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 62, 62, 96)   0           conv1/conv[0][0]                 \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 62, 62, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 62, 62, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 62, 62, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 62, 62, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 62, 62, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 62, 62, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 62, 62, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 62, 62, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 62, 62, 128)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 62, 62, 64)   8192        pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 31, 31, 64)   0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 31, 31, 64)   256         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 31, 31, 64)   0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 31, 31, 128)  8192        conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 31, 31, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 31, 31, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 31, 31, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 31, 31, 96)   0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 31, 31, 96)   384         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 31, 31, 96)   0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 31, 31, 128)  12288       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 31, 31, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 31, 31, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 31, 31, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 31, 31, 128)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 31, 31, 128)  512         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 31, 31, 128)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 31, 31, 64)   8192        pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 15, 15, 64)   0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 15, 15, 64)   256         pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 15, 15, 64)   0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 15, 15, 128)  8192        conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 15, 15, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 15, 15, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 15, 15, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 15, 15, 96)   0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 15, 15, 96)   384         conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 15, 15, 96)   0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 15, 15, 128)  12288       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 15, 15, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 15, 15, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 15, 15, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 15, 15, 128)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 15, 15, 128)  512         conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 15, 15, 128)  0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 15, 15, 64)   8192        pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 7, 7, 64)     0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 64)     256         pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 7, 7, 64)     0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    8192        conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 7, 7, 96)     0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 7, 7, 96)     384         conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 7, 7, 96)     0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    12288       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 7, 7, 128)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 7, 7, 128)    512         conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 7, 7, 128)    0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 128)          0           relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc (Dense)                      (None, 190)          24510       avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 436,350\n",
      "Trainable params: 431,998\n",
      "Non-trainable params: 4,352\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30576 samples, validate on 7645 samples\n",
      "Epoch 1/100\n",
      "16128/30576 [==============>...............] - ETA: 2:11:36 - loss: 5.1581 - categorical_accuracy: 0.0287"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-19b82834cf52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     h = img_model.model.fit(train_part_img, train_part_target, validation_data=(validate_part_img, validate_part_target), \n\u001b[1;32m     47\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                   callbacks=[EarlyStopping(monitor='val_categorical_accuracy', patience=15, verbose=0)])\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fold = 5\n",
    "kf = KFold(n_splits=fold, shuffle=True)\n",
    "num_fold = 0\n",
    "batch_size = 256\n",
    "# train_image_feature_map = np.reshape(vgg_model.predict(train_data_part_img, verbose=  1), (train_data_part_length, -1))\n",
    "# with open(path + 'train_vgg16_img_div255.pickle', 'wb+') as handle:\n",
    "#     pickle.dump(train_y, handle)\n",
    "# with open(path + 'train_vgg16_img_div255.pickle', 'rb') as handle:\n",
    "#     train_image_feature_map = pickle.load(handle)\n",
    "\n",
    "for train_index, test_index in kf.split(train_img):\n",
    "    train_part_img = train_img[train_index]\n",
    "    validate_part_img = train_img[test_index]\n",
    "    \n",
    "    train_part_target = train_target[train_index]\n",
    "    validate_part_target = train_target[test_index]\n",
    "    \n",
    "    print ('Train target nunique: ', np.unique(np.argwhere(train_part_target == 1)[:, 1]).shape[0], \n",
    "           'Validate target nuique: ', np.unique(np.argwhere(validate_part_target == 1)[:, 1]).shape[0])\n",
    "#     model = vgg16.VGG16(include_top=True, weights = None, input_shape=(64, 64, 3), classes = category.shape[0])\n",
    "#     model = resnet50.ResNet50(include_top=True, weights = None, input_shape=(224, 224, 3), classes = category.shape[0])\n",
    "#     print (model.summary())\n",
    "#     model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
    "    img_model = DenseNet(blocks = [2, 2, 2, 2], \n",
    "                                cat_max = train_target.shape[1],\n",
    "                                weight_decay = 1e-4, \n",
    "                                kernel_initializer = 'glorot_normal',\n",
    "                                reduction = 0.5) \n",
    "#     print (img_model.summary())\n",
    "    # img_model = small_vgg(img_input_shape = (64, 64, 3), classes = train_target.shape[1])\n",
    "#     datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "# #             featurewise_center=True,\n",
    "# #             featurewise_std_normalization=True,\n",
    "#             rotation_range=45,\n",
    "#             shear_range = 0.2,\n",
    "#             zoom_range=0.2,\n",
    "# #             width_shift_range=0.2,\n",
    "# #             height_shift_range=0.2,\n",
    "#             horizontal_flip=True)\n",
    "#     datagen.fit(train_part_img)\n",
    "#     h = img_model.model.fit_generator(datagen.flow(train_part_img, train_part_target, batch_size=batch_size), \n",
    "#                   validation_data=(validate_part_img, validate_part_target), \n",
    "#                   epochs=100, shuffle=True, verbose = 1, workers=1, use_multiprocessing=False, \n",
    "#                   callbacks=[EarlyStopping(monitor='val_categorical_accuracy', patience=25, verbose=0)],\n",
    "#                   steps_per_epoch = train_part_img.shape[0]//batch_size,)\n",
    "    h = img_model.model.fit(train_part_img, train_part_target, validation_data=(validate_part_img, validate_part_target), \n",
    "                  epochs=100, batch_size = 256, shuffle=True, verbose = 1, \n",
    "                  callbacks=[EarlyStopping(monitor='val_categorical_accuracy', patience=15, verbose=0)])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8e6b6548d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(h.history)[['val_categorical_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train target nunique:  190 Validate target nuique:  190\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 190)               194750    \n",
      "=================================================================\n",
      "Total params: 194,750\n",
      "Trainable params: 194,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 30576 samples, validate on 7645 samples\n",
      "Epoch 1/100\n",
      "30576/30576 [==============================] - 3s 110us/step - loss: 1.4284 - categorical_accuracy: 0.7520 - val_loss: 0.7710 - val_categorical_accuracy: 0.8599\n",
      "Epoch 2/100\n",
      "30576/30576 [==============================] - 3s 92us/step - loss: 0.6374 - categorical_accuracy: 0.8731 - val_loss: 0.7699 - val_categorical_accuracy: 0.8570\n",
      "Epoch 3/100\n",
      "30576/30576 [==============================] - 3s 99us/step - loss: 0.5348 - categorical_accuracy: 0.8864 - val_loss: 0.7764 - val_categorical_accuracy: 0.8581\n",
      "Epoch 4/100\n",
      "30576/30576 [==============================] - 3s 101us/step - loss: 0.4593 - categorical_accuracy: 0.8967 - val_loss: 0.7952 - val_categorical_accuracy: 0.8583\n"
     ]
    }
   ],
   "source": [
    "def small_dnn(classes = None, \n",
    "                       img_input_shape = (64, 64, 3), \n",
    "                       blocks = [6, 12, 24, 16], \n",
    "                       weight_decay = 1e-4, \n",
    "                       kernel_initializer = 'he_normal',\n",
    "                       reduction = 0.5):\n",
    "    img_input = Input(shape = (img_input_shape))\n",
    "    x = layers.Dense(classes, activation='softmax',\n",
    "        kernel_initializer = kernel_initializer, \n",
    "#             kernel_regularizer = l2(weight_decay), \n",
    "        name='fc')(img_input)\n",
    "\n",
    "    model = Model(img_input, x)\n",
    "    print (model.summary())\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "fold = 5\n",
    "kf = KFold(n_splits=fold, shuffle=True)\n",
    "num_fold = 0\n",
    "batch_size = 256\n",
    "train_image_feature_map = extract_array_from_series(train_data['target'])\n",
    "\n",
    "for train_index, test_index in kf.split(train_img):\n",
    "    train_part_img = train_image_feature_map[train_index]\n",
    "    validate_part_img = train_image_feature_map[test_index]\n",
    "    \n",
    "    train_part_target = train_target[train_index]\n",
    "    validate_part_target = train_target[test_index]\n",
    "    \n",
    "    print ('Train target nunique: ', np.unique(np.argwhere(train_part_target == 1)[:, 1]).shape[0], \n",
    "           'Validate target nuique: ', np.unique(np.argwhere(validate_part_target == 1)[:, 1]).shape[0])\n",
    "\n",
    "    img_classifi_model = small_dnn(img_input_shape = (1024, ), \n",
    "                                classes = train_target.shape[1]) \n",
    "    \n",
    "    h = img_classifi_model.fit(train_part_img, train_part_target, validation_data=(validate_part_img, validate_part_target), \n",
    "                  epochs=100, batch_size = 64, shuffle=True, verbose = 1, \n",
    "                  callbacks=[EarlyStopping(monitor='val_categorical_accuracy', patience=3, verbose=0)])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38221/38221 [==============================] - 2s 45us/step\n"
     ]
    }
   ],
   "source": [
    "pred_class_probas = img_classifi_model.predict(train_image_feature_map, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class_max_proba = pred_class_probas.max(axis = 1)\n",
    "pred_class_id = category[pred_class_probas.argmax(axis = 1)]\n",
    "# np.sum(pred_class_id == train_data['class_id']) / train_data.shape[0]\n",
    "train_data['pred_max_proba'] = pred_class_max_proba\n",
    "train_data['pred_class_id'] = pred_class_id\n",
    "# pred_class_max_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.973545127192473\n",
      "0.5555555555555556 0.9791893883566691\n",
      "0.6111111111111112 0.9839496930678245\n",
      "0.6666666666666666 0.9879055597867479\n",
      "0.7222222222222222 0.9910592325841301\n",
      "0.7777777777777778 0.9933481858688733\n",
      "0.8333333333333333 0.9950834818187818\n",
      "0.8888888888888888 0.9968510066164243\n",
      "0.9444444444444444 0.9981974428840914\n",
      "1.0 nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for th in np.linspace(0.5, 1, 10):\n",
    "    precision = np.sum((train_data.pred_max_proba > th) & (train_data.pred_class_id == train_data.class_id)) \\\n",
    "        / np.sum(train_data.pred_max_proba > th)    \n",
    "    print (th, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['preds'] = list(pred_class_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-0662d19f6b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcategory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ZJL1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ZJL1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "category_dict[('ZJL1', 'ZJL1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n03598930', 'jigsaw_puzzle', 0.023861792), ('n02095314', 'wire-haired_fox_terrier', 0.022931756), ('n04589890', 'window_screen', 0.021361042)]\n"
     ]
    }
   ],
   "source": [
    "# np.asarray(list(train_data['img'])).shape\n",
    "# train_data[train_data['class_id'] == 'ZJL1']\n",
    "# train_data['class_id'].value_counts().hist()\n",
    "# train_data.head()\n",
    "# class_id_emb_attr.iloc[0].name\n",
    "# resnet50_model = ResNet50(weights='imagenet', input_shape=(224, 224, 3))\n",
    "img = image.load_img(path + '/DatasetA_train_20180813/train/000c0d617f5b67d116dee15c40d1d47d.jpeg', target_size=(224, 224))\n",
    "img = image.img_to_array(img)\n",
    "img = vgg16.preprocess_input(img)\n",
    "preds = vgg_model.predict(np.expand_dims(img, axis=0))\n",
    "print('Predicted:', vgg16.decode_predictions(preds, top=3)[0])\n",
    "# imread(path + '/DatasetA_train_20180813/train/000c0d617f5b67d116dee15c40d1d47d.jpeg')\n",
    "# image.img_to_array(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_train(train_part, train_part_label, valide_part, valide_part_label, fold_seed,\n",
    "        fold = 5, train_weight = None, valide_weight = None, flags = None):\n",
    "    \"\"\"\n",
    "    LGBM Training\n",
    "    \"\"\"\n",
    "    print(\"-----LGBM training-----\")\n",
    "\n",
    "    d_train = lgb.Dataset(train_part, train_part_label) #, categorical_feature = CATEGORY_FEATURES) #, init_score = train_part[:, -1])\n",
    "    d_valide = lgb.Dataset(valide_part, valide_part_label) #, categorical_feature = CATEGORY_FEATURES) #, init_score = valide_part[:, -1])\n",
    "    params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt', #'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            # 'num_leaves':-1,\n",
    "            'min_sum_hessian_in_leaf':None,\n",
    "            'max_depth':7,\n",
    "            'learning_rate':0.005,\n",
    "            'feature_fraction':0.1,\n",
    "            'verbose':-1,\n",
    "            'num_boost_round':3000,\n",
    "            'drop_rate':None,\n",
    "            'bagging_fraction':0.6,\n",
    "            'bagging_freq':5,\n",
    "            'early_stopping_round':100,\n",
    "            # 'min_data_in_leaf':100,\n",
    "            'max_bin': None,\n",
    "            'scale_pos_weight':None,\n",
    "        }\n",
    "    print (\"lightgbm params: {0}\\n\".format(params))\n",
    "\n",
    "    bst = lgb.train(\n",
    "                    params ,\n",
    "                    d_train,\n",
    "                    verbose_eval = 200,\n",
    "                    valid_sets = [d_train, d_valide],\n",
    "                    # feature_name= keras_train.DENSE_FEATURE_LIST,\n",
    "                    #feval = gini_lgbm\n",
    "                    #num_boost_round = 1\n",
    "                    )\n",
    "    #pred = model_eval(bst, 'l', valide_part)\n",
    "    #print(pred[:10])\n",
    "    #print(valide_part_label[:10])\n",
    "    #print(valide_part[:10, -1])\n",
    "    # exit(0)\n",
    "    feature_imp = bst.feature_importance(importance_type = 'gain')\n",
    "    sort_ind = np.argsort(feature_imp)[::-1]\n",
    "    print (np.c_[np.array(FEATURE_LIST)[sort_ind], feature_imp[sort_ind]][:10])\n",
    "    # print (np.array(keras_train.FEATURE_LIST)[np.argsort(feature_imp)])\n",
    "    # exit(0)\n",
    "    # cv_result = lgb.cv(params, d_train, nfold=fold) #, feval = gini_lgbm)\n",
    "    # pd.DataFrame(cv_result).to_csv('cv_result', index = False)\n",
    "    # exit(0)\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 64, 64, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D) (None, 70, 70, 3)     0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)              (None, 68, 68, 64)    1728        zero_padding2d_1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)    (None, 68, 68, 64)    256         conv1/conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1/relu (Activation)          (None, 68, 68, 64)    0           conv1/bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D) (None, 70, 70, 64)    0           conv1/relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (AveragePooling2D)         (None, 34, 34, 64)    0           zero_padding2d_2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormaliz (None, 34, 34, 64)    256         pool1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation) (None, 34, 34, 64)    0           conv2_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)     (None, 34, 34, 64)    4096        conv2_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenate (None, 34, 34, 80)    0           pool1[0][0]                      \n",
      "                                                                   conv2_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormaliz (None, 34, 34, 80)    320         conv2_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation) (None, 34, 34, 80)    0           conv2_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)     (None, 34, 34, 64)    5120        conv2_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenate (None, 34, 34, 96)    0           conv2_block1_concat[0][0]        \n",
      "                                                                   conv2_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormaliz (None, 34, 34, 96)    384         conv2_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation) (None, 34, 34, 96)    0           conv2_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)     (None, 34, 34, 64)    6144        conv2_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenate (None, 34, 34, 112)   0           conv2_block2_concat[0][0]        \n",
      "                                                                   conv2_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormaliz (None, 34, 34, 112)   448         conv2_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation) (None, 34, 34, 112)   0           conv2_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)     (None, 34, 34, 64)    7168        conv2_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenate (None, 34, 34, 128)   0           conv2_block3_concat[0][0]        \n",
      "                                                                   conv2_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormaliz (None, 34, 34, 128)   512         conv2_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation) (None, 34, 34, 128)   0           conv2_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)     (None, 34, 34, 64)    8192        conv2_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenate (None, 34, 34, 144)   0           conv2_block4_concat[0][0]        \n",
      "                                                                   conv2_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormaliz (None, 34, 34, 144)   576         conv2_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation) (None, 34, 34, 144)   0           conv2_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)     (None, 34, 34, 64)    9216        conv2_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenate (None, 34, 34, 160)   0           conv2_block5_concat[0][0]        \n",
      "                                                                   conv2_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)    (None, 34, 34, 160)   640         conv2_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "pool2_relu (Activation)          (None, 34, 34, 160)   0           pool2_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)              (None, 34, 34, 80)    12800       pool2_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)    (None, 17, 17, 80)    0           pool2_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormaliz (None, 17, 17, 80)    320         pool2_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation) (None, 17, 17, 80)    0           conv3_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)     (None, 17, 17, 64)    5120        conv3_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenate (None, 17, 17, 96)    0           pool2_pool[0][0]                 \n",
      "                                                                   conv3_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormaliz (None, 17, 17, 96)    384         conv3_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation) (None, 17, 17, 96)    0           conv3_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)     (None, 17, 17, 64)    6144        conv3_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenate (None, 17, 17, 112)   0           conv3_block1_concat[0][0]        \n",
      "                                                                   conv3_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormaliz (None, 17, 17, 112)   448         conv3_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation) (None, 17, 17, 112)   0           conv3_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)     (None, 17, 17, 64)    7168        conv3_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenate (None, 17, 17, 128)   0           conv3_block2_concat[0][0]        \n",
      "                                                                   conv3_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormaliz (None, 17, 17, 128)   512         conv3_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation) (None, 17, 17, 128)   0           conv3_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)     (None, 17, 17, 64)    8192        conv3_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenate (None, 17, 17, 144)   0           conv3_block3_concat[0][0]        \n",
      "                                                                   conv3_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormaliz (None, 17, 17, 144)   576         conv3_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation) (None, 17, 17, 144)   0           conv3_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)     (None, 17, 17, 64)    9216        conv3_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenate (None, 17, 17, 160)   0           conv3_block4_concat[0][0]        \n",
      "                                                                   conv3_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormaliz (None, 17, 17, 160)   640         conv3_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation) (None, 17, 17, 160)   0           conv3_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)     (None, 17, 17, 64)    10240       conv3_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenate (None, 17, 17, 176)   0           conv3_block5_concat[0][0]        \n",
      "                                                                   conv3_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormaliz (None, 17, 17, 176)   704         conv3_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation) (None, 17, 17, 176)   0           conv3_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)     (None, 17, 17, 64)    11264       conv3_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenate (None, 17, 17, 192)   0           conv3_block6_concat[0][0]        \n",
      "                                                                   conv3_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormaliz (None, 17, 17, 192)   768         conv3_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation) (None, 17, 17, 192)   0           conv3_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)     (None, 17, 17, 64)    12288       conv3_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenate (None, 17, 17, 208)   0           conv3_block7_concat[0][0]        \n",
      "                                                                   conv3_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormaliz (None, 17, 17, 208)   832         conv3_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation) (None, 17, 17, 208)   0           conv3_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)     (None, 17, 17, 64)    13312       conv3_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenate (None, 17, 17, 224)   0           conv3_block8_concat[0][0]        \n",
      "                                                                   conv3_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormali (None, 17, 17, 224)   896         conv3_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activation (None, 17, 17, 224)   0           conv3_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)    (None, 17, 17, 64)    14336       conv3_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activation (None, 17, 17, 64)    0           conv3_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatenat (None, 17, 17, 240)   0           conv3_block9_concat[0][0]        \n",
      "                                                                   conv3_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormali (None, 17, 17, 240)   960         conv3_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activation (None, 17, 17, 240)   0           conv3_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)    (None, 17, 17, 64)    15360       conv3_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activation (None, 17, 17, 64)    0           conv3_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatenat (None, 17, 17, 256)   0           conv3_block10_concat[0][0]       \n",
      "                                                                   conv3_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormali (None, 17, 17, 256)   1024        conv3_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activation (None, 17, 17, 256)   0           conv3_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)    (None, 17, 17, 64)    16384       conv3_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activation (None, 17, 17, 64)    0           conv3_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatenat (None, 17, 17, 272)   0           conv3_block11_concat[0][0]       \n",
      "                                                                   conv3_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)    (None, 17, 17, 272)   1088        conv3_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool3_relu (Activation)          (None, 17, 17, 272)   0           pool3_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)              (None, 17, 17, 136)   36992       pool3_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)    (None, 8, 8, 136)     0           pool3_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormaliz (None, 8, 8, 136)     544         pool3_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation) (None, 8, 8, 136)     0           conv4_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)     (None, 8, 8, 64)      8704        conv4_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenate (None, 8, 8, 152)     0           pool3_pool[0][0]                 \n",
      "                                                                   conv4_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormaliz (None, 8, 8, 152)     608         conv4_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation) (None, 8, 8, 152)     0           conv4_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)     (None, 8, 8, 64)      9728        conv4_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenate (None, 8, 8, 168)     0           conv4_block1_concat[0][0]        \n",
      "                                                                   conv4_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormaliz (None, 8, 8, 168)     672         conv4_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation) (None, 8, 8, 168)     0           conv4_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)     (None, 8, 8, 64)      10752       conv4_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenate (None, 8, 8, 184)     0           conv4_block2_concat[0][0]        \n",
      "                                                                   conv4_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormaliz (None, 8, 8, 184)     736         conv4_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation) (None, 8, 8, 184)     0           conv4_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)     (None, 8, 8, 64)      11776       conv4_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenate (None, 8, 8, 200)     0           conv4_block3_concat[0][0]        \n",
      "                                                                   conv4_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormaliz (None, 8, 8, 200)     800         conv4_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation) (None, 8, 8, 200)     0           conv4_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)     (None, 8, 8, 64)      12800       conv4_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenate (None, 8, 8, 216)     0           conv4_block4_concat[0][0]        \n",
      "                                                                   conv4_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormaliz (None, 8, 8, 216)     864         conv4_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation) (None, 8, 8, 216)     0           conv4_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)     (None, 8, 8, 64)      13824       conv4_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenate (None, 8, 8, 232)     0           conv4_block5_concat[0][0]        \n",
      "                                                                   conv4_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormaliz (None, 8, 8, 232)     928         conv4_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation) (None, 8, 8, 232)     0           conv4_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)     (None, 8, 8, 64)      14848       conv4_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenate (None, 8, 8, 248)     0           conv4_block6_concat[0][0]        \n",
      "                                                                   conv4_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormaliz (None, 8, 8, 248)     992         conv4_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation) (None, 8, 8, 248)     0           conv4_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)     (None, 8, 8, 64)      15872       conv4_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenate (None, 8, 8, 264)     0           conv4_block7_concat[0][0]        \n",
      "                                                                   conv4_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormaliz (None, 8, 8, 264)     1056        conv4_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation) (None, 8, 8, 264)     0           conv4_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)     (None, 8, 8, 64)      16896       conv4_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenate (None, 8, 8, 280)     0           conv4_block8_concat[0][0]        \n",
      "                                                                   conv4_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormali (None, 8, 8, 280)     1120        conv4_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activation (None, 8, 8, 280)     0           conv4_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)    (None, 8, 8, 64)      17920       conv4_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activation (None, 8, 8, 64)      0           conv4_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatenat (None, 8, 8, 296)     0           conv4_block9_concat[0][0]        \n",
      "                                                                   conv4_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormali (None, 8, 8, 296)     1184        conv4_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activation (None, 8, 8, 296)     0           conv4_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)    (None, 8, 8, 64)      18944       conv4_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activation (None, 8, 8, 64)      0           conv4_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatenat (None, 8, 8, 312)     0           conv4_block10_concat[0][0]       \n",
      "                                                                   conv4_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormali (None, 8, 8, 312)     1248        conv4_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activation (None, 8, 8, 312)     0           conv4_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)    (None, 8, 8, 64)      19968       conv4_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activation (None, 8, 8, 64)      0           conv4_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatenat (None, 8, 8, 328)     0           conv4_block11_concat[0][0]       \n",
      "                                                                   conv4_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormali (None, 8, 8, 328)     1312        conv4_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activation (None, 8, 8, 328)     0           conv4_block13_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)    (None, 8, 8, 64)      20992       conv4_block13_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block13_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activation (None, 8, 8, 64)      0           conv4_block13_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block13_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatenat (None, 8, 8, 344)     0           conv4_block12_concat[0][0]       \n",
      "                                                                   conv4_block13_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormali (None, 8, 8, 344)     1376        conv4_block13_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activation (None, 8, 8, 344)     0           conv4_block14_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)    (None, 8, 8, 64)      22016       conv4_block14_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block14_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activation (None, 8, 8, 64)      0           conv4_block14_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block14_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatenat (None, 8, 8, 360)     0           conv4_block13_concat[0][0]       \n",
      "                                                                   conv4_block14_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormali (None, 8, 8, 360)     1440        conv4_block14_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activation (None, 8, 8, 360)     0           conv4_block15_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)    (None, 8, 8, 64)      23040       conv4_block15_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block15_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activation (None, 8, 8, 64)      0           conv4_block15_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block15_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatenat (None, 8, 8, 376)     0           conv4_block14_concat[0][0]       \n",
      "                                                                   conv4_block15_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormali (None, 8, 8, 376)     1504        conv4_block15_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activation (None, 8, 8, 376)     0           conv4_block16_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)    (None, 8, 8, 64)      24064       conv4_block16_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block16_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activation (None, 8, 8, 64)      0           conv4_block16_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block16_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatenat (None, 8, 8, 392)     0           conv4_block15_concat[0][0]       \n",
      "                                                                   conv4_block16_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormali (None, 8, 8, 392)     1568        conv4_block16_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activation (None, 8, 8, 392)     0           conv4_block17_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)    (None, 8, 8, 64)      25088       conv4_block17_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block17_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activation (None, 8, 8, 64)      0           conv4_block17_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block17_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatenat (None, 8, 8, 408)     0           conv4_block16_concat[0][0]       \n",
      "                                                                   conv4_block17_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormali (None, 8, 8, 408)     1632        conv4_block17_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activation (None, 8, 8, 408)     0           conv4_block18_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)    (None, 8, 8, 64)      26112       conv4_block18_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block18_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activation (None, 8, 8, 64)      0           conv4_block18_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block18_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatenat (None, 8, 8, 424)     0           conv4_block17_concat[0][0]       \n",
      "                                                                   conv4_block18_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormali (None, 8, 8, 424)     1696        conv4_block18_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activation (None, 8, 8, 424)     0           conv4_block19_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)    (None, 8, 8, 64)      27136       conv4_block19_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block19_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activation (None, 8, 8, 64)      0           conv4_block19_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block19_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatenat (None, 8, 8, 440)     0           conv4_block18_concat[0][0]       \n",
      "                                                                   conv4_block19_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormali (None, 8, 8, 440)     1760        conv4_block19_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activation (None, 8, 8, 440)     0           conv4_block20_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)    (None, 8, 8, 64)      28160       conv4_block20_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block20_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activation (None, 8, 8, 64)      0           conv4_block20_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block20_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatenat (None, 8, 8, 456)     0           conv4_block19_concat[0][0]       \n",
      "                                                                   conv4_block20_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormali (None, 8, 8, 456)     1824        conv4_block20_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activation (None, 8, 8, 456)     0           conv4_block21_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)    (None, 8, 8, 64)      29184       conv4_block21_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block21_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activation (None, 8, 8, 64)      0           conv4_block21_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block21_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatenat (None, 8, 8, 472)     0           conv4_block20_concat[0][0]       \n",
      "                                                                   conv4_block21_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormali (None, 8, 8, 472)     1888        conv4_block21_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activation (None, 8, 8, 472)     0           conv4_block22_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)    (None, 8, 8, 64)      30208       conv4_block22_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block22_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activation (None, 8, 8, 64)      0           conv4_block22_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block22_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatenat (None, 8, 8, 488)     0           conv4_block21_concat[0][0]       \n",
      "                                                                   conv4_block22_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormali (None, 8, 8, 488)     1952        conv4_block22_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activation (None, 8, 8, 488)     0           conv4_block23_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)    (None, 8, 8, 64)      31232       conv4_block23_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block23_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activation (None, 8, 8, 64)      0           conv4_block23_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block23_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatenat (None, 8, 8, 504)     0           conv4_block22_concat[0][0]       \n",
      "                                                                   conv4_block23_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormali (None, 8, 8, 504)     2016        conv4_block23_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activation (None, 8, 8, 504)     0           conv4_block24_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)    (None, 8, 8, 64)      32256       conv4_block24_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block24_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activation (None, 8, 8, 64)      0           conv4_block24_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block24_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatenat (None, 8, 8, 520)     0           conv4_block23_concat[0][0]       \n",
      "                                                                   conv4_block24_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)    (None, 8, 8, 520)     2080        conv4_block24_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool4_relu (Activation)          (None, 8, 8, 520)     0           pool4_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)              (None, 8, 8, 260)     135200      pool4_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)    (None, 4, 4, 260)     0           pool4_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormaliz (None, 4, 4, 260)     1040        pool4_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation) (None, 4, 4, 260)     0           conv5_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)     (None, 4, 4, 64)      16640       conv5_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenate (None, 4, 4, 276)     0           pool4_pool[0][0]                 \n",
      "                                                                   conv5_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormaliz (None, 4, 4, 276)     1104        conv5_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation) (None, 4, 4, 276)     0           conv5_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)     (None, 4, 4, 64)      17664       conv5_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenate (None, 4, 4, 292)     0           conv5_block1_concat[0][0]        \n",
      "                                                                   conv5_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormaliz (None, 4, 4, 292)     1168        conv5_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation) (None, 4, 4, 292)     0           conv5_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)     (None, 4, 4, 64)      18688       conv5_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenate (None, 4, 4, 308)     0           conv5_block2_concat[0][0]        \n",
      "                                                                   conv5_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormaliz (None, 4, 4, 308)     1232        conv5_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation) (None, 4, 4, 308)     0           conv5_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)     (None, 4, 4, 64)      19712       conv5_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenate (None, 4, 4, 324)     0           conv5_block3_concat[0][0]        \n",
      "                                                                   conv5_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormaliz (None, 4, 4, 324)     1296        conv5_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation) (None, 4, 4, 324)     0           conv5_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)     (None, 4, 4, 64)      20736       conv5_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenate (None, 4, 4, 340)     0           conv5_block4_concat[0][0]        \n",
      "                                                                   conv5_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormaliz (None, 4, 4, 340)     1360        conv5_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation) (None, 4, 4, 340)     0           conv5_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)     (None, 4, 4, 64)      21760       conv5_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenate (None, 4, 4, 356)     0           conv5_block5_concat[0][0]        \n",
      "                                                                   conv5_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormaliz (None, 4, 4, 356)     1424        conv5_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation) (None, 4, 4, 356)     0           conv5_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)     (None, 4, 4, 64)      22784       conv5_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenate (None, 4, 4, 372)     0           conv5_block6_concat[0][0]        \n",
      "                                                                   conv5_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormaliz (None, 4, 4, 372)     1488        conv5_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation) (None, 4, 4, 372)     0           conv5_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)     (None, 4, 4, 64)      23808       conv5_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenate (None, 4, 4, 388)     0           conv5_block7_concat[0][0]        \n",
      "                                                                   conv5_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormaliz (None, 4, 4, 388)     1552        conv5_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation) (None, 4, 4, 388)     0           conv5_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)     (None, 4, 4, 64)      24832       conv5_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenate (None, 4, 4, 404)     0           conv5_block8_concat[0][0]        \n",
      "                                                                   conv5_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormali (None, 4, 4, 404)     1616        conv5_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activation (None, 4, 4, 404)     0           conv5_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)    (None, 4, 4, 64)      25856       conv5_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activation (None, 4, 4, 64)      0           conv5_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatenat (None, 4, 4, 420)     0           conv5_block9_concat[0][0]        \n",
      "                                                                   conv5_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormali (None, 4, 4, 420)     1680        conv5_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activation (None, 4, 4, 420)     0           conv5_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)    (None, 4, 4, 64)      26880       conv5_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activation (None, 4, 4, 64)      0           conv5_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatenat (None, 4, 4, 436)     0           conv5_block10_concat[0][0]       \n",
      "                                                                   conv5_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormali (None, 4, 4, 436)     1744        conv5_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activation (None, 4, 4, 436)     0           conv5_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)    (None, 4, 4, 64)      27904       conv5_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activation (None, 4, 4, 64)      0           conv5_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatenat (None, 4, 4, 452)     0           conv5_block11_concat[0][0]       \n",
      "                                                                   conv5_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormali (None, 4, 4, 452)     1808        conv5_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activation (None, 4, 4, 452)     0           conv5_block13_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)    (None, 4, 4, 64)      28928       conv5_block13_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block13_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activation (None, 4, 4, 64)      0           conv5_block13_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block13_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatenat (None, 4, 4, 468)     0           conv5_block12_concat[0][0]       \n",
      "                                                                   conv5_block13_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormali (None, 4, 4, 468)     1872        conv5_block13_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activation (None, 4, 4, 468)     0           conv5_block14_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)    (None, 4, 4, 64)      29952       conv5_block14_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block14_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activation (None, 4, 4, 64)      0           conv5_block14_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block14_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatenat (None, 4, 4, 484)     0           conv5_block13_concat[0][0]       \n",
      "                                                                   conv5_block14_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormali (None, 4, 4, 484)     1936        conv5_block14_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activation (None, 4, 4, 484)     0           conv5_block15_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)    (None, 4, 4, 64)      30976       conv5_block15_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block15_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activation (None, 4, 4, 64)      0           conv5_block15_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block15_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatenat (None, 4, 4, 500)     0           conv5_block14_concat[0][0]       \n",
      "                                                                   conv5_block15_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormali (None, 4, 4, 500)     2000        conv5_block15_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activation (None, 4, 4, 500)     0           conv5_block16_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)    (None, 4, 4, 64)      32000       conv5_block16_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block16_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activation (None, 4, 4, 64)      0           conv5_block16_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block16_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatenat (None, 4, 4, 516)     0           conv5_block15_concat[0][0]       \n",
      "                                                                   conv5_block16_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "bn (BatchNormalization)          (None, 4, 4, 516)     2064        conv5_block16_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "relu (Activation)                (None, 4, 4, 516)     0           bn[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2D (None, 516)           0           relu[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "fc (Dense)                       (None, 171)           88407       avg_pool[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 1,945,831\n",
      "Trainable params: 1,902,543\n",
      "Non-trainable params: 43,288\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"av...)`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38208/38221 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "img_model = DenseNet(blocks = [6, 12, 24, 16], \n",
    "                                cat_max = 171,\n",
    "                                weight_decay = 1e-4, \n",
    "                                kernel_initializer = 'glorot_normal',\n",
    "                                reduction = 0.5, \n",
    "                                init_filters = 64, \n",
    "                                growth_rate = 16).model\n",
    "img_model.load_weights(path + 'model_sub/6_12_24_16_ini64_growth16_02962/model_0_2018_09_13_08_23_48.h5')\n",
    "img_model_flat = Model(input = img_model.input, output = img_model.get_layer(name = 'avg_pool').output)\n",
    "train_data['target'] = list(img_model_flat.predict(train_img, verbose = 1))\n",
    "train_data['preds'] = list(img_model.predict(train_img, verbose = 1))\n",
    "# train_data['target'] = list(train_y) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn_data(df):\n",
    "    return [extract_array_from_series(df['attr']), extract_array_from_series(df['emb'])[:, :300]]\n",
    "\n",
    "train_part_img_id = pd.read_csv(path + '/model_sub/6_12_24_16_ini64_growth16_02962/train_part_img_id_0.csv', header = None)\n",
    "validate_part_img_id = pd.read_csv(path + '/model_sub/6_12_24_16_ini64_growth16_02962/validate_part_img_id_0.csv', header = None)\n",
    "train_part_img_id = train_part_img_id[0].values\n",
    "validate_part_img_id = validate_part_img_id[0].values\n",
    "\n",
    "train_part_df = train_data[train_data['image_id'].isin(train_part_img_id)]\n",
    "validate_part_df = train_data[train_data['image_id'].isin(validate_part_img_id)]\n",
    "\n",
    "seen_class = train_part_df.append(validate_part_df).class_id.unique()\n",
    "\n",
    "unseen_class_df = train_data[~train_data['class_id'].isin(seen_class)]\n",
    "unseen_class = unseen_class_df.class_id.unique()\n",
    "\n",
    "# validate_part_df = validate_part_df.append(unseen_class_df)\n",
    "train_part_df = train_part_df.append(validate_part_df)\n",
    "validate_part_df = unseen_class_df\n",
    "\n",
    "train_part_data = create_dnn_data(train_part_df)\n",
    "train_part_target = extract_array_from_series(train_part_df['target'])\n",
    "\n",
    "validate_part_data = create_dnn_data(validate_part_df)\n",
    "validate_part_target = extract_array_from_series(validate_part_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:144: UserWarning: Output \"dense_70\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_70\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "wv (InputLayer)              (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 768)               787200    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 768)               3072      \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 516)               396804    \n",
      "=================================================================\n",
      "Total params: 1,500,596\n",
      "Trainable params: 1,496,412\n",
      "Non-trainable params: 4,184\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 34420 samples, validate on 3801 samples\n",
      "Epoch 1/1\n",
      "33920/34420 [============================>.] - ETA: 0s - loss: 0.8484\n",
      "All_re: \t0.141805\t539\t3801\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.141805\t539\t3801\n",
      "34420/34420 [==============================] - 3s - loss: 0.8462 - val_loss: 0.5672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0123418fd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "        \n",
    "def find_nearest_class(class_id_emb_attr, model, eval_df, cand_feature_map, img_feature_map, \n",
    "                       threshold = None, gamma = None, seen_class = None):\n",
    "#     cand_feature_map = model.predict(create_dnn_data(class_id_emb_attr))\n",
    "    # img_feature_map = np.repeat(img_feature_map, [cand_feature_map.shape[0]] * img_feature_map.shape[0], axis = 0)\n",
    "    nearest_class_id = ['ZJL'] * eval_df.shape[0]\n",
    "#     seen_class_bias = np.zeros(class_id_emb_attr.shape[0])\n",
    "#     seen_class_bias[class_id_emb_attr.class_id.isin(seen_class)] = gamma\n",
    "    \n",
    "#     seen_indice = [category_dict[c] for c in seen_class]\n",
    "#     preds = np.asarray(list(eval_df['preds']))[:, seen_indice]\n",
    "    for i in range(img_feature_map.shape[0]):\n",
    "        if False: #np.amax(preds[i]) > threshold:\n",
    "            nearest_class_id[i] = seen_class[np.argmax(preds[i])]\n",
    "        else:\n",
    "            img = img_feature_map[i]\n",
    "#             dis = 1 - sklearn.metrics.pairwise.cosine_similarity([img], \n",
    "#                                     cand_feature_map).reshape((cand_feature_map.shape[0]))\n",
    "            dis = np.linalg.norm(img - cand_feature_map, axis = 1)\n",
    "#             print (dis.shape)\n",
    "#             if np.amin(dis[class_id_emb_attr.class_id.isin(seen_class)]) > \\\n",
    "#                     gamma * np.amin(dis[~class_id_emb_attr.class_id.isin(seen_class)]):\n",
    "#                 dis[class_id_emb_attr.class_id.isin(seen_class)] = 200\n",
    "#             dis += seen_class_bias\n",
    "#             print(dis)\n",
    "#             plt.hist(dis)\n",
    "#             return\n",
    "            min_ind = np.where(dis == np.amin(dis))[0]\n",
    "            if len(min_ind) > 1:\n",
    "                print ('eval img id: ', eval_df.iloc[i]['class_id'], 'has multiple best candidates: ', len(min_ind), 'min val: ', np.amin(dis))\n",
    "    #         print (i, img, class_id_emb_attr.iloc[min_ind[0]].name)\n",
    "            nearest_class_id[i] = class_id_emb_attr.iloc[min_ind[0]]['class_id']\n",
    "#     print (nearest_class_id)\n",
    "    return np.asarray(nearest_class_id)\n",
    "        \n",
    "def calc_accuracy(eval_df, eval_class, preds):\n",
    "    eval_mask = eval_df.class_id.isin(eval_class)\n",
    "    eval_num = np.sum(eval_mask)\n",
    "    right_num = np.sum(preds[eval_mask] == eval_df.class_id[eval_mask])\n",
    "    return right_num / np.sum(eval_mask), right_num, eval_num\n",
    "    \n",
    "def calc_detailed_accuracy(eval_df, preds, seen_class, unseen_class):\n",
    "    all_re = calc_accuracy(eval_df, eval_df['class_id'].values, preds)\n",
    "    seen_re = calc_accuracy(eval_df, seen_class, preds)\n",
    "    unseen_re = calc_accuracy(eval_df, unseen_class, preds)\n",
    "    print(\"\\nAll_re: \\t%.6f\\t%.0f\\t%.0f\" % all_re)\n",
    "    print(\"Seen_re: \\t%.6f\\t%.0f\\t%.0f\" % seen_re)\n",
    "    print(\"Unseen_re: \\t%.6f\\t%.0f\\t%.0f\" % unseen_re)\n",
    "\n",
    "def model_eval(model, model_type, eval_df, cand_class_id_emb_attr, img_feature_map, seen_class = None, unseen_class = None):\n",
    "    if model_type == 'DEM':\n",
    "        zs_model = Model(inputs = model.inputs[:2], outputs = model.outputs[0])\n",
    "        cand_feature_map = zs_model.predict(create_dnn_data(cand_class_id_emb_attr))\n",
    "    elif model_type == 'GCN':\n",
    "        zs_model = Model(inputs = model.inputs[2:], outputs = model.outputs[0])\n",
    "        cand_class_to_id = [class_to_id[c] for c in cand_class_id_emb_attr.class_id.values]\n",
    "        cand_feature_map = zs_model.predict(None, steps = 1)[cand_class_to_id]\n",
    "    preds = find_nearest_class(cand_class_id_emb_attr, zs_model, eval_df, cand_feature_map, img_feature_map)\n",
    "    calc_detailed_accuracy(eval_df, preds, seen_class, unseen_class)\n",
    "    return preds\n",
    "\n",
    "def models_eval(models, eval_df, cand_class_id_emb_attr, img_feature_map, seen_class = None, unseen_class = None):\n",
    "    for model, model_type in models:\n",
    "        model_eval(model, model_type, eval_df, cand_class_id_emb_attr, img_feature_map, seen_class, unseen_class)\n",
    "        \n",
    "class AccuracyEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1, batch_interval = 1000000, verbose = 2, \\\n",
    "            scores = [], class_id_emb_attr = None, eval_df = None, threshold = None, \\\n",
    "                 seen_class = None, unseen_class = None, gamma = None, model_type = None):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        # print(\"y_val shape:{0}\".format(self.y_val.shape))\n",
    "        self.batch_interval = batch_interval\n",
    "        self.verbose = verbose\n",
    "        self.scores = scores\n",
    "        self.class_id_emb_attr = class_id_emb_attr\n",
    "        self.eval_df = eval_df\n",
    "        self.threshold = threshold\n",
    "        self.seen_class = seen_class\n",
    "        self.unseen_class = unseen_class\n",
    "        self.gamma = gamma\n",
    "        self.model_type = model_type\n",
    "#         seen_indice = [category_dict[c] for c in seen_class]\n",
    "#         self.preds = np.asarray(list(eval_df['preds']))[:, seen_indice]\n",
    "#         self.img_feature_map = self.y_val\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            model_eval(self.model, self.model_type, self.eval_df, self.class_id_emb_attr, \n",
    "                seen_class = self.seen_class, unseen_class = self.unseen_class, img_feature_map = self.y_val)\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "        if(self.verbose >= 2) and (batch % self.batch_interval == 0):\n",
    "            # y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            # loss = metrics.log_loss(self.y_val, y_pred)\n",
    "            print(\"Hi! on_batch_end() , batch=\",batch,\",logs:\",logs)\n",
    "            # print(\"Valide size=\",y_pred.shape[0], \"  Valide loss=\",loss)\n",
    "\n",
    "def full_connect_layer(input, hidden_dim, activation, resnet = False):\n",
    "    full_connect = input\n",
    "    for hn in hidden_dim:\n",
    "        fc_in = full_connect\n",
    "        # if self.full_connect_dropout > 0:\n",
    "        full_connect = Dropout(0.2)(full_connect)\n",
    "        full_connect = BatchNormalization(epsilon=1.001e-5)(full_connect)\n",
    "        # full_connect = Dense(hn, kernel_regularizer = l2(0.001), activity_regularizer = l1(0.001))(full_connect)\n",
    "#         full_connect = Concatenate()([Dense(hn, kernel_initializer='lecun_uniform', activation = 'relu')(full_connect), \n",
    "#             Dense(hn, kernel_initializer='lecun_uniform', activation = 'sigmoid')(full_connect)])\n",
    "        full_connect = Dense(hn, kernel_initializer='he_normal', kernel_regularizer = l2(1e-4), activation = activation)(full_connect)\n",
    "#         full_connect = LeakyReLU(alpha=0.02)(full_connect)\n",
    "        # full_connect = self.act_blend(full_connect)\n",
    "        # if self.full_connect_dropout > 0:\n",
    "#             full_connect = Dropout(self.full_connect_dropout)(full_connect) #Dropout(self.full_connect_dropout)(full_connect)\n",
    "        if resnet:\n",
    "            full_connect = Concatenate()([fc_in, full_connect])\n",
    "    return full_connect\n",
    "\n",
    "def create_dnn():\n",
    "    alpha = 0.03\n",
    "    attr_input = Input(shape = (30,), name = 'attr')\n",
    "    word_emb = Input(shape = (300,), name = 'wv')\n",
    "    imag_classifier = Input(shape = (516,), name = 'img')\n",
    "\n",
    "    attr_dense = Dense(300, use_bias = False, kernel_initializer='he_normal', kernel_regularizer = l2(1e-4))(attr_input)\n",
    "#     attr_dense = Dense(512)(attr_dense)\n",
    "#     attr_dense = LeakyReLU(alpha=alpha)(attr_dense)\n",
    "#     attr_dense = Dense(256)(attr_dense)\n",
    "#     attr_dense = LeakyReLU(alpha=alpha)(attr_dense)\n",
    "#     attr_dense = Dense(300, activation=\"relu\")(attr_dense)\n",
    "    # attr_dense = Dense(512, activation=\"relu\")(attr_dense)\n",
    "    word_emb_dense = Dense(300, use_bias = False, kernel_initializer='he_normal', kernel_regularizer = l2(1e-4))(word_emb)\n",
    "\n",
    "#     attr_word_emb = word_emb #Concatenate()([word_emb, attr_dense])\n",
    "    attr_word_emb = word_emb #Add()([word_emb_dense, attr_dense])\n",
    "    attr_word_emb_dense = full_connect_layer(attr_word_emb, hidden_dim = [1024, 768], activation = 'relu', resnet = False)\n",
    "    attr_word_emb_dense = full_connect_layer(attr_word_emb_dense, hidden_dim = [516], activation = 'relu')\n",
    "\n",
    "    mse_loss = K.mean(keras.losses.mean_squared_error(imag_classifier, attr_word_emb_dense))\n",
    "    \n",
    "    model = Model([attr_input, word_emb, imag_classifier], outputs = attr_word_emb_dense) #, vgg_output])\n",
    "    model.add_loss(mse_loss)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=None)\n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n",
    "        AccuracyEvaluation(validation_data=(validate_part_data,  validate_part_target), interval=1, \\\n",
    "#                         class_id_emb_attr = class_id_emb_attr, \\\n",
    "                           eval_df = validate_part_df, threshold= 0.3, \\\n",
    "                          seen_class = seen_class, unseen_class = unseen_class, \\\n",
    "            class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \\\n",
    "                          gamma = 0.8, model_type = 'DEM')\n",
    "        ]\n",
    "\n",
    "zs_model = create_dnn()\n",
    "print (zs_model.summary())\n",
    "zs_model.fit(train_part_data + [train_part_target],  validation_data = (validate_part_data + [validate_part_target], None),\n",
    "              epochs=1, batch_size = 128, shuffle=True, verbose = 1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_model_list = []\n",
    "zs_model_list.append((zs_model, 'DEM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = np.array(list(class_id_emb_attr['attr']))\n",
    "# adj_graph = scipy.eye(attr.shape[0]) #1 - sklearn.metrics.pairwise.pairwise_distances(attr, metric = 'cosine')\n",
    "adj_graph = 1 - sklearn.metrics.pairwise.pairwise_distances(\n",
    "    np.array(list(class_id_emb_attr['emb'])), metric = 'cosine')\n",
    "# th = 0.99999\n",
    "# adj_graph[adj_graph > th] = 1\n",
    "# adj_graph[adj_graph <= th] = 0\n",
    "# adj_graph = adj_graph / np.linalg.norm(adj_graph)\n",
    "# adj_graph = adj_graph[:, np.argsort(adj_graph)[:]]\n",
    "# adj_graph[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ZJL1': 0,\n",
       " 'ZJL10': 1,\n",
       " 'ZJL100': 2,\n",
       " 'ZJL101': 3,\n",
       " 'ZJL102': 4,\n",
       " 'ZJL103': 5,\n",
       " 'ZJL104': 6,\n",
       " 'ZJL105': 7,\n",
       " 'ZJL106': 8,\n",
       " 'ZJL107': 9,\n",
       " 'ZJL108': 10,\n",
       " 'ZJL109': 11,\n",
       " 'ZJL11': 12,\n",
       " 'ZJL110': 13,\n",
       " 'ZJL111': 14,\n",
       " 'ZJL113': 15,\n",
       " 'ZJL114': 16,\n",
       " 'ZJL115': 17,\n",
       " 'ZJL116': 18,\n",
       " 'ZJL117': 19,\n",
       " 'ZJL118': 20,\n",
       " 'ZJL119': 21,\n",
       " 'ZJL12': 22,\n",
       " 'ZJL120': 23,\n",
       " 'ZJL121': 24,\n",
       " 'ZJL122': 25,\n",
       " 'ZJL123': 26,\n",
       " 'ZJL124': 27,\n",
       " 'ZJL125': 28,\n",
       " 'ZJL126': 29,\n",
       " 'ZJL127': 30,\n",
       " 'ZJL128': 31,\n",
       " 'ZJL129': 32,\n",
       " 'ZJL13': 33,\n",
       " 'ZJL130': 34,\n",
       " 'ZJL131': 35,\n",
       " 'ZJL132': 36,\n",
       " 'ZJL133': 37,\n",
       " 'ZJL135': 38,\n",
       " 'ZJL137': 39,\n",
       " 'ZJL138': 40,\n",
       " 'ZJL139': 41,\n",
       " 'ZJL14': 42,\n",
       " 'ZJL140': 43,\n",
       " 'ZJL141': 44,\n",
       " 'ZJL142': 45,\n",
       " 'ZJL143': 46,\n",
       " 'ZJL144': 47,\n",
       " 'ZJL145': 48,\n",
       " 'ZJL146': 49,\n",
       " 'ZJL147': 50,\n",
       " 'ZJL149': 51,\n",
       " 'ZJL15': 52,\n",
       " 'ZJL150': 53,\n",
       " 'ZJL151': 54,\n",
       " 'ZJL152': 55,\n",
       " 'ZJL153': 56,\n",
       " 'ZJL154': 57,\n",
       " 'ZJL156': 58,\n",
       " 'ZJL157': 59,\n",
       " 'ZJL158': 60,\n",
       " 'ZJL159': 61,\n",
       " 'ZJL16': 62,\n",
       " 'ZJL160': 63,\n",
       " 'ZJL161': 64,\n",
       " 'ZJL162': 65,\n",
       " 'ZJL163': 66,\n",
       " 'ZJL164': 67,\n",
       " 'ZJL165': 68,\n",
       " 'ZJL166': 69,\n",
       " 'ZJL167': 70,\n",
       " 'ZJL168': 71,\n",
       " 'ZJL169': 72,\n",
       " 'ZJL170': 73,\n",
       " 'ZJL171': 74,\n",
       " 'ZJL172': 75,\n",
       " 'ZJL173': 76,\n",
       " 'ZJL174': 77,\n",
       " 'ZJL175': 78,\n",
       " 'ZJL176': 79,\n",
       " 'ZJL177': 80,\n",
       " 'ZJL178': 81,\n",
       " 'ZJL179': 82,\n",
       " 'ZJL18': 83,\n",
       " 'ZJL180': 84,\n",
       " 'ZJL181': 85,\n",
       " 'ZJL182': 86,\n",
       " 'ZJL183': 87,\n",
       " 'ZJL184': 88,\n",
       " 'ZJL185': 89,\n",
       " 'ZJL186': 90,\n",
       " 'ZJL187': 91,\n",
       " 'ZJL188': 92,\n",
       " 'ZJL189': 93,\n",
       " 'ZJL19': 94,\n",
       " 'ZJL190': 95,\n",
       " 'ZJL191': 96,\n",
       " 'ZJL192': 97,\n",
       " 'ZJL193': 98,\n",
       " 'ZJL194': 99,\n",
       " 'ZJL195': 100,\n",
       " 'ZJL196': 101,\n",
       " 'ZJL197': 102,\n",
       " 'ZJL198': 103,\n",
       " 'ZJL199': 104,\n",
       " 'ZJL2': 105,\n",
       " 'ZJL200': 106,\n",
       " 'ZJL201': 190,\n",
       " 'ZJL202': 191,\n",
       " 'ZJL203': 192,\n",
       " 'ZJL204': 193,\n",
       " 'ZJL205': 194,\n",
       " 'ZJL206': 195,\n",
       " 'ZJL207': 196,\n",
       " 'ZJL208': 197,\n",
       " 'ZJL209': 198,\n",
       " 'ZJL21': 107,\n",
       " 'ZJL210': 199,\n",
       " 'ZJL211': 200,\n",
       " 'ZJL212': 201,\n",
       " 'ZJL213': 202,\n",
       " 'ZJL214': 203,\n",
       " 'ZJL215': 204,\n",
       " 'ZJL216': 205,\n",
       " 'ZJL217': 206,\n",
       " 'ZJL218': 207,\n",
       " 'ZJL219': 208,\n",
       " 'ZJL22': 108,\n",
       " 'ZJL220': 209,\n",
       " 'ZJL221': 210,\n",
       " 'ZJL222': 211,\n",
       " 'ZJL223': 212,\n",
       " 'ZJL224': 213,\n",
       " 'ZJL225': 214,\n",
       " 'ZJL226': 215,\n",
       " 'ZJL227': 216,\n",
       " 'ZJL228': 217,\n",
       " 'ZJL229': 218,\n",
       " 'ZJL23': 109,\n",
       " 'ZJL230': 219,\n",
       " 'ZJL231': 220,\n",
       " 'ZJL232': 221,\n",
       " 'ZJL233': 222,\n",
       " 'ZJL234': 223,\n",
       " 'ZJL235': 224,\n",
       " 'ZJL236': 225,\n",
       " 'ZJL237': 226,\n",
       " 'ZJL238': 227,\n",
       " 'ZJL239': 228,\n",
       " 'ZJL24': 110,\n",
       " 'ZJL240': 229,\n",
       " 'ZJL25': 111,\n",
       " 'ZJL26': 112,\n",
       " 'ZJL28': 113,\n",
       " 'ZJL29': 114,\n",
       " 'ZJL3': 115,\n",
       " 'ZJL30': 116,\n",
       " 'ZJL31': 117,\n",
       " 'ZJL32': 118,\n",
       " 'ZJL34': 119,\n",
       " 'ZJL35': 120,\n",
       " 'ZJL36': 121,\n",
       " 'ZJL37': 122,\n",
       " 'ZJL38': 123,\n",
       " 'ZJL39': 124,\n",
       " 'ZJL4': 125,\n",
       " 'ZJL40': 126,\n",
       " 'ZJL41': 127,\n",
       " 'ZJL42': 128,\n",
       " 'ZJL43': 129,\n",
       " 'ZJL44': 130,\n",
       " 'ZJL45': 131,\n",
       " 'ZJL46': 132,\n",
       " 'ZJL47': 133,\n",
       " 'ZJL48': 134,\n",
       " 'ZJL49': 135,\n",
       " 'ZJL5': 136,\n",
       " 'ZJL50': 137,\n",
       " 'ZJL51': 138,\n",
       " 'ZJL52': 139,\n",
       " 'ZJL53': 140,\n",
       " 'ZJL54': 141,\n",
       " 'ZJL55': 142,\n",
       " 'ZJL56': 143,\n",
       " 'ZJL57': 144,\n",
       " 'ZJL58': 145,\n",
       " 'ZJL59': 146,\n",
       " 'ZJL6': 147,\n",
       " 'ZJL60': 148,\n",
       " 'ZJL61': 149,\n",
       " 'ZJL62': 150,\n",
       " 'ZJL63': 151,\n",
       " 'ZJL64': 152,\n",
       " 'ZJL65': 153,\n",
       " 'ZJL66': 154,\n",
       " 'ZJL67': 155,\n",
       " 'ZJL68': 156,\n",
       " 'ZJL69': 157,\n",
       " 'ZJL7': 158,\n",
       " 'ZJL70': 159,\n",
       " 'ZJL71': 160,\n",
       " 'ZJL72': 161,\n",
       " 'ZJL73': 162,\n",
       " 'ZJL75': 163,\n",
       " 'ZJL76': 164,\n",
       " 'ZJL77': 165,\n",
       " 'ZJL78': 166,\n",
       " 'ZJL79': 167,\n",
       " 'ZJL8': 168,\n",
       " 'ZJL80': 169,\n",
       " 'ZJL81': 170,\n",
       " 'ZJL82': 171,\n",
       " 'ZJL83': 172,\n",
       " 'ZJL84': 173,\n",
       " 'ZJL85': 174,\n",
       " 'ZJL86': 175,\n",
       " 'ZJL87': 176,\n",
       " 'ZJL88': 177,\n",
       " 'ZJL89': 178,\n",
       " 'ZJL9': 179,\n",
       " 'ZJL90': 180,\n",
       " 'ZJL91': 181,\n",
       " 'ZJL92': 182,\n",
       " 'ZJL93': 183,\n",
       " 'ZJL94': 184,\n",
       " 'ZJL95': 185,\n",
       " 'ZJL96': 186,\n",
       " 'ZJL97': 187,\n",
       " 'ZJL98': 188,\n",
       " 'ZJL99': 189}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_ids = class_id_emb_attr.class_id.values\n",
    "class_to_id = dict([(c, i) for i, c in enumerate(class_ids)])\n",
    "class_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:143: UserWarning: Output \"rela_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"rela_1\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_18 (InputLayer)            (230, 300)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNor (230, 300)            1200        input_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (230, 1024)           308224      batch_normalization_22[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "input_19 (InputLayer)            (230, 230)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "rela_0 (Lambda)                  (230, 1024)           0           dense_22[0][0]                   \n",
      "                                                                   input_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNor (230, 1024)           4096        rela_0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (230, 516)            528900      batch_normalization_23[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "rela_1 (Lambda)                  (230, 516)            0           dense_23[0][0]                   \n",
      "                                                                   input_19[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 842,420\n",
      "Trainable params: 839,772\n",
      "Non-trainable params: 2,648\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 30595 samples, validate on 7626 samples\n",
      "Epoch 1/150\n",
      "30496/30595 [============================>.] - ETA: 0s - loss: 2.0886\n",
      "All_re: \t0.028062\t214\t7626\n",
      "Seen_re: \t0.032157\t123\t3825\n",
      "Unseen_re: \t0.023941\t91\t3801\n",
      "30595/30595 [==============================] - 7s - loss: 2.0832 - val_loss: 0.4080\n",
      "Epoch 2/150\n",
      "30592/30595 [============================>.] - ETA: 0s - loss: 0.3956\n",
      "All_re: \t0.039995\t305\t7626\n",
      "Seen_re: \t0.041569\t159\t3825\n",
      "Unseen_re: \t0.038411\t146\t3801\n",
      "30595/30595 [==============================] - 7s - loss: 0.3956 - val_loss: 0.3841\n",
      "Epoch 3/150\n",
      "30592/30595 [============================>.] - ETA: 0s - loss: 0.3816\n",
      "All_re: \t0.042355\t323\t7626\n",
      "Seen_re: \t0.049673\t190\t3825\n",
      "Unseen_re: \t0.034991\t133\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.3816 - val_loss: 0.3749\n",
      "Epoch 4/150\n",
      "30592/30595 [============================>.] - ETA: 0s - loss: 0.3710\n",
      "All_re: \t0.043929\t335\t7626\n",
      "Seen_re: \t0.053856\t206\t3825\n",
      "Unseen_re: \t0.033938\t129\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.3710 - val_loss: 0.3643\n",
      "Epoch 5/150\n",
      "30528/30595 [============================>.] - ETA: 0s - loss: 0.3583\n",
      "All_re: \t0.042880\t327\t7626\n",
      "Seen_re: \t0.059608\t228\t3825\n",
      "Unseen_re: \t0.026046\t99\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.3582 - val_loss: 0.3514\n",
      "Epoch 6/150\n",
      "30400/30595 [============================>.] - ETA: 0s - loss: 0.3430\n",
      "All_re: \t0.050747\t387\t7626\n",
      "Seen_re: \t0.066928\t256\t3825\n",
      "Unseen_re: \t0.034465\t131\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.3430 - val_loss: 0.3364\n",
      "Epoch 7/150\n",
      "30592/30595 [============================>.] - ETA: 0s - loss: 0.3259\n",
      "All_re: \t0.053108\t405\t7626\n",
      "Seen_re: \t0.073987\t283\t3825\n",
      "Unseen_re: \t0.032097\t122\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.3259 - val_loss: 0.3190\n",
      "Epoch 8/150\n",
      "30432/30595 [============================>.] - ETA: 0s - loss: 0.3081\n",
      "All_re: \t0.052846\t403\t7626\n",
      "Seen_re: \t0.079216\t303\t3825\n",
      "Unseen_re: \t0.026309\t100\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.3081 - val_loss: 0.3029\n",
      "Epoch 9/150\n",
      "30368/30595 [============================>.] - ETA: 0s - loss: 0.2907\n",
      "All_re: \t0.058484\t446\t7626\n",
      "Seen_re: \t0.087582\t335\t3825\n",
      "Unseen_re: \t0.029203\t111\t3801\n",
      "30595/30595 [==============================] - 7s - loss: 0.2907 - val_loss: 0.2858\n",
      "Epoch 10/150\n",
      "30464/30595 [============================>.] - ETA: 0s - loss: 0.2746\n",
      "All_re: \t0.059927\t457\t7626\n",
      "Seen_re: \t0.086536\t331\t3825\n",
      "Unseen_re: \t0.033149\t126\t3801\n",
      "30595/30595 [==============================] - 7s - loss: 0.2745 - val_loss: 0.2697\n",
      "Epoch 11/150\n",
      "30560/30595 [============================>.] - ETA: 0s - loss: 0.2596\n",
      "All_re: \t0.055075\t420\t7626\n",
      "Seen_re: \t0.084444\t323\t3825\n",
      "Unseen_re: \t0.025520\t97\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.2596 - val_loss: 0.2570\n",
      "Epoch 12/150\n",
      "30304/30595 [============================>.] - ETA: 0s - loss: 0.2464\n",
      "All_re: \t0.061107\t466\t7626\n",
      "Seen_re: \t0.096732\t370\t3825\n",
      "Unseen_re: \t0.025257\t96\t3801\n",
      "30595/30595 [==============================] - 7s - loss: 0.2463 - val_loss: 0.2455\n",
      "Epoch 13/150\n",
      "30560/30595 [============================>.] - ETA: 0s - loss: 0.2346\n",
      "All_re: \t0.059927\t457\t7626\n",
      "Seen_re: \t0.093856\t359\t3825\n",
      "Unseen_re: \t0.025783\t98\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.2346 - val_loss: 0.2324\n",
      "Epoch 14/150\n",
      "30496/30595 [============================>.] - ETA: 0s - loss: 0.2243\n",
      "All_re: \t0.064385\t491\t7626\n",
      "Seen_re: \t0.101961\t390\t3825\n",
      "Unseen_re: \t0.026572\t101\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.2243 - val_loss: 0.2258\n",
      "Epoch 15/150\n",
      "30432/30595 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "All_re: \t0.059009\t450\t7626\n",
      "Seen_re: \t0.091503\t350\t3825\n",
      "Unseen_re: \t0.026309\t100\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.2155 - val_loss: 0.2149\n",
      "Epoch 16/150\n",
      "30240/30595 [============================>.] - ETA: 0s - loss: 0.2078\n",
      "All_re: \t0.058091\t443\t7626\n",
      "Seen_re: \t0.094902\t363\t3825\n",
      "Unseen_re: \t0.021047\t80\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.2077 - val_loss: 0.2095\n",
      "Epoch 17/150\n",
      "30368/30595 [============================>.] - ETA: 0s - loss: 0.2010\n",
      "All_re: \t0.061894\t472\t7626\n",
      "Seen_re: \t0.103791\t397\t3825\n",
      "Unseen_re: \t0.019732\t75\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.2010 - val_loss: 0.2034\n",
      "Epoch 18/150\n",
      "30432/30595 [============================>.] - ETA: 0s - loss: 0.1951\n",
      "All_re: \t0.059664\t455\t7626\n",
      "Seen_re: \t0.098301\t376\t3825\n",
      "Unseen_re: \t0.020784\t79\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1951 - val_loss: 0.1965\n",
      "Epoch 19/150\n",
      "30464/30595 [============================>.] - ETA: 0s - loss: 0.1899\n",
      "All_re: \t0.060189\t459\t7626\n",
      "Seen_re: \t0.100654\t385\t3825\n",
      "Unseen_re: \t0.019469\t74\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1899 - val_loss: 0.1938\n",
      "Epoch 20/150\n",
      "30560/30595 [============================>.] - ETA: 0s - loss: 0.1854\n",
      "All_re: \t0.061762\t471\t7626\n",
      "Seen_re: \t0.100915\t386\t3825\n",
      "Unseen_re: \t0.022363\t85\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1853 - val_loss: 0.1875\n",
      "Epoch 21/150\n",
      "30496/30595 [============================>.] - ETA: 0s - loss: 0.1814\n",
      "All_re: \t0.059795\t456\t7626\n",
      "Seen_re: \t0.097516\t373\t3825\n",
      "Unseen_re: \t0.021836\t83\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1813 - val_loss: 0.1833\n",
      "Epoch 22/150\n",
      "30464/30595 [============================>.] - ETA: 0s - loss: 0.1778\n",
      "All_re: \t0.060189\t459\t7626\n",
      "Seen_re: \t0.100915\t386\t3825\n",
      "Unseen_re: \t0.019205\t73\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1777 - val_loss: 0.1818\n",
      "Epoch 23/150\n",
      "30432/30595 [============================>.] - ETA: 0s - loss: 0.1746\n",
      "All_re: \t0.069106\t527\t7626\n",
      "Seen_re: \t0.113725\t435\t3825\n",
      "Unseen_re: \t0.024204\t92\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1746 - val_loss: 0.1783\n",
      "Epoch 24/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30592/30595 [============================>.] - ETA: 0s - loss: 0.1717\n",
      "All_re: \t0.064910\t495\t7626\n",
      "Seen_re: \t0.103529\t396\t3825\n",
      "Unseen_re: \t0.026046\t99\t3801\n",
      "30595/30595 [==============================] - 7s - loss: 0.1717 - val_loss: 0.1743\n",
      "Epoch 25/150\n",
      "30560/30595 [============================>.] - ETA: 0s - loss: 0.1693\n",
      "All_re: \t0.066221\t505\t7626\n",
      "Seen_re: \t0.107712\t412\t3825\n",
      "Unseen_re: \t0.024467\t93\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1692 - val_loss: 0.1727\n",
      "Epoch 26/150\n",
      "30496/30595 [============================>.] - ETA: 0s - loss: 0.1671\n",
      "All_re: \t0.061631\t470\t7626\n",
      "Seen_re: \t0.099869\t382\t3825\n",
      "Unseen_re: \t0.023152\t88\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1670 - val_loss: 0.1718\n",
      "Epoch 27/150\n",
      "30592/30595 [============================>.] - ETA: 0s - loss: 0.1651\n",
      "All_re: \t0.061107\t466\t7626\n",
      "Seen_re: \t0.106405\t407\t3825\n",
      "Unseen_re: \t0.015522\t59\t3801\n",
      "30595/30595 [==============================] - 7s - loss: 0.1651 - val_loss: 0.1690\n",
      "Epoch 28/150\n",
      "30464/30595 [============================>.] - ETA: 0s - loss: 0.1631\n",
      "All_re: \t0.063074\t481\t7626\n",
      "Seen_re: \t0.105621\t404\t3825\n",
      "Unseen_re: \t0.020258\t77\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1631 - val_loss: 0.1660\n",
      "Epoch 29/150\n",
      "30336/30595 [============================>.] - ETA: 0s - loss: 0.1615\n",
      "All_re: \t0.065303\t498\t7626\n",
      "Seen_re: \t0.110850\t424\t3825\n",
      "Unseen_re: \t0.019469\t74\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1615 - val_loss: 0.1656\n",
      "Epoch 30/150\n",
      "30592/30595 [============================>.] - ETA: 0s - loss: 0.1601\n",
      "All_re: \t0.061107\t466\t7626\n",
      "Seen_re: \t0.098562\t377\t3825\n",
      "Unseen_re: \t0.023415\t89\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1601 - val_loss: 0.1637\n",
      "Epoch 31/150\n",
      "30272/30595 [============================>.] - ETA: 0s - loss: 0.1589\n",
      "All_re: \t0.065434\t499\t7626\n",
      "Seen_re: \t0.105882\t405\t3825\n",
      "Unseen_re: \t0.024730\t94\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1589 - val_loss: 0.1626\n",
      "Epoch 32/150\n",
      "30496/30595 [============================>.] - ETA: 0s - loss: 0.1575\n",
      "All_re: \t0.063336\t483\t7626\n",
      "Seen_re: \t0.106405\t407\t3825\n",
      "Unseen_re: \t0.019995\t76\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1576 - val_loss: 0.1610\n",
      "Epoch 33/150\n",
      "30560/30595 [============================>.] - ETA: 0s - loss: 0.1564\n",
      "All_re: \t0.061894\t472\t7626\n",
      "Seen_re: \t0.103007\t394\t3825\n",
      "Unseen_re: \t0.020521\t78\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1564 - val_loss: 0.1608\n",
      "Epoch 34/150\n",
      "30592/30595 [============================>.] - ETA: 0s - loss: 0.1555\n",
      "All_re: \t0.065827\t502\t7626\n",
      "Seen_re: \t0.112418\t430\t3825\n",
      "Unseen_re: \t0.018942\t72\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1555 - val_loss: 0.1593\n",
      "Epoch 35/150\n",
      "30272/30595 [============================>.] - ETA: 0s - loss: 0.1546\n",
      "All_re: \t0.063598\t485\t7626\n",
      "Seen_re: \t0.106928\t409\t3825\n",
      "Unseen_re: \t0.019995\t76\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1547 - val_loss: 0.1587\n",
      "Epoch 36/150\n",
      "30304/30595 [============================>.] - ETA: 0s - loss: 0.1539\n",
      "All_re: \t0.063860\t487\t7626\n",
      "Seen_re: \t0.103268\t395\t3825\n",
      "Unseen_re: \t0.024204\t92\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1539 - val_loss: 0.1580\n",
      "Epoch 37/150\n",
      "30400/30595 [============================>.] - ETA: 0s - loss: 0.1532\n",
      "All_re: \t0.063992\t488\t7626\n",
      "Seen_re: \t0.109804\t420\t3825\n",
      "Unseen_re: \t0.017890\t68\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1531 - val_loss: 0.1561\n",
      "Epoch 38/150\n",
      "30528/30595 [============================>.] - ETA: 0s - loss: 0.1524\n",
      "All_re: \t0.065827\t502\t7626\n",
      "Seen_re: \t0.111634\t427\t3825\n",
      "Unseen_re: \t0.019732\t75\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1524 - val_loss: 0.1566\n",
      "Epoch 39/150\n",
      "30272/30595 [============================>.] - ETA: 0s - loss: 0.1519\n",
      "All_re: \t0.065172\t497\t7626\n",
      "Seen_re: \t0.107974\t413\t3825\n",
      "Unseen_re: \t0.022099\t84\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1519 - val_loss: 0.1559\n",
      "Epoch 40/150\n",
      "30336/30595 [============================>.] - ETA: 0s - loss: 0.1512\n",
      "All_re: \t0.066614\t508\t7626\n",
      "Seen_re: \t0.112680\t431\t3825\n",
      "Unseen_re: \t0.020258\t77\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1512 - val_loss: 0.1561\n",
      "Epoch 41/150\n",
      "30240/30595 [============================>.] - ETA: 0s - loss: 0.1508\n",
      "All_re: \t0.062549\t477\t7626\n",
      "Seen_re: \t0.107451\t411\t3825\n",
      "Unseen_re: \t0.017364\t66\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1508 - val_loss: 0.1555\n",
      "Epoch 42/150\n",
      "30464/30595 [============================>.] - ETA: 0s - loss: 0.1502\n",
      "All_re: \t0.069761\t532\t7626\n",
      "Seen_re: \t0.112418\t430\t3825\n",
      "Unseen_re: \t0.026835\t102\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1502 - val_loss: 0.1550\n",
      "Epoch 43/150\n",
      "30432/30595 [============================>.] - ETA: 0s - loss: 0.1498\n",
      "All_re: \t0.069237\t528\t7626\n",
      "Seen_re: \t0.114771\t439\t3825\n",
      "Unseen_re: \t0.023415\t89\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1499 - val_loss: 0.1546\n",
      "Epoch 44/150\n",
      "30496/30595 [============================>.] - ETA: 0s - loss: 0.1495\n",
      "All_re: \t0.066876\t510\t7626\n",
      "Seen_re: \t0.111373\t426\t3825\n",
      "Unseen_re: \t0.022099\t84\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1495 - val_loss: 0.1535\n",
      "Epoch 45/150\n",
      "30592/30595 [============================>.] - ETA: 0s - loss: 0.1491\n",
      "All_re: \t0.062811\t479\t7626\n",
      "Seen_re: \t0.108235\t414\t3825\n",
      "Unseen_re: \t0.017101\t65\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1491 - val_loss: 0.1531\n",
      "Epoch 46/150\n",
      "30400/30595 [============================>.] - ETA: 0s - loss: 0.1487\n",
      "All_re: \t0.064910\t495\t7626\n",
      "Seen_re: \t0.106667\t408\t3825\n",
      "Unseen_re: \t0.022889\t87\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1488 - val_loss: 0.1531\n",
      "Epoch 47/150\n",
      "30208/30595 [============================>.] - ETA: 0s - loss: 0.1484\n",
      "All_re: \t0.064385\t491\t7626\n",
      "Seen_re: \t0.108758\t416\t3825\n",
      "Unseen_re: \t0.019732\t75\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1484 - val_loss: 0.1535\n",
      "Epoch 48/150\n",
      "30336/30595 [============================>.] - ETA: 0s - loss: 0.1480\n",
      "All_re: \t0.068712\t524\t7626\n",
      "Seen_re: \t0.115294\t441\t3825\n",
      "Unseen_re: \t0.021836\t83\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1481 - val_loss: 0.1533\n",
      "Epoch 49/150\n",
      "30272/30595 [============================>.] - ETA: 0s - loss: 0.1478\n",
      "All_re: \t0.065041\t496\t7626\n",
      "Seen_re: \t0.108758\t416\t3825\n",
      "Unseen_re: \t0.021047\t80\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1478 - val_loss: 0.1522\n",
      "Epoch 50/150\n",
      "30592/30595 [============================>.] - ETA: 0s - loss: 0.1475\n",
      "All_re: \t0.066614\t508\t7626\n",
      "Seen_re: \t0.115294\t441\t3825\n",
      "Unseen_re: \t0.017627\t67\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1475 - val_loss: 0.1521\n",
      "Epoch 51/150\n",
      "30560/30595 [============================>.] - ETA: 0s - loss: 0.1474\n",
      "All_re: \t0.065565\t500\t7626\n",
      "Seen_re: \t0.112157\t429\t3825\n",
      "Unseen_re: \t0.018679\t71\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1474 - val_loss: 0.1518\n",
      "Epoch 52/150\n",
      "30304/30595 [============================>.] - ETA: 0s - loss: 0.1470\n",
      "All_re: \t0.066876\t510\t7626\n",
      "Seen_re: \t0.116340\t445\t3825\n",
      "Unseen_re: \t0.017101\t65\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1471 - val_loss: 0.1535\n",
      "Epoch 53/150\n",
      "30304/30595 [============================>.] - ETA: 0s - loss: 0.1470\n",
      "All_re: \t0.067270\t513\t7626\n",
      "Seen_re: \t0.111634\t427\t3825\n",
      "Unseen_re: \t0.022626\t86\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1470 - val_loss: 0.1517\n",
      "Epoch 54/150\n",
      "30304/30595 [============================>.] - ETA: 0s - loss: 0.1467\n",
      "All_re: \t0.067008\t511\t7626\n",
      "Seen_re: \t0.114510\t438\t3825\n",
      "Unseen_re: \t0.019205\t73\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1467 - val_loss: 0.1509\n",
      "Epoch 55/150\n",
      "30464/30595 [============================>.] - ETA: 0s - loss: 0.1465\n",
      "All_re: \t0.066483\t507\t7626\n",
      "Seen_re: \t0.109804\t420\t3825\n",
      "Unseen_re: \t0.022889\t87\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1464 - val_loss: 0.1509\n",
      "Epoch 56/150\n",
      "30208/30595 [============================>.] - ETA: 0s - loss: 0.1463\n",
      "All_re: \t0.067139\t512\t7626\n",
      "Seen_re: \t0.119216\t456\t3825\n",
      "Unseen_re: \t0.014733\t56\t3801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30595/30595 [==============================] - 6s - loss: 0.1463 - val_loss: 0.1510\n",
      "Epoch 57/150\n",
      "30240/30595 [============================>.] - ETA: 0s - loss: 0.1461\n",
      "All_re: \t0.068319\t521\t7626\n",
      "Seen_re: \t0.117647\t450\t3825\n",
      "Unseen_re: \t0.018679\t71\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1461 - val_loss: 0.1507\n",
      "Epoch 58/150\n",
      "30368/30595 [============================>.] - ETA: 0s - loss: 0.1460\n",
      "All_re: \t0.068712\t524\t7626\n",
      "Seen_re: \t0.115817\t443\t3825\n",
      "Unseen_re: \t0.021310\t81\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1460 - val_loss: 0.1509\n",
      "Epoch 59/150\n",
      "30528/30595 [============================>.] - ETA: 0s - loss: 0.1458\n",
      "All_re: \t0.066352\t506\t7626\n",
      "Seen_re: \t0.113203\t433\t3825\n",
      "Unseen_re: \t0.019205\t73\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1459 - val_loss: 0.1500\n",
      "Epoch 60/150\n",
      "30368/30595 [============================>.] - ETA: 0s - loss: 0.1457\n",
      "All_re: \t0.068450\t522\t7626\n",
      "Seen_re: \t0.116601\t446\t3825\n",
      "Unseen_re: \t0.019995\t76\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1457 - val_loss: 0.1497\n",
      "Epoch 61/150\n",
      "30240/30595 [============================>.] - ETA: 0s - loss: 0.1456\n",
      "All_re: \t0.064778\t494\t7626\n",
      "Seen_re: \t0.113464\t434\t3825\n",
      "Unseen_re: \t0.015785\t60\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1456 - val_loss: 0.1496\n",
      "Epoch 62/150\n",
      "30496/30595 [============================>.] - ETA: 0s - loss: 0.1454\n",
      "All_re: \t0.067532\t515\t7626\n",
      "Seen_re: \t0.114248\t437\t3825\n",
      "Unseen_re: \t0.020521\t78\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1454 - val_loss: 0.1497\n",
      "Epoch 63/150\n",
      "30240/30595 [============================>.] - ETA: 0s - loss: 0.1453\n",
      "All_re: \t0.065827\t502\t7626\n",
      "Seen_re: \t0.113464\t434\t3825\n",
      "Unseen_re: \t0.017890\t68\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1453 - val_loss: 0.1490\n",
      "Epoch 64/150\n",
      "30400/30595 [============================>.] - ETA: 0s - loss: 0.1451\n",
      "All_re: \t0.065696\t501\t7626\n",
      "Seen_re: \t0.112941\t432\t3825\n",
      "Unseen_re: \t0.018153\t69\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1451 - val_loss: 0.1499\n",
      "Epoch 65/150\n",
      "30496/30595 [============================>.] - ETA: 0s - loss: 0.1451\n",
      "All_re: \t0.066876\t510\t7626\n",
      "Seen_re: \t0.114771\t439\t3825\n",
      "Unseen_re: \t0.018679\t71\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1451 - val_loss: 0.1496\n",
      "Epoch 66/150\n",
      "30272/30595 [============================>.] - ETA: 0s - loss: 0.1449\n",
      "All_re: \t0.067532\t515\t7626\n",
      "Seen_re: \t0.115817\t443\t3825\n",
      "Unseen_re: \t0.018942\t72\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1449 - val_loss: 0.1496\n",
      "Epoch 67/150\n",
      "30336/30595 [============================>.] - ETA: 0s - loss: 0.1447\n",
      "All_re: \t0.064778\t494\t7626\n",
      "Seen_re: \t0.109804\t420\t3825\n",
      "Unseen_re: \t0.019469\t74\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1448 - val_loss: 0.1501\n",
      "Epoch 68/150\n",
      "30528/30595 [============================>.] - ETA: 0s - loss: 0.1447\n",
      "All_re: \t0.068450\t522\t7626\n",
      "Seen_re: \t0.121569\t465\t3825\n",
      "Unseen_re: \t0.014996\t57\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1447 - val_loss: 0.1495\n",
      "Epoch 69/150\n",
      "30272/30595 [============================>.] - ETA: 0s - loss: 0.1446\n",
      "All_re: \t0.065303\t498\t7626\n",
      "Seen_re: \t0.112157\t429\t3825\n",
      "Unseen_re: \t0.018153\t69\t3801\n",
      "30595/30595 [==============================] - 6s - loss: 0.1446 - val_loss: 0.1493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7080569668>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "# adj_graph = sklearn.metrics.pairwise.pairwise_distances(attr, metric = 'manhattan')\n",
    "\n",
    "def find_nearest_class(cand_class_id_emb_attr, model, eval_df, img_feature_map = None, \n",
    "                       threshold = None, gamma = None, seen_class = None):\n",
    "    if img_feature_map is None:\n",
    "        img_feature_map = np.reshape(vgg_model.predict(np.asarray(list(eval_df['img'])), verbose = 1), (eval_img.shape[0], -1))\n",
    "#     cand_feature_map = pd.DataFrame(list(model.predict(None, steps = 1)), columns = ['feature_map'], \\\n",
    "#                                     index = class_id_emb_attr.class_id).reset_index()\n",
    "    # img_feature_map = np.repeat(img_feature_map, [cand_feature_map.shape[0]] * img_feature_map.shape[0], axis = 0)\n",
    "    nearest_class_id = ['ZJL'] * eval_df.shape[0]\n",
    "#     seen_class_bias = np.zeros(class_id_emb_attr.shape[0])\n",
    "#     seen_class_bias[class_id_emb_attr.class_id.isin(seen_class)] = gamma\n",
    "#     cand_mask = np.zeros(class_id_emb_attr.shape[0])\n",
    "#     cand_mask[~class_id_emb_attr.class_id.isin(seen_class)] = 1\n",
    "#     cand_feature_map = cand_feature_map[cand_feature_map.class_id.isin()]\n",
    "    \n",
    "#     cand_class_id_emb_attr = cand_class_id_emb_attr.merge(cand_feature_map, how = 'left', on = 'class_id')\n",
    "#     print (cand_class_id_emb_attr)\n",
    "    cand_class_to_id = [class_to_id[c] for c in cand_class_id_emb_attr.class_id.values]\n",
    "    cand_feature_map = model.predict(None, steps = 1)[cand_class_to_id]\n",
    "#     print (cand_feature_map)\n",
    "#     seen_indice = [category_dict[c] for c in seen_class]\n",
    "#     preds = np.asarray(list(eval_df['preds']))[:, seen_indice]\n",
    "    for i in range(img_feature_map.shape[0]):\n",
    "        if False: #np.amax(preds[i]) > threshold:\n",
    "            nearest_class_id[i] = seen_class[np.argmax(preds[i])]\n",
    "        else:\n",
    "            img = img_feature_map[i]\n",
    "#             dis = 1 - sklearn.metrics.pairwise.cosine_similarity([img], \n",
    "#                                     cand_feature_map).reshape((cand_feature_map.shape[0]))\n",
    "            dis = np.linalg.norm(img - cand_feature_map, axis = 1)\n",
    "#             print (dis.shape)\n",
    "#             print (dis.shape)\n",
    "#             if np.amin(dis[class_id_emb_attr.class_id.isin(seen_class)]) > \\\n",
    "#                     gamma * np.amin(dis[~class_id_emb_attr.class_id.isin(seen_class)]):\n",
    "#                 dis[class_id_emb_attr.class_id.isin(seen_class)] = 200\n",
    "#             dis += seen_class_bias\n",
    "#             print(dis)\n",
    "#             plt.hist(dis)\n",
    "#             return\n",
    "            min_ind = np.where(dis == np.amin(dis))[0]\n",
    "#             if len(min_ind) > 1:\n",
    "#                 print ('eval img id: ', eval_df.iloc[i]['class_id'], 'has multiple best candidates: ', len(min_ind), 'min val: ', np.amin(dis))\n",
    "    #         print (i, img, class_id_emb_attr.iloc[min_ind[0]].name)\n",
    "            nearest_class_id[i] = cand_class_id_emb_attr.iloc[min_ind[0]]['class_id']\n",
    "#     print (nearest_class_id)\n",
    "    return np.asarray(nearest_class_id)\n",
    "\n",
    "class AccuracyEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1, batch_interval = 1000000, verbose = 2, \\\n",
    "            scores = [], class_id_emb_attr = None, eval_df = None, threshold = None, \\\n",
    "                 seen_class = None, unseen_class = None, gamma = None):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        # print(\"y_val shape:{0}\".format(self.y_val.shape))\n",
    "        self.batch_interval = batch_interval\n",
    "        self.verbose = verbose\n",
    "        self.scores = scores\n",
    "        self.class_id_emb_attr = class_id_emb_attr\n",
    "        self.eval_df = eval_df\n",
    "        self.threshold = threshold\n",
    "        self.seen_class = seen_class\n",
    "        self.unseen_class = unseen_class\n",
    "        self.gamma = gamma\n",
    "#         seen_indice = [category_dict[c] for c in seen_class]\n",
    "#         self.preds = np.asarray(list(eval_df['preds']))[:, seen_indice]\n",
    "#         self.img_feature_map = self.y_val\n",
    "    \n",
    "    def calc_accuracy(self, eval_df, eval_class, preds):\n",
    "        eval_mask = eval_df.class_id.isin(eval_class)\n",
    "        eval_num = np.sum(eval_mask)\n",
    "        right_num = np.sum(preds[eval_mask] == eval_df.class_id[eval_mask])\n",
    "        return right_num / np.sum(eval_mask), right_num, eval_num\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            zs_model = Model(inputs = self.model.inputs[2:], outputs = self.model.outputs[0])\n",
    "            pred_nearest_class_id = find_nearest_class(self.class_id_emb_attr, zs_model, \n",
    "                                self.eval_df, self.y_val, self.threshold, self.gamma, self.seen_class)\n",
    "            all_re = self.calc_accuracy(self.eval_df, self.eval_df['class_id'].values, pred_nearest_class_id)\n",
    "            seen_re = self.calc_accuracy(self.eval_df, self.seen_class, pred_nearest_class_id)\n",
    "            unseen_re = self.calc_accuracy(self.eval_df, self.unseen_class, pred_nearest_class_id)\n",
    "#             true_class_id = self.eval_df['class_id'].values\n",
    "# #             print (pred_nearest_class_id, \"\\n\", true_class_id)\n",
    "#             right_num = np.sum(pred_nearest_class_id == true_class_id)\n",
    "#             score = right_num / true_class_id.shape[0]\n",
    "#             self.scores.append(\"epoch:{0} {1}\".format(epoch + 1, score))\n",
    "            print(\"\\nAll_re: \\t%.6f\\t%.0f\\t%.0f\" % all_re)\n",
    "            print(\"Seen_re: \\t%.6f\\t%.0f\\t%.0f\" % seen_re)\n",
    "            print(\"Unseen_re: \\t%.6f\\t%.0f\\t%.0f\" % unseen_re)\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "        if(self.verbose >= 2) and (batch % self.batch_interval == 0):\n",
    "            # y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            # loss = metrics.log_loss(self.y_val, y_pred)\n",
    "            print(\"Hi! on_batch_end() , batch=\",batch,\",logs:\",logs)\n",
    "            # print(\"Valide size=\",y_pred.shape[0], \"  Valide loss=\",loss)\n",
    "\n",
    "def full_connect_layer(input, hidden_dim, activation, adj_graphs):\n",
    "    full_connect = input\n",
    "    for i, hn in enumerate(hidden_dim):\n",
    "        fc_in = full_connect\n",
    "        # if self.full_connect_dropout > 0:\n",
    "#         full_connect = Dropout(0.2)(full_connect)\n",
    "        full_connect = BatchNormalization(epsilon=1.001e-5)(full_connect)\n",
    "        # full_connect = Dense(hn, kernel_regularizer = l2(0.001), activity_regularizer = l1(0.001))(full_connect)\n",
    "#         full_connect = Concatenate()([Dense(hn, kernel_initializer='lecun_uniform', activation = 'relu')(full_connect), \n",
    "#             Dense(hn, kernel_initializer='lecun_uniform', activation = 'sigmoid')(full_connect)])\n",
    "        full_connect = Dense(hn, kernel_initializer='he_normal', kernel_regularizer = l2(1e-4), activation = activation)(full_connect)\n",
    "#         full_connect = LeakyReLU(alpha=0.02)(full_connect)\n",
    "        # full_connect = self.act_blend(full_connect)\n",
    "        # if self.full_connect_dropout > 0:\n",
    "#             full_connect = Dropout(self.full_connect_dropout)(full_connect) #Dropout(self.full_connect_dropout)(full_connect)\n",
    "#         full_connect = Concatenate()([fc_in, full_connect])\n",
    "        full_connect = Lambda(lambda x: K.dot(x[1], x[0]), name = 'rela_' + str(i))([full_connect, adj_graphs])\n",
    "    return full_connect\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def create_gcn():\n",
    "    alpha = 0.03\n",
    "#     attr_input = Input(shape = (30,), name = 'attr')\n",
    "    all_word_emb = Input(tensor=tf.constant(np.array(list(class_id_emb_attr['emb']), dtype = 'float32'))) #Input(shape = (230, 300,), name = 'wv')\n",
    "    class_index = Input(shape = (1, ), name = 'class_index', dtype = 'int32')\n",
    "    adj_graphs = Input(tensor=tf.constant(adj_graph, dtype = 'float32')) #Input(shape = (230, 230,), name = 'adj_graph')\n",
    "    imag_classifier = Input(shape = (516,), name = 'img')\n",
    "    \n",
    "#     x = Lambda(lambda xx: all_word_emb)(class_index)\n",
    "#     x = Dense(516, kernel_initializer='he_normal', kernel_regularizer = l2(1e-4), \n",
    "#               activation = 'relu', name = 'conv')(all_word_emb)\n",
    "#     all_classifier = Lambda(lambda x: K.dot(x[1], x[0]), name = 'rela')([x, adj_graphs])\n",
    "    all_classifier = full_connect_layer(all_word_emb, hidden_dim = [1024, 516], \n",
    "                                             activation = 'relu', adj_graphs = adj_graphs)\n",
    "    x = tf.gather_nd(all_classifier, class_index)\n",
    "\n",
    "    mse_loss = K.mean(keras.losses.mean_squared_error(imag_classifier, x))\n",
    "    \n",
    "    model = Model([class_index, imag_classifier, all_word_emb, adj_graphs], outputs = [all_classifier]) #, vgg_output])\n",
    "    model.add_loss(mse_loss)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=None)\n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "train_part_data = np.array([class_to_id[c] for c in train_part_df['class_id'].values]).astype('int32')\n",
    "# train_part_data = np.c_[train_part_data, np.zeros(train_part_data.shape[0])]\n",
    "validate_part_data = np.array([class_to_id[c] for c in validate_part_df['class_id'].values]).astype('int32')\n",
    "\n",
    "# train_all_word_emb = np.array(list(class_id_emb_attr['emb']) * train_part_data.shape[0])\n",
    "# validate_all_word_emb = np.array(list(class_id_emb_attr['emb']) * validate_part_data.shape[0])\n",
    "\n",
    "callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n",
    "        AccuracyEvaluation(validation_data=(validate_part_data,  validate_part_target), interval=1, \\\n",
    "                        class_id_emb_attr = class_id_emb_attr, \\\n",
    "                           eval_df = validate_part_df, threshold= 0.3, \\\n",
    "                          seen_class = seen_class, unseen_class = unseen_class, \\\n",
    "#             class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \\\n",
    "                          gamma = 0.8)\n",
    "        ]\n",
    "\n",
    "zs_model = create_gcn()\n",
    "print (zs_model.summary())\n",
    "zs_model.fit([train_part_data, train_part_target],  \n",
    "             validation_data = ([validate_part_data, validate_part_target], None),\n",
    "              epochs=150, batch_size = BATCH_SIZE, shuffle=True, verbose = 1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 71, 71, 71], dtype=int32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_part_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19437]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(adj[0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(path + '/DatasetA_test_20180813/DatasetA_test/image.txt', header = None, names = ['img_id'])\n",
    "imag_path = path + '/DatasetA_test_20180813/DatasetA_test/test/'\n",
    "test_data['img'] = test_data['img_id'].apply(lambda id: read_image(imag_path, id))\n",
    "# with open(path + 'test_data.pickle', 'rb') as handle:\n",
    "#     test_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.12423625, 0.1190791, 0.7566846]\n",
       "1       [1.7260123e-13, 2.1865514e-08, 1.0]\n",
       "2       [0.2117803, 0.17989996, 0.60831976]\n",
       "3        [0.2420589, 0.17926142, 0.5786797]\n",
       "4    [0.041276723, 0.95284086, 0.005882429]\n",
       "dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(path + 'test_data.pickle', 'wb+') as handle:\n",
    "#     pickle.dump(test_data, handle)\n",
    "# test_data.head()\n",
    "with open(path + '/model_sub/stacking_train_label_2018_09_12_12_10_06.pickle', 'rb') as handle:\n",
    "    stacking_train_data = pickle.load(handle)\n",
    "stacking_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_17 (InputLayer)            (None, 64, 64, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D) (None, 70, 70, 3)     0           input_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)              (None, 68, 68, 64)    1728        zero_padding2d_3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)    (None, 68, 68, 64)    256         conv1/conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1/relu (Activation)          (None, 68, 68, 64)    0           conv1/bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D) (None, 70, 70, 64)    0           conv1/relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (AveragePooling2D)         (None, 34, 34, 64)    0           zero_padding2d_4[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormaliz (None, 34, 34, 64)    256         pool1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation) (None, 34, 34, 64)    0           conv2_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)     (None, 34, 34, 64)    4096        conv2_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenate (None, 34, 34, 80)    0           pool1[0][0]                      \n",
      "                                                                   conv2_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormaliz (None, 34, 34, 80)    320         conv2_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation) (None, 34, 34, 80)    0           conv2_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)     (None, 34, 34, 64)    5120        conv2_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenate (None, 34, 34, 96)    0           conv2_block1_concat[0][0]        \n",
      "                                                                   conv2_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormaliz (None, 34, 34, 96)    384         conv2_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation) (None, 34, 34, 96)    0           conv2_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)     (None, 34, 34, 64)    6144        conv2_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenate (None, 34, 34, 112)   0           conv2_block2_concat[0][0]        \n",
      "                                                                   conv2_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormaliz (None, 34, 34, 112)   448         conv2_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation) (None, 34, 34, 112)   0           conv2_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)     (None, 34, 34, 64)    7168        conv2_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenate (None, 34, 34, 128)   0           conv2_block3_concat[0][0]        \n",
      "                                                                   conv2_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormaliz (None, 34, 34, 128)   512         conv2_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation) (None, 34, 34, 128)   0           conv2_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)     (None, 34, 34, 64)    8192        conv2_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenate (None, 34, 34, 144)   0           conv2_block4_concat[0][0]        \n",
      "                                                                   conv2_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormaliz (None, 34, 34, 144)   576         conv2_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation) (None, 34, 34, 144)   0           conv2_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)     (None, 34, 34, 64)    9216        conv2_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenate (None, 34, 34, 160)   0           conv2_block5_concat[0][0]        \n",
      "                                                                   conv2_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)    (None, 34, 34, 160)   640         conv2_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "pool2_relu (Activation)          (None, 34, 34, 160)   0           pool2_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)              (None, 34, 34, 80)    12800       pool2_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)    (None, 17, 17, 80)    0           pool2_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormaliz (None, 17, 17, 80)    320         pool2_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation) (None, 17, 17, 80)    0           conv3_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)     (None, 17, 17, 64)    5120        conv3_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenate (None, 17, 17, 96)    0           pool2_pool[0][0]                 \n",
      "                                                                   conv3_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormaliz (None, 17, 17, 96)    384         conv3_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation) (None, 17, 17, 96)    0           conv3_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)     (None, 17, 17, 64)    6144        conv3_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenate (None, 17, 17, 112)   0           conv3_block1_concat[0][0]        \n",
      "                                                                   conv3_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormaliz (None, 17, 17, 112)   448         conv3_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation) (None, 17, 17, 112)   0           conv3_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)     (None, 17, 17, 64)    7168        conv3_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenate (None, 17, 17, 128)   0           conv3_block2_concat[0][0]        \n",
      "                                                                   conv3_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormaliz (None, 17, 17, 128)   512         conv3_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation) (None, 17, 17, 128)   0           conv3_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)     (None, 17, 17, 64)    8192        conv3_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenate (None, 17, 17, 144)   0           conv3_block3_concat[0][0]        \n",
      "                                                                   conv3_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormaliz (None, 17, 17, 144)   576         conv3_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation) (None, 17, 17, 144)   0           conv3_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)     (None, 17, 17, 64)    9216        conv3_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenate (None, 17, 17, 160)   0           conv3_block4_concat[0][0]        \n",
      "                                                                   conv3_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormaliz (None, 17, 17, 160)   640         conv3_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation) (None, 17, 17, 160)   0           conv3_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)     (None, 17, 17, 64)    10240       conv3_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenate (None, 17, 17, 176)   0           conv3_block5_concat[0][0]        \n",
      "                                                                   conv3_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormaliz (None, 17, 17, 176)   704         conv3_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation) (None, 17, 17, 176)   0           conv3_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)     (None, 17, 17, 64)    11264       conv3_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenate (None, 17, 17, 192)   0           conv3_block6_concat[0][0]        \n",
      "                                                                   conv3_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormaliz (None, 17, 17, 192)   768         conv3_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation) (None, 17, 17, 192)   0           conv3_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)     (None, 17, 17, 64)    12288       conv3_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenate (None, 17, 17, 208)   0           conv3_block7_concat[0][0]        \n",
      "                                                                   conv3_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormaliz (None, 17, 17, 208)   832         conv3_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation) (None, 17, 17, 208)   0           conv3_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)     (None, 17, 17, 64)    13312       conv3_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenate (None, 17, 17, 224)   0           conv3_block8_concat[0][0]        \n",
      "                                                                   conv3_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormali (None, 17, 17, 224)   896         conv3_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activation (None, 17, 17, 224)   0           conv3_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)    (None, 17, 17, 64)    14336       conv3_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activation (None, 17, 17, 64)    0           conv3_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatenat (None, 17, 17, 240)   0           conv3_block9_concat[0][0]        \n",
      "                                                                   conv3_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormali (None, 17, 17, 240)   960         conv3_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activation (None, 17, 17, 240)   0           conv3_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)    (None, 17, 17, 64)    15360       conv3_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activation (None, 17, 17, 64)    0           conv3_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatenat (None, 17, 17, 256)   0           conv3_block10_concat[0][0]       \n",
      "                                                                   conv3_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormali (None, 17, 17, 256)   1024        conv3_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activation (None, 17, 17, 256)   0           conv3_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)    (None, 17, 17, 64)    16384       conv3_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activation (None, 17, 17, 64)    0           conv3_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatenat (None, 17, 17, 272)   0           conv3_block11_concat[0][0]       \n",
      "                                                                   conv3_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)    (None, 17, 17, 272)   1088        conv3_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool3_relu (Activation)          (None, 17, 17, 272)   0           pool3_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)              (None, 17, 17, 136)   36992       pool3_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)    (None, 8, 8, 136)     0           pool3_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormaliz (None, 8, 8, 136)     544         pool3_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation) (None, 8, 8, 136)     0           conv4_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)     (None, 8, 8, 64)      8704        conv4_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenate (None, 8, 8, 152)     0           pool3_pool[0][0]                 \n",
      "                                                                   conv4_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormaliz (None, 8, 8, 152)     608         conv4_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation) (None, 8, 8, 152)     0           conv4_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)     (None, 8, 8, 64)      9728        conv4_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenate (None, 8, 8, 168)     0           conv4_block1_concat[0][0]        \n",
      "                                                                   conv4_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormaliz (None, 8, 8, 168)     672         conv4_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation) (None, 8, 8, 168)     0           conv4_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)     (None, 8, 8, 64)      10752       conv4_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenate (None, 8, 8, 184)     0           conv4_block2_concat[0][0]        \n",
      "                                                                   conv4_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormaliz (None, 8, 8, 184)     736         conv4_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation) (None, 8, 8, 184)     0           conv4_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)     (None, 8, 8, 64)      11776       conv4_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenate (None, 8, 8, 200)     0           conv4_block3_concat[0][0]        \n",
      "                                                                   conv4_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormaliz (None, 8, 8, 200)     800         conv4_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation) (None, 8, 8, 200)     0           conv4_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)     (None, 8, 8, 64)      12800       conv4_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenate (None, 8, 8, 216)     0           conv4_block4_concat[0][0]        \n",
      "                                                                   conv4_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormaliz (None, 8, 8, 216)     864         conv4_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation) (None, 8, 8, 216)     0           conv4_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)     (None, 8, 8, 64)      13824       conv4_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenate (None, 8, 8, 232)     0           conv4_block5_concat[0][0]        \n",
      "                                                                   conv4_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormaliz (None, 8, 8, 232)     928         conv4_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation) (None, 8, 8, 232)     0           conv4_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)     (None, 8, 8, 64)      14848       conv4_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenate (None, 8, 8, 248)     0           conv4_block6_concat[0][0]        \n",
      "                                                                   conv4_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormaliz (None, 8, 8, 248)     992         conv4_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation) (None, 8, 8, 248)     0           conv4_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)     (None, 8, 8, 64)      15872       conv4_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenate (None, 8, 8, 264)     0           conv4_block7_concat[0][0]        \n",
      "                                                                   conv4_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormaliz (None, 8, 8, 264)     1056        conv4_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation) (None, 8, 8, 264)     0           conv4_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)     (None, 8, 8, 64)      16896       conv4_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenate (None, 8, 8, 280)     0           conv4_block8_concat[0][0]        \n",
      "                                                                   conv4_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormali (None, 8, 8, 280)     1120        conv4_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activation (None, 8, 8, 280)     0           conv4_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)    (None, 8, 8, 64)      17920       conv4_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activation (None, 8, 8, 64)      0           conv4_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatenat (None, 8, 8, 296)     0           conv4_block9_concat[0][0]        \n",
      "                                                                   conv4_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormali (None, 8, 8, 296)     1184        conv4_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activation (None, 8, 8, 296)     0           conv4_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)    (None, 8, 8, 64)      18944       conv4_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activation (None, 8, 8, 64)      0           conv4_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatenat (None, 8, 8, 312)     0           conv4_block10_concat[0][0]       \n",
      "                                                                   conv4_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormali (None, 8, 8, 312)     1248        conv4_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activation (None, 8, 8, 312)     0           conv4_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)    (None, 8, 8, 64)      19968       conv4_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activation (None, 8, 8, 64)      0           conv4_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatenat (None, 8, 8, 328)     0           conv4_block11_concat[0][0]       \n",
      "                                                                   conv4_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormali (None, 8, 8, 328)     1312        conv4_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activation (None, 8, 8, 328)     0           conv4_block13_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)    (None, 8, 8, 64)      20992       conv4_block13_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block13_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activation (None, 8, 8, 64)      0           conv4_block13_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block13_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatenat (None, 8, 8, 344)     0           conv4_block12_concat[0][0]       \n",
      "                                                                   conv4_block13_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormali (None, 8, 8, 344)     1376        conv4_block13_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activation (None, 8, 8, 344)     0           conv4_block14_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)    (None, 8, 8, 64)      22016       conv4_block14_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block14_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activation (None, 8, 8, 64)      0           conv4_block14_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block14_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatenat (None, 8, 8, 360)     0           conv4_block13_concat[0][0]       \n",
      "                                                                   conv4_block14_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormali (None, 8, 8, 360)     1440        conv4_block14_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activation (None, 8, 8, 360)     0           conv4_block15_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)    (None, 8, 8, 64)      23040       conv4_block15_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block15_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activation (None, 8, 8, 64)      0           conv4_block15_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block15_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatenat (None, 8, 8, 376)     0           conv4_block14_concat[0][0]       \n",
      "                                                                   conv4_block15_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormali (None, 8, 8, 376)     1504        conv4_block15_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activation (None, 8, 8, 376)     0           conv4_block16_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)    (None, 8, 8, 64)      24064       conv4_block16_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block16_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activation (None, 8, 8, 64)      0           conv4_block16_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block16_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatenat (None, 8, 8, 392)     0           conv4_block15_concat[0][0]       \n",
      "                                                                   conv4_block16_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormali (None, 8, 8, 392)     1568        conv4_block16_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activation (None, 8, 8, 392)     0           conv4_block17_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)    (None, 8, 8, 64)      25088       conv4_block17_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block17_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activation (None, 8, 8, 64)      0           conv4_block17_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block17_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatenat (None, 8, 8, 408)     0           conv4_block16_concat[0][0]       \n",
      "                                                                   conv4_block17_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormali (None, 8, 8, 408)     1632        conv4_block17_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activation (None, 8, 8, 408)     0           conv4_block18_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)    (None, 8, 8, 64)      26112       conv4_block18_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block18_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activation (None, 8, 8, 64)      0           conv4_block18_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block18_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatenat (None, 8, 8, 424)     0           conv4_block17_concat[0][0]       \n",
      "                                                                   conv4_block18_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormali (None, 8, 8, 424)     1696        conv4_block18_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activation (None, 8, 8, 424)     0           conv4_block19_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)    (None, 8, 8, 64)      27136       conv4_block19_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block19_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activation (None, 8, 8, 64)      0           conv4_block19_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block19_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatenat (None, 8, 8, 440)     0           conv4_block18_concat[0][0]       \n",
      "                                                                   conv4_block19_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormali (None, 8, 8, 440)     1760        conv4_block19_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activation (None, 8, 8, 440)     0           conv4_block20_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)    (None, 8, 8, 64)      28160       conv4_block20_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block20_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activation (None, 8, 8, 64)      0           conv4_block20_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block20_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatenat (None, 8, 8, 456)     0           conv4_block19_concat[0][0]       \n",
      "                                                                   conv4_block20_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormali (None, 8, 8, 456)     1824        conv4_block20_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activation (None, 8, 8, 456)     0           conv4_block21_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)    (None, 8, 8, 64)      29184       conv4_block21_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block21_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activation (None, 8, 8, 64)      0           conv4_block21_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block21_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatenat (None, 8, 8, 472)     0           conv4_block20_concat[0][0]       \n",
      "                                                                   conv4_block21_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormali (None, 8, 8, 472)     1888        conv4_block21_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activation (None, 8, 8, 472)     0           conv4_block22_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)    (None, 8, 8, 64)      30208       conv4_block22_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block22_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activation (None, 8, 8, 64)      0           conv4_block22_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block22_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatenat (None, 8, 8, 488)     0           conv4_block21_concat[0][0]       \n",
      "                                                                   conv4_block22_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormali (None, 8, 8, 488)     1952        conv4_block22_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activation (None, 8, 8, 488)     0           conv4_block23_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)    (None, 8, 8, 64)      31232       conv4_block23_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block23_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activation (None, 8, 8, 64)      0           conv4_block23_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block23_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatenat (None, 8, 8, 504)     0           conv4_block22_concat[0][0]       \n",
      "                                                                   conv4_block23_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormali (None, 8, 8, 504)     2016        conv4_block23_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activation (None, 8, 8, 504)     0           conv4_block24_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)    (None, 8, 8, 64)      32256       conv4_block24_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block24_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activation (None, 8, 8, 64)      0           conv4_block24_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block24_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatenat (None, 8, 8, 520)     0           conv4_block23_concat[0][0]       \n",
      "                                                                   conv4_block24_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)    (None, 8, 8, 520)     2080        conv4_block24_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool4_relu (Activation)          (None, 8, 8, 520)     0           pool4_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)              (None, 8, 8, 260)     135200      pool4_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)    (None, 4, 4, 260)     0           pool4_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormaliz (None, 4, 4, 260)     1040        pool4_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation) (None, 4, 4, 260)     0           conv5_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)     (None, 4, 4, 64)      16640       conv5_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenate (None, 4, 4, 276)     0           pool4_pool[0][0]                 \n",
      "                                                                   conv5_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormaliz (None, 4, 4, 276)     1104        conv5_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation) (None, 4, 4, 276)     0           conv5_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)     (None, 4, 4, 64)      17664       conv5_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenate (None, 4, 4, 292)     0           conv5_block1_concat[0][0]        \n",
      "                                                                   conv5_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormaliz (None, 4, 4, 292)     1168        conv5_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation) (None, 4, 4, 292)     0           conv5_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)     (None, 4, 4, 64)      18688       conv5_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenate (None, 4, 4, 308)     0           conv5_block2_concat[0][0]        \n",
      "                                                                   conv5_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormaliz (None, 4, 4, 308)     1232        conv5_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation) (None, 4, 4, 308)     0           conv5_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)     (None, 4, 4, 64)      19712       conv5_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenate (None, 4, 4, 324)     0           conv5_block3_concat[0][0]        \n",
      "                                                                   conv5_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormaliz (None, 4, 4, 324)     1296        conv5_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation) (None, 4, 4, 324)     0           conv5_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)     (None, 4, 4, 64)      20736       conv5_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenate (None, 4, 4, 340)     0           conv5_block4_concat[0][0]        \n",
      "                                                                   conv5_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormaliz (None, 4, 4, 340)     1360        conv5_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation) (None, 4, 4, 340)     0           conv5_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)     (None, 4, 4, 64)      21760       conv5_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenate (None, 4, 4, 356)     0           conv5_block5_concat[0][0]        \n",
      "                                                                   conv5_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormaliz (None, 4, 4, 356)     1424        conv5_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation) (None, 4, 4, 356)     0           conv5_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)     (None, 4, 4, 64)      22784       conv5_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenate (None, 4, 4, 372)     0           conv5_block6_concat[0][0]        \n",
      "                                                                   conv5_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormaliz (None, 4, 4, 372)     1488        conv5_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation) (None, 4, 4, 372)     0           conv5_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)     (None, 4, 4, 64)      23808       conv5_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenate (None, 4, 4, 388)     0           conv5_block7_concat[0][0]        \n",
      "                                                                   conv5_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormaliz (None, 4, 4, 388)     1552        conv5_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation) (None, 4, 4, 388)     0           conv5_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)     (None, 4, 4, 64)      24832       conv5_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenate (None, 4, 4, 404)     0           conv5_block8_concat[0][0]        \n",
      "                                                                   conv5_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormali (None, 4, 4, 404)     1616        conv5_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activation (None, 4, 4, 404)     0           conv5_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)    (None, 4, 4, 64)      25856       conv5_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activation (None, 4, 4, 64)      0           conv5_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatenat (None, 4, 4, 420)     0           conv5_block9_concat[0][0]        \n",
      "                                                                   conv5_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormali (None, 4, 4, 420)     1680        conv5_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activation (None, 4, 4, 420)     0           conv5_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)    (None, 4, 4, 64)      26880       conv5_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activation (None, 4, 4, 64)      0           conv5_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatenat (None, 4, 4, 436)     0           conv5_block10_concat[0][0]       \n",
      "                                                                   conv5_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormali (None, 4, 4, 436)     1744        conv5_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activation (None, 4, 4, 436)     0           conv5_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)    (None, 4, 4, 64)      27904       conv5_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activation (None, 4, 4, 64)      0           conv5_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatenat (None, 4, 4, 452)     0           conv5_block11_concat[0][0]       \n",
      "                                                                   conv5_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormali (None, 4, 4, 452)     1808        conv5_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activation (None, 4, 4, 452)     0           conv5_block13_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)    (None, 4, 4, 64)      28928       conv5_block13_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block13_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activation (None, 4, 4, 64)      0           conv5_block13_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block13_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatenat (None, 4, 4, 468)     0           conv5_block12_concat[0][0]       \n",
      "                                                                   conv5_block13_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormali (None, 4, 4, 468)     1872        conv5_block13_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activation (None, 4, 4, 468)     0           conv5_block14_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)    (None, 4, 4, 64)      29952       conv5_block14_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block14_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activation (None, 4, 4, 64)      0           conv5_block14_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block14_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatenat (None, 4, 4, 484)     0           conv5_block13_concat[0][0]       \n",
      "                                                                   conv5_block14_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormali (None, 4, 4, 484)     1936        conv5_block14_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activation (None, 4, 4, 484)     0           conv5_block15_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)    (None, 4, 4, 64)      30976       conv5_block15_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block15_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activation (None, 4, 4, 64)      0           conv5_block15_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block15_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatenat (None, 4, 4, 500)     0           conv5_block14_concat[0][0]       \n",
      "                                                                   conv5_block15_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormali (None, 4, 4, 500)     2000        conv5_block15_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activation (None, 4, 4, 500)     0           conv5_block16_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)    (None, 4, 4, 64)      32000       conv5_block16_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block16_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activation (None, 4, 4, 64)      0           conv5_block16_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block16_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatenat (None, 4, 4, 516)     0           conv5_block15_concat[0][0]       \n",
      "                                                                   conv5_block16_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "bn (BatchNormalization)          (None, 4, 4, 516)     2064        conv5_block16_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "relu (Activation)                (None, 4, 4, 516)     0           bn[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2D (None, 516)           0           relu[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "fc (Dense)                       (None, 171)           88407       avg_pool[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 1,945,831\n",
      "Trainable params: 1,902,543\n",
      "Non-trainable params: 43,288\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"av...)`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "img_model = DenseNet(blocks = [6, 12, 24, 16], \n",
    "                                cat_max = 171,\n",
    "                                weight_decay = 1e-4, \n",
    "                                kernel_initializer = 'glorot_normal',\n",
    "                                reduction = 0.5, \n",
    "                                init_filters = 64, \n",
    "                                growth_rate = 16).model\n",
    "img_model.load_weights(path + 'model_sub/6_12_24_16_ini64_growth16_02962/model_0_2018_09_13_08_23_48.h5')\n",
    "img_model_flat = Model(input = img_model.input, output = img_model.get_layer(name = 'avg_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14624/14633 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_img = extract_array_from_series(test_data['img'])\n",
    "test_img = vgg16.preprocess_input(test_img)\n",
    "test_img_feature_map = img_model_flat.predict(test_img, verbose = 1)\n",
    "\n",
    "train_id = train_data['class_id'].unique()\n",
    "class_id_emb_attr.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_partial_model = Model(inputs = zs_model.inputs[2:], outputs = zs_model.outputs[0])\n",
    "test_class_id_emb_attr = class_id_emb_attr #[~class_id_emb_attr['class_id'].isin(train_id)]\n",
    "pred_nearest_class_id = find_nearest_class(test_class_id_emb_attr, zs_partial_model, test_data, test_img_feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_label = time.strftime('_%Y_%m_%d_%H_%M_%S', time.gmtime())\n",
    "sub_name = path + '/model_sub/sub' + time_label + '.txt'\n",
    "sub = pd.DataFrame(pred_nearest_class_id, index = test_data['img_id'])\n",
    "sub.to_csv(sub_name, header = False, sep = '\\t')\n",
    "zs_model.save(path + 'zs_model' + time_label + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ZJL1', 'ZJL10', 'ZJL100', 'ZJL101', 'ZJL102', 'ZJL103', 'ZJL104',\n",
       "       'ZJL105', 'ZJL106', 'ZJL107', 'ZJL108', 'ZJL109', 'ZJL11',\n",
       "       'ZJL110', 'ZJL111', 'ZJL113', 'ZJL114', 'ZJL115', 'ZJL116',\n",
       "       'ZJL117', 'ZJL118', 'ZJL119', 'ZJL12', 'ZJL120', 'ZJL121',\n",
       "       'ZJL122', 'ZJL123', 'ZJL124', 'ZJL125', 'ZJL126', 'ZJL127',\n",
       "       'ZJL128', 'ZJL129', 'ZJL13', 'ZJL130', 'ZJL131', 'ZJL132',\n",
       "       'ZJL133', 'ZJL135', 'ZJL137', 'ZJL138', 'ZJL139', 'ZJL14',\n",
       "       'ZJL140', 'ZJL141', 'ZJL142', 'ZJL143', 'ZJL144', 'ZJL145',\n",
       "       'ZJL146', 'ZJL147', 'ZJL149', 'ZJL15', 'ZJL150', 'ZJL151',\n",
       "       'ZJL152', 'ZJL153', 'ZJL154', 'ZJL156', 'ZJL157', 'ZJL158',\n",
       "       'ZJL159', 'ZJL16', 'ZJL18', 'ZJL19', 'ZJL2', 'ZJL21', 'ZJL22',\n",
       "       'ZJL23', 'ZJL24', 'ZJL25', 'ZJL26', 'ZJL28', 'ZJL29', 'ZJL3',\n",
       "       'ZJL30', 'ZJL31', 'ZJL32', 'ZJL34', 'ZJL35', 'ZJL36', 'ZJL37',\n",
       "       'ZJL38', 'ZJL39', 'ZJL4', 'ZJL40', 'ZJL41', 'ZJL42', 'ZJL43',\n",
       "       'ZJL44', 'ZJL45', 'ZJL46', 'ZJL47', 'ZJL48', 'ZJL49', 'ZJL5',\n",
       "       'ZJL50', 'ZJL51', 'ZJL52', 'ZJL53', 'ZJL54', 'ZJL55', 'ZJL56',\n",
       "       'ZJL57', 'ZJL58', 'ZJL59', 'ZJL6', 'ZJL60', 'ZJL61', 'ZJL62',\n",
       "       'ZJL63', 'ZJL64', 'ZJL65', 'ZJL66', 'ZJL67', 'ZJL68', 'ZJL69',\n",
       "       'ZJL7', 'ZJL70', 'ZJL71', 'ZJL72', 'ZJL73', 'ZJL75', 'ZJL76',\n",
       "       'ZJL77', 'ZJL78', 'ZJL79', 'ZJL8', 'ZJL80', 'ZJL81', 'ZJL82',\n",
       "       'ZJL83', 'ZJL84', 'ZJL85', 'ZJL86', 'ZJL87', 'ZJL88', 'ZJL89',\n",
       "       'ZJL9', 'ZJL90', 'ZJL91', 'ZJL92', 'ZJL93', 'ZJL94', 'ZJL95',\n",
       "       'ZJL96', 'ZJL97', 'ZJL98', 'ZJL99', 'ZJL160', 'ZJL161', 'ZJL162',\n",
       "       'ZJL163', 'ZJL164', 'ZJL165', 'ZJL166', 'ZJL167', 'ZJL168',\n",
       "       'ZJL169', 'ZJL170', 'ZJL171', 'ZJL172', 'ZJL173', 'ZJL174',\n",
       "       'ZJL175', 'ZJL176', 'ZJL177', 'ZJL178', 'ZJL179', 'ZJL180',\n",
       "       'ZJL181', 'ZJL182', 'ZJL183', 'ZJL184', 'ZJL185', 'ZJL186',\n",
       "       'ZJL187', 'ZJL188', 'ZJL189', 'ZJL190', 'ZJL191', 'ZJL192',\n",
       "       'ZJL193', 'ZJL194', 'ZJL195', 'ZJL196', 'ZJL197', 'ZJL198',\n",
       "       'ZJL199', 'ZJL200'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
