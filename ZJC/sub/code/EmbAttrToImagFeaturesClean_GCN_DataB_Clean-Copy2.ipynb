{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import glob\n",
    "import gc\n",
    "import sklearn\n",
    "from DenseNet import DenseNet\n",
    "from DEM import DEM\n",
    "from sklearn.model_selection import KFold\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "from sklearn import metrics, preprocessing, pipeline, \\\n",
    "    feature_extraction, decomposition, model_selection\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import image\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator\n",
    "from tensorflow.python.keras import layers, preprocessing\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.python.keras.regularizers import l1, l2\n",
    "from tensorflow.python.keras.optimizers import SGD, RMSprop, Adam, Nadam\n",
    "from tensorflow.python.keras.losses import mean_squared_error, binary_crossentropy, categorical_crossentropy, cosine_proximity\n",
    "from tensorflow.python.keras.applications import vgg16\n",
    "\n",
    "# from keras.preprocessing import image\n",
    "# from keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator\n",
    "# from keras import layers, preprocessing\n",
    "# from keras import backend as K\n",
    "# from keras.models import Model, load_model\n",
    "# from keras.callbacks import EarlyStopping, Callback\n",
    "# from keras.regularizers import l1, l2\n",
    "# from keras.optimizers import SGD, RMSprop, Adam, Nadam\n",
    "# from keras.losses import mean_squared_error, binary_crossentropy\n",
    "# from keras.applications import vgg16\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "import numpy as np\n",
    "# np.random.seed(seed = 100)\n",
    "import json\n",
    "import scipy \n",
    "import lightgbm as lgb\n",
    "\n",
    "import gensim\n",
    "import os\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.models.wrappers.fasttext import FastText\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "def read_image(imag_path, image_id):\n",
    "    img = image.load_img(imag_path + image_id)\n",
    "    img = image.img_to_array(img)\n",
    "#     img= vgg16.preprocess_input(image.img_to_array(img))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del setA_train_data, setB_train_data\n",
    "# with open(path + 'setB_class_id_emb_attr.pickle', 'rb') as handle:\n",
    "#     class_id_emb_attr = pickle.load(handle)\n",
    "# with open(path + '/setA_train_data.pickle', 'rb') as handle:\n",
    "#     setA_train_data = pickle.load(handle)\n",
    "# with open(path + '/setB_train_data.pickle', 'rb') as handle:\n",
    "#     setB_train_data = pickle.load(handle)\n",
    "# with open(path + 'setB_test_data.pickle', 'rb') as handle:\n",
    "#     test_data = pickle.load(handle)\n",
    "    \n",
    "# train_data = setA_train_data.append(setB_train_data)\n",
    "# del setA_train_data, setB_train_data\n",
    "# with open(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/flat_train_re_2018_09_24_03_07_15.pickle', 'rb') as handle:\n",
    "#     flat_train_re = pickle.load(handle)\n",
    "# train_data['target'] = list(flat_train_re)\n",
    "# with open(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/flat_test_re_2018_09_24_03_07_15.pickle', 'rb') as handle:\n",
    "#     flat_test_re = pickle.load(handle)\n",
    "# test_data['target'] = list(flat_test_re)\n",
    "# train_data.drop(columns = ['img_id'], inplace = True)\n",
    "\n",
    "# with open(path + '/model_sub/6_12_24_16_ini64_growth32_inistride1_augdata_05010/train_data_2018_09_23_10_47_03.pickle', 'rb') as handle:\n",
    "#     train_data = pickle.load(handle)\n",
    "# with open(path + '/model_sub/6_12_24_16_ini64_growth32_inistride1_augdata_05010/test_data_2018_09_23_10_47_03.pickle', 'rb') as handle:\n",
    "#     test_data = pickle.load(handle)\n",
    "    \n",
    "with open(path + 'pix72/round2B_class_id_emb_attr_ft1300_encodedAttr.pkl', 'rb') as handle:\n",
    "    class_id_emb_attr = pickle.load(handle)\n",
    "#     class_id_emb_attr['emb'] = class_id_emb_attr['emb_glove']\n",
    "#     class_id_emb_attr.drop(columns = ['emb_glove', 'emb_fasttext', 'emb_glove_crawl', 'emb_glove_crawl_42B'], inplace = True)\n",
    "# del loaded_model\n",
    "emb_path = path + 'ft_class_embs/'\n",
    "ft_files = [\n",
    "#             path + '/skipgram_100_5Epoch/',\n",
    "#             path + '/skipgram_200_5Epoch/',\n",
    "# #             path + 'skipgram_300_5Epoch/',\n",
    "# #             path + 'skipgram_300_5Epoch_24Thread/',\n",
    "#             emb_path + 'skipgram_100_10Epoch',\n",
    "#             emb_path + 'skipgram_300_10Epoch',\n",
    "# # #             emb_path + 'cbow_300_10Epoch/',\n",
    "#             emb_path + 'skipgram_100_10Epoch_NewNorm',\n",
    "#             emb_path + 'LatestCorpus_skipgram_100_5Epoch',\n",
    "#               emb_path + 'skipgram_300_10Epoch_NewNorm',\n",
    "#             emb_path + 'LatestCorpus_skipgram_100_10Epoch',\n",
    "#             emb_path + 'LatestCorpus_skipgram_100_15Epoch',\n",
    "#             emb_path + 'LatestCorpus_skipgram_100_15Epoch_shuf',\n",
    "# #             emb_path + 'LatestCorpus_skipgram_300_5Epoch',\n",
    "# #             emb_path + 'LatestCorpus_skipgram_300_5Epoch',\n",
    "#             emb_path + 'LatestCorpus_skipgram_200_10Epoch',\n",
    "            emb_path + 'skipgram_300_15Epoch_NewNorm',\n",
    "            emb_path + 'skipgram_100_15Epoch_NewNorm',\n",
    "              emb_path + 'skipgram_200_15Epoch_NewNorm',\n",
    "#             emb_path + 'LatestCorpus_skipgram_200_10Epoch_shuf',\n",
    "              emb_path + 'skipgram_300_15Epoch_NewNorm_LrUpdate3e4',\n",
    "              emb_path + 'skipgram_300_15Epoch_NewNorm_LrUpdate2e4',\n",
    "#               emb_path + 'LatestCorpus_skipgram_300_10Epoch_shuf', \n",
    "           ]\n",
    "# glove_files =  [\n",
    "#             emb_path + 'LatestCorpus_glove_100',\n",
    "#             emb_path + 'LatestCorpus_glove_200',\n",
    "#            ]\n",
    "for ft_dir in ft_files:\n",
    "#     loaded_model = FastText.load_fasttext_format(ft_dir + '/')\n",
    "#     loaded_model = FastText.load(ft_dir + '/ft')\n",
    "    class_emb = pd.read_csv(ft_dir)\n",
    "    class_id_emb_attr['FT'] = class_emb.FT.apply(lambda s: np.array([float(x) for x in s.split(' ')]))\n",
    "#     class_id_emb_attr['FT'] = class_id_emb_attr['class_name'].apply(lambda c: loaded_model[c]).apply(lambda s: ' '.join(str(f) for f in s))\n",
    "#     ft_prefix = ft_dir.split('/')[-1]\n",
    "#     class_id_emb_attr[['class_name', 'FT']].to_csv(path + 'ft_class_embs/' + ft_prefix, index = False)\n",
    "#     del loaded_model\n",
    "    class_id_emb_attr['emb'] = class_id_emb_attr.apply(lambda s: np.hstack([s['emb'], s['FT']]), axis = 1)\n",
    "    \n",
    "# class_names = class_id_emb_attr.class_name.values\n",
    "# glove_dict = {}\n",
    "# with open(path + 'GloveEmb/vectors_200.txt') as f:\n",
    "#     for line in f:\n",
    "#         ls = line.split(' ')\n",
    "#         if ls[0] in class_names:\n",
    "#             glove_dict[ls[0]] = np.array([float(x) for x in ls[1:]])\n",
    "#             if len(glove_dict) == class_names.shape[0]:\n",
    "#                 print ('done')\n",
    "\n",
    "# class_id_emb_attr['Glove'] = class_id_emb_attr['class_name'].apply(lambda c: glove_dict[c])\n",
    "# class_id_emb_attr['emb'] = class_id_emb_attr.apply(lambda s: np.hstack([s['emb'], s['Glove']]), axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Load Word2Vec Model-----\n",
      "word_index size: 53963\n",
      "52624 Words in corpus!\n"
     ]
    }
   ],
   "source": [
    "def get_word2vec_embedding(location = 'wv_model_norm.gensim', tokenizer = None, nb_words = 60000, \\\n",
    "                embed_size = 300, model_type = \"fast_text\", uniform_init_emb = False):\n",
    "    \"\"\"Returns trained word2vec\n",
    "\n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "\n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if not os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        return None\n",
    "    print(\"-----Load Word2Vec Model-----\")\n",
    "    word_index = tokenizer.word_index\n",
    "    wv_model = dict()\n",
    "    with open(location, encoding=\"utf8\") as emb_file:\n",
    "        # with open(\"../../Data/normal_stem_wv_indata\", \"w+\") as emb_file_indata:\n",
    "        for line in emb_file:\n",
    "            ls = line.strip().split(' ')\n",
    "            word = ls[0]\n",
    "                # if word in word_index:\n",
    "                #     emb_file_indata.write(line)\n",
    "            wv_model[word] = np.asarray(ls[1:], dtype='float32')\n",
    "    print(\"word_index size: {0}\".format(len(word_index)))\n",
    "    if uniform_init_emb:\n",
    "        embedding_matrix = np.random.uniform(0, 1, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    word_in_corpus = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words: continue\n",
    "        if word in wv_model:\n",
    "            embedding_matrix[i] = wv_model[word]\n",
    "            word_in_corpus += 1\n",
    "    print(\"{0} Words in corpus!\".format(word_in_corpus))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "class_desc_index = pd.read_csv(path + './class_desc.index', names = ['class_name'], sep = ',')\n",
    "class_desc = pd.read_csv(path + './norm_class_desc.value', names = ['class_desc'], \n",
    "                               sep = ',')\n",
    "class_desc['class_name'] = class_desc_index.class_name\n",
    "class_desc['class_desc'] = class_desc.apply(lambda s: s.class_desc.replace(s.class_name, ' '), axis = 1)\n",
    "class_desc_value = list(class_desc.class_desc)\n",
    "tokenizer = preprocessing.text.Tokenizer(num_words = 60000)\n",
    "tokenizer.fit_on_texts(class_desc_value)\n",
    "emb_weight = get_word2vec_embedding(location = path + 'norm_class_desc.term_vecs', \n",
    "                                    tokenizer = tokenizer)\n",
    "class_desc_id = tokenizer.texts_to_sequences(class_desc_value)\n",
    "class_desc_id = preprocessing.sequence.pad_sequences(class_desc_id, maxlen = 200)\n",
    "class_id_emb_attr['desc'] = list(class_desc_id)\n",
    "# emb_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'pix72/round2B_class_id_emb_attr_ft2500_encodedAttr.pkl', 'wb') as handle:\n",
    "    pickle.dump(class_id_emb_attr, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>class_name</th>\n",
       "      <th>emb</th>\n",
       "      <th>attr</th>\n",
       "      <th>FT</th>\n",
       "      <th>encoded_attr</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>[-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...</td>\n",
       "      <td>[0.1149646, -0.3140999, 0.57411474, 0.29630047...</td>\n",
       "      <td>[17, 148, 10, 134, 239, 202, 279, 163, 22, 82,...</td>\n",
       "      <td>[6, 1564, 1, 4360, 1367, 13391, 1821, 408, 151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZJL10</td>\n",
       "      <td>tarantula</td>\n",
       "      <td>[-0.1564, 0.38850999999999997, -0.331580000000...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, ...</td>\n",
       "      <td>[0.3325013, 0.42204815, -0.1808518, 0.14346813...</td>\n",
       "      <td>[17, 148, 10, 134, 239, 202, 279, 8, 22, 181, ...</td>\n",
       "      <td>[413, 6, 32, 6540, 3, 277, 4368, 13, 70, 11, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZJL100</td>\n",
       "      <td>drumstick</td>\n",
       "      <td>[-0.15132, 0.18007, 0.12005, 0.21443, -0.40094...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.013632397, 0.71902895, 0.032226212, 0.19544...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 202, 100, 8, 22, 220,...</td>\n",
       "      <td>[2557, 1, 2964, 2, 37, 31, 41, 8945, 51, 188, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZJL101</td>\n",
       "      <td>dumbbell</td>\n",
       "      <td>[-0.3159, 0.15689, -0.037299, -0.2394800000000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.1, 0.3, ...</td>\n",
       "      <td>[-0.035188664, 0.1517249, -0.46055317, 0.46080...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 202, 267, 2, 0, 252, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZJL102</td>\n",
       "      <td>flagpole</td>\n",
       "      <td>[0.06352200000000001, -0.17278, 0.12012, -0.15...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, ...</td>\n",
       "      <td>[0.0317293, 0.31921837, -0.18210353, 0.6699042...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 202, 100, 2, 22, 82, ...</td>\n",
       "      <td>[8452, 6549, 12, 4042, 22, 33, 19, 41, 10213, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ZJL103</td>\n",
       "      <td>fountain</td>\n",
       "      <td>[-0.06857, 0.22303, 0.02907, -0.59109, 0.23779...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, ...</td>\n",
       "      <td>[-0.23176979, -0.12205786, 0.00305963, 0.26619...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 202, 100, 50, 88, 265...</td>\n",
       "      <td>[1739, 1888, 23, 2982, 2566, 461, 11, 29361, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ZJL104</td>\n",
       "      <td>car</td>\n",
       "      <td>[0.46443, 0.3773, -0.21459, -0.50768, -0.24575...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.4, ...</td>\n",
       "      <td>[-0.21230997, 0.41450253, -0.21426353, 0.46661...</td>\n",
       "      <td>[165, 7, 10, 134, 239, 202, 279, 50, 144, 162,...</td>\n",
       "      <td>[4, 4, 986, 122, 4, 2263, 574, 5, 1555, 612, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ZJL105</td>\n",
       "      <td>pan</td>\n",
       "      <td>[0.91643, 0.095422, 0.36995, -0.93058999999999...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.15048559, 0.12888779, 0.37634444, 0.0908370...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 221, 67, 8, 22, 82, 2...</td>\n",
       "      <td>[29388, 679, 4, 2701, 174, 4, 234, 5264, 10, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZJL106</td>\n",
       "      <td>coat</td>\n",
       "      <td>[0.033257999999999996, -0.43096, -0.0578690000...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6, 0.5, 0.3, ...</td>\n",
       "      <td>[0.010170216, 0.33417445, -0.3893057, -0.24055...</td>\n",
       "      <td>[165, 148, 90, 134, 239, 202, 24, 50, 0, 162, ...</td>\n",
       "      <td>[79, 781, 30, 3587, 4, 321, 59, 237, 1368, 10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ZJL107</td>\n",
       "      <td>mask</td>\n",
       "      <td>[0.28328000000000003, 0.0095288, -0.1115400000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7, 0.3, 0.0, ...</td>\n",
       "      <td>[-0.07759988, 0.36443108, 0.043521836, -0.0222...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 221, 278, 286, 22, 82...</td>\n",
       "      <td>[3506, 2, 5, 4395, 1679, 9, 38, 16, 13471, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ZJL108</td>\n",
       "      <td>go-kart</td>\n",
       "      <td>[0.43715, -0.20469, 0.21794000000000002, 0.047...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.2, 0.5, ...</td>\n",
       "      <td>[-0.33664346, -0.37547466, -0.5528914, 0.51069...</td>\n",
       "      <td>[165, 7, 10, 134, 239, 202, 279, 249, 88, 287,...</td>\n",
       "      <td>[5, 9593, 331, 1, 15373, 1, 3, 1, 21915, 29760...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ZJL109</td>\n",
       "      <td>gondola</td>\n",
       "      <td>[0.25874, -0.2576, -0.04796, -0.44076000000000...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.6426963, 0.36929774, -0.4408595, 0.1827686...</td>\n",
       "      <td>[165, 7, 10, 134, 239, 202, 267, 8, 22, 265, 1...</td>\n",
       "      <td>[12, 3872, 7225, 33, 2869, 16, 1, 135, 2, 3872...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ZJL11</td>\n",
       "      <td>centipede</td>\n",
       "      <td>[-0.45733999999999997, 0.65053, -0.15815, 0.08...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.052038547, 0.056434162, -0.1277771, 0.1577...</td>\n",
       "      <td>[17, 148, 10, 134, 239, 202, 100, 8, 22, 220, ...</td>\n",
       "      <td>[15, 870, 28, 3212, 460, 1849, 19, 1636, 6, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ZJL110</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>[-0.43633, 0.057562, -0.19513, 0.29417, 0.3261...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3, 0.6, 0.1, ...</td>\n",
       "      <td>[-0.25765032, 0.63641816, -0.121435784, 0.6777...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 221, 51, 102, 36, 82,...</td>\n",
       "      <td>[1582, 1, 191, 7245, 2, 1, 11183, 29944, 7, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ZJL111</td>\n",
       "      <td>ipod</td>\n",
       "      <td>[-0.25544, 0.07092899999999999, -0.23465999999...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.5, ...</td>\n",
       "      <td>[-0.10160401, 0.14163275, -0.1724485, 1.085940...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 221, 279, 50, 88, 82,...</td>\n",
       "      <td>[64, 135, 16, 1, 217, 8, 1474, 5, 3150, 2, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ZJL113</td>\n",
       "      <td>kimono</td>\n",
       "      <td>[-0.43646999999999997, -0.31595, -0.064084, 0....</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, 0.3, 0.3, ...</td>\n",
       "      <td>[-0.011755146, -0.2673759, 0.023946565, 0.7359...</td>\n",
       "      <td>[165, 148, 90, 134, 239, 202, 51, 286, 0, 287,...</td>\n",
       "      <td>[1058, 45, 1, 688, 7, 953, 100, 160, 12, 553, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ZJL114</td>\n",
       "      <td>lampshade</td>\n",
       "      <td>[-0.31351999999999997, -0.35048, -0.18804, -0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.8, 0.5, ...</td>\n",
       "      <td>[-0.037145477, -0.0045884233, -0.07360987, 0.3...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 221, 279, 180, 88, 26...</td>\n",
       "      <td>[51, 67, 3, 1516, 10, 5, 6612, 2165, 4, 842, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ZJL115</td>\n",
       "      <td>mower</td>\n",
       "      <td>[0.41175, -0.083526, -0.18375999999999998, -0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.2, 0.5, ...</td>\n",
       "      <td>[-0.3025545, 0.121124536, 0.028867356, 0.74450...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 221, 279, 249, 88, 26...</td>\n",
       "      <td>[5430, 59, 2, 1, 1216, 4, 2439, 57, 154, 21, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ZJL116</td>\n",
       "      <td>lifeboat</td>\n",
       "      <td>[-0.2953, -0.6340899999999999, 0.1097799999999...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.060779553, 0.07980797, -0.23010205, 0.4668...</td>\n",
       "      <td>[165, 7, 10, 134, 239, 202, 100, 8, 22, 82, 19...</td>\n",
       "      <td>[280, 12, 770, 299, 13, 120, 1, 6939, 7, 79, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ZJL117</td>\n",
       "      <td>limousine</td>\n",
       "      <td>[0.39878, -0.5906600000000001, -0.36968, -0.20...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.4, ...</td>\n",
       "      <td>[0.062857, 0.117792785, -0.15806851, 0.734136,...</td>\n",
       "      <td>[165, 7, 10, 134, 239, 202, 279, 50, 144, 162,...</td>\n",
       "      <td>[416, 1014, 1137, 998, 27, 315, 43, 2852, 2, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ZJL118</td>\n",
       "      <td>compass</td>\n",
       "      <td>[0.21789, -0.36199000000000003, 0.33075, -0.26...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.2, 0.0, ...</td>\n",
       "      <td>[-0.11611738, 0.16444671, 0.0868177, 0.4125058...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 221, 267, 249, 22, 82...</td>\n",
       "      <td>[11, 1, 3520, 2498, 312, 2, 5, 4694, 602, 6944...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ZJL119</td>\n",
       "      <td>maypole</td>\n",
       "      <td>[0.2263, -0.28943, 0.5158699999999999, -0.1743...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.046605904, 0.1282926, -0.11414856, 0.548003...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 202, 100, 8, 22, 220,...</td>\n",
       "      <td>[105, 1448, 3, 1, 2250, 4, 34, 393, 1, 3480, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ZJL12</td>\n",
       "      <td>goose</td>\n",
       "      <td>[0.23565999999999998, 0.30683, 0.18506, 0.2682...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, ...</td>\n",
       "      <td>[-0.08899335, -0.2932532, -0.3248234, 0.475758...</td>\n",
       "      <td>[17, 148, 10, 134, 239, 202, 100, 164, 22, 252...</td>\n",
       "      <td>[9, 31, 459, 16, 1, 1837, 4, 5329, 12, 6837, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ZJL120</td>\n",
       "      <td>uniform</td>\n",
       "      <td>[-0.44696, -0.13147999999999999, 0.11385, -0.1...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, ...</td>\n",
       "      <td>[0.004506878, 0.25240996, -0.34056577, 0.68645...</td>\n",
       "      <td>[165, 148, 90, 134, 239, 202, 279, 50, 88, 265...</td>\n",
       "      <td>[8, 14, 13681, 5838, 9650, 3, 1442, 70, 4, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ZJL121</td>\n",
       "      <td>miniskirt</td>\n",
       "      <td>[0.041488, -0.012031, -0.75962, 0.084367, -0.0...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, ...</td>\n",
       "      <td>[0.09917627, 0.43743566, -0.46129873, 0.450758...</td>\n",
       "      <td>[165, 148, 90, 134, 239, 202, 279, 50, 88, 265...</td>\n",
       "      <td>[3, 4496, 6374, 28, 129, 180, 1, 440, 3, 442, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ZJL122</td>\n",
       "      <td>van</td>\n",
       "      <td>[0.16976, -0.49576000000000003, -0.4419, 0.275...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.3, ...</td>\n",
       "      <td>[0.2220249, -0.034862347, 0.10664205, -0.14505...</td>\n",
       "      <td>[165, 7, 10, 134, 239, 202, 279, 50, 0, 262, 5...</td>\n",
       "      <td>[632, 12, 15542, 11247, 9, 26, 6, 1798, 1, 195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ZJL123</td>\n",
       "      <td>nail</td>\n",
       "      <td>[0.37637, 0.36164, -0.38082, 0.128229999999999...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.31332412, 0.42760685, -0.34325367, 0.22245...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 221, 67, 8, 22, 82, 2...</td>\n",
       "      <td>[15, 5, 360, 135, 2, 307, 33, 18, 4106, 14, 5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ZJL124</td>\n",
       "      <td>brace</td>\n",
       "      <td>[0.23373000000000002, -0.29489, -0.14131, -0.2...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.3792209, 0.29244858, 0.1584825, 0.44065905,...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 221, 67, 8, 22, 82, 2...</td>\n",
       "      <td>[18189, 10, 85, 10, 1, 8035, 3, 8477, 8627, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ZJL125</td>\n",
       "      <td>obelisk</td>\n",
       "      <td>[-0.12885, -0.19426, -0.12495999999999999, -0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.21504942, 0.40321508, 0.028531153, 0.31103...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 202, 67, 8, 22, 82, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ZJL126</td>\n",
       "      <td>oboe</td>\n",
       "      <td>[-0.48135, 0.28976999999999997, -0.62751, 0.45...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.440083, 0.31476673, 0.122580215, 0.3073282...</td>\n",
       "      <td>[165, 148, 10, 134, 239, 202, 278, 8, 22, 82, ...</td>\n",
       "      <td>[15611, 880, 596, 5, 127, 2, 821, 8, 17, 22327...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>ZJL491</td>\n",
       "      <td>admiral</td>\n",
       "      <td>[0.053237, -0.3005, 0.53386, -0.84404, -0.3232...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.11922852, 0.3325219, -0.14239976, 0.601296...</td>\n",
       "      <td>[17, 148, 10, 134, 129, 202, 267, 2, 22, 245, ...</td>\n",
       "      <td>[2298, 1, 4882, 11, 148, 88, 21, 33, 380, 1851...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>ZJL492</td>\n",
       "      <td>cello</td>\n",
       "      <td>[-0.29549000000000003, -0.2104, -0.64006000000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "      <td>[-0.093174115, -0.31567568, -0.4065277, -0.042...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 202, 81, 8, 22, 245, ...</td>\n",
       "      <td>[4467, 9, 31, 5, 135, 2, 8, 12, 12957, 8, 9, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>ZJL493</td>\n",
       "      <td>puffer</td>\n",
       "      <td>[0.60475, 0.37146999999999997, 0.4640399999999...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.18508978, -0.31733564, -0.05878605, 0.4287...</td>\n",
       "      <td>[17, 148, 10, 134, 129, 202, 279, 8, 22, 245, ...</td>\n",
       "      <td>[2, 770, 8, 30, 26, 6, 567, 4427, 4464, 13, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>ZJL494</td>\n",
       "      <td>coyote</td>\n",
       "      <td>[0.23928000000000002, 0.39632, -0.032925, 0.18...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.3189742, -0.006816056, -0.15609737, 0.12310...</td>\n",
       "      <td>[17, 148, 10, 134, 129, 202, 66, 50, 22, 245, ...</td>\n",
       "      <td>[1332, 730, 2397, 1144, 46, 959, 252, 14, 2511...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>ZJL495</td>\n",
       "      <td>diaper</td>\n",
       "      <td>[-0.48094, -0.47023000000000004, -0.066839, 0....</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.105973884, -0.091514, -0.049738236, 0.2206...</td>\n",
       "      <td>[165, 148, 90, 134, 129, 202, 100, 50, 88, 245...</td>\n",
       "      <td>[18, 996, 10, 3068, 6051, 28, 33, 31, 18, 26, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>ZJL496</td>\n",
       "      <td>lasagna</td>\n",
       "      <td>[0.12351, 0.20221, 0.12410999999999998, 0.0591...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.1, ...</td>\n",
       "      <td>[-0.009666942, -0.20590933, 0.2226488, 0.03433...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 202, 67, 102, 22, 245...</td>\n",
       "      <td>[10232, 547, 1, 123, 126, 1084, 356, 26, 268, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>ZJL497</td>\n",
       "      <td>ravioli</td>\n",
       "      <td>[0.090012, -0.27005999999999997, 0.73474, 0.52...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.1, 0.8, ...</td>\n",
       "      <td>[0.39993888, 0.47747856, -0.22309932, 0.171701...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 202, 100, 126, 22, 24...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 15168, 15168, 7, 5, 281,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>ZJL498</td>\n",
       "      <td>crepe</td>\n",
       "      <td>[0.31296999999999997, 0.21861999999999998, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, ...</td>\n",
       "      <td>[-0.24545537, -0.15801899, 0.26520485, 0.43122...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 202, 198, 163, 22, 24...</td>\n",
       "      <td>[439, 6, 457, 1173, 6418, 7, 56, 14, 5, 6002, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>ZJL499</td>\n",
       "      <td>hotpot</td>\n",
       "      <td>[-0.21215, 0.11823, 0.29638000000000003, -0.00...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, ...</td>\n",
       "      <td>[-0.42331988, 0.024399038, 0.22775544, 0.34899...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 202, 279, 50, 22, 245...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>ZJL500</td>\n",
       "      <td>waffle</td>\n",
       "      <td>[-0.069418, 0.056188, -0.13912, 0.54079, 0.017...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.07989164, -0.3023921, -0.22021729, 0.513184...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 202, 100, 163, 22, 24...</td>\n",
       "      <td>[19, 6, 18, 3314, 84, 9, 996, 14, 24, 1727, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>ZJL501</td>\n",
       "      <td>badminton</td>\n",
       "      <td>[0.19937, -0.38802, 0.45327, -0.23504, 0.6048,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, ...</td>\n",
       "      <td>[0.04777963, 0.043404106, -0.4154273, -0.07342...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 202, 198, 29, 22, 245...</td>\n",
       "      <td>[3, 3913, 12, 268, 51, 1845, 4, 245, 1, 586, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>ZJL502</td>\n",
       "      <td>battery</td>\n",
       "      <td>[0.017086, 0.13187000000000001, -0.74831, -0.3...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, ...</td>\n",
       "      <td>[0.0069941445, -0.29150572, 0.02395971, 0.4950...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 202, 279, 50, 22, 245...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>ZJL503</td>\n",
       "      <td>chameleon</td>\n",
       "      <td>[0.2041, -0.060529, -0.00031101, 0.01189099999...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, ...</td>\n",
       "      <td>[0.25405788, 0.073970295, 0.12269053, 0.228917...</td>\n",
       "      <td>[17, 148, 10, 134, 129, 202, 279, 50, 275, 245...</td>\n",
       "      <td>[20240, 7, 65, 124, 353, 47, 1, 32, 19159, 107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>ZJL504</td>\n",
       "      <td>chihuahua</td>\n",
       "      <td>[0.089863, 0.6418699999999999, -0.156679999999...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[-0.53531253, -0.16149446, -0.27518195, 0.4222...</td>\n",
       "      <td>[17, 148, 10, 134, 129, 202, 81, 163, 22, 245,...</td>\n",
       "      <td>[232, 5, 653, 1675, 3, 5270, 6308, 11, 5, 1050...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>ZJL505</td>\n",
       "      <td>cloak</td>\n",
       "      <td>[-0.19025999999999998, -0.32337, 0.14146, -0.1...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, ...</td>\n",
       "      <td>[-0.21465665, 0.19575532, -0.1632817, 0.403134...</td>\n",
       "      <td>[165, 148, 90, 134, 129, 202, 67, 163, 22, 245...</td>\n",
       "      <td>[7, 64, 3, 1188, 6149, 53, 82, 53583, 12, 128,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>ZJL506</td>\n",
       "      <td>crayfish</td>\n",
       "      <td>[0.31388, 0.10839000000000001, 0.34991, 0.6145...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, ...</td>\n",
       "      <td>[-0.7809395, 0.044108216, 0.007919356, -0.0367...</td>\n",
       "      <td>[17, 148, 10, 134, 129, 202, 100, 8, 275, 245,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>ZJL507</td>\n",
       "      <td>drum</td>\n",
       "      <td>[-0.1569, 0.41031999999999996, -0.885320000000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...</td>\n",
       "      <td>[-0.23050831, 0.14783262, -0.16996807, 0.51367...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 221, 279, 50, 22, 245...</td>\n",
       "      <td>[1, 4574, 53602, 214, 6, 2527, 1, 11786, 3, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>ZJL508</td>\n",
       "      <td>earplug</td>\n",
       "      <td>[-0.25473, -0.10052, -0.024631999999999998, 0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...</td>\n",
       "      <td>[-0.14431265, 0.4027606, -0.0016577862, 0.3109...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 221, 279, 50, 22, 245...</td>\n",
       "      <td>[17984, 12, 1356, 6, 13164, 13388, 567, 2, 133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>ZJL509</td>\n",
       "      <td>football</td>\n",
       "      <td>[0.010569, 1.4321, 0.43626000000000004, 0.3217...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.3, ...</td>\n",
       "      <td>[-0.1689589, 0.15929084, -0.37840053, 0.595256...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 202, 66, 163, 22, 245...</td>\n",
       "      <td>[655, 2, 1, 655, 45, 21, 752, 6, 1102, 13655, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>ZJL510</td>\n",
       "      <td>grand</td>\n",
       "      <td>[-0.49588000000000004, -0.018563, -0.086023, -...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...</td>\n",
       "      <td>[-0.25950426, -0.023487244, -0.1318331, 0.7389...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 221, 279, 50, 22, 245...</td>\n",
       "      <td>[75, 718, 238, 27, 1, 677, 22, 456, 34, 2, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>ZJL511</td>\n",
       "      <td>hammer</td>\n",
       "      <td>[0.24715, 0.19080999999999998, -0.226809999999...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, ...</td>\n",
       "      <td>[-0.2360338, -0.07647127, 0.22583552, -0.25658...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 221, 279, 50, 22, 245...</td>\n",
       "      <td>[4, 2843, 1, 16284, 945, 223, 648, 3, 5, 194, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>ZJL512</td>\n",
       "      <td>leatherback</td>\n",
       "      <td>[0.19159, -0.30508, 0.085977, -0.22526, 0.1997...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n",
       "      <td>[-0.416903, -0.11632784, -0.34706607, 0.544050...</td>\n",
       "      <td>[17, 148, 10, 134, 129, 202, 100, 8, 22, 245, ...</td>\n",
       "      <td>[129, 18, 2923, 3079, 109, 8, 9, 145, 1782, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>ZJL513</td>\n",
       "      <td>lotus</td>\n",
       "      <td>[-0.44666000000000006, -0.34273000000000003, -...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3, 0.0, 1.0, ...</td>\n",
       "      <td>[-0.21619053, -0.3781762, 0.20469096, 0.298500...</td>\n",
       "      <td>[165, 148, 10, 250, 129, 202, 100, 249, 22, 24...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>ZJL514</td>\n",
       "      <td>mailbox</td>\n",
       "      <td>[-0.5008100000000001, -0.12595, -0.71704, 0.11...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...</td>\n",
       "      <td>[0.28955215, -0.27664104, -0.26771367, 0.11479...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 221, 279, 50, 22, 245...</td>\n",
       "      <td>[3, 5, 10170, 18285, 124, 6, 13310, 189, 11, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>ZJL515</td>\n",
       "      <td>microphone</td>\n",
       "      <td>[-0.53493, 0.079057, 0.05472899999999999, -0.5...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...</td>\n",
       "      <td>[0.15795235, -0.26573238, -0.21733414, 0.18031...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 221, 279, 50, 22, 245...</td>\n",
       "      <td>[11324, 1, 230, 11, 8, 6, 232, 161, 12151, 27,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>ZJL516</td>\n",
       "      <td>pumpkin</td>\n",
       "      <td>[-0.25501, -0.36505, 0.33899999999999997, 0.23...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, ...</td>\n",
       "      <td>[0.32753658, -0.30290368, -0.4469345, 0.128047...</td>\n",
       "      <td>[165, 148, 10, 250, 129, 202, 67, 50, 22, 245,...</td>\n",
       "      <td>[1298, 62, 1589, 6, 679, 77, 59, 2943, 1016, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>ZJL517</td>\n",
       "      <td>sax</td>\n",
       "      <td>[0.11925999999999999, -0.24611999999999998, -0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.5, ...</td>\n",
       "      <td>[-0.08635518, -0.003806839, -0.07592675, -0.00...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 202, 100, 163, 22, 24...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>ZJL518</td>\n",
       "      <td>shield</td>\n",
       "      <td>[0.049525, 0.19760999999999998, -0.16082, -0.5...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...</td>\n",
       "      <td>[0.10307992, -0.118768804, -0.14841947, 0.4026...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 221, 279, 50, 22, 245...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>ZJL519</td>\n",
       "      <td>soccer</td>\n",
       "      <td>[0.3012, 0.9056200000000001, 0.387240000000000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.8, ...</td>\n",
       "      <td>[0.45865828, -0.03809078, -0.3546014, 0.187495...</td>\n",
       "      <td>[165, 148, 10, 134, 129, 202, 81, 249, 22, 245...</td>\n",
       "      <td>[189, 30, 182, 11, 1418, 57, 1332, 1868, 16, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>ZJL520</td>\n",
       "      <td>zucchini</td>\n",
       "      <td>[-0.037667, -0.37043000000000004, 0.60163, -0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2, ...</td>\n",
       "      <td>[0.23991816, -0.3139726, -0.11096571, 0.024766...</td>\n",
       "      <td>[165, 148, 10, 250, 129, 202, 100, 8, 22, 245,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class_id   class_name                                                emb  \\\n",
       "0       ZJL1     goldfish  [-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...   \n",
       "1      ZJL10    tarantula  [-0.1564, 0.38850999999999997, -0.331580000000...   \n",
       "2     ZJL100    drumstick  [-0.15132, 0.18007, 0.12005, 0.21443, -0.40094...   \n",
       "3     ZJL101     dumbbell  [-0.3159, 0.15689, -0.037299, -0.2394800000000...   \n",
       "4     ZJL102     flagpole  [0.06352200000000001, -0.17278, 0.12012, -0.15...   \n",
       "5     ZJL103     fountain  [-0.06857, 0.22303, 0.02907, -0.59109, 0.23779...   \n",
       "6     ZJL104          car  [0.46443, 0.3773, -0.21459, -0.50768, -0.24575...   \n",
       "7     ZJL105          pan  [0.91643, 0.095422, 0.36995, -0.93058999999999...   \n",
       "8     ZJL106         coat  [0.033257999999999996, -0.43096, -0.0578690000...   \n",
       "9     ZJL107         mask  [0.28328000000000003, 0.0095288, -0.1115400000...   \n",
       "10    ZJL108      go-kart  [0.43715, -0.20469, 0.21794000000000002, 0.047...   \n",
       "11    ZJL109      gondola  [0.25874, -0.2576, -0.04796, -0.44076000000000...   \n",
       "12     ZJL11    centipede  [-0.45733999999999997, 0.65053, -0.15815, 0.08...   \n",
       "13    ZJL110    hourglass  [-0.43633, 0.057562, -0.19513, 0.29417, 0.3261...   \n",
       "14    ZJL111         ipod  [-0.25544, 0.07092899999999999, -0.23465999999...   \n",
       "15    ZJL113       kimono  [-0.43646999999999997, -0.31595, -0.064084, 0....   \n",
       "16    ZJL114    lampshade  [-0.31351999999999997, -0.35048, -0.18804, -0....   \n",
       "17    ZJL115        mower  [0.41175, -0.083526, -0.18375999999999998, -0....   \n",
       "18    ZJL116     lifeboat  [-0.2953, -0.6340899999999999, 0.1097799999999...   \n",
       "19    ZJL117    limousine  [0.39878, -0.5906600000000001, -0.36968, -0.20...   \n",
       "20    ZJL118      compass  [0.21789, -0.36199000000000003, 0.33075, -0.26...   \n",
       "21    ZJL119      maypole  [0.2263, -0.28943, 0.5158699999999999, -0.1743...   \n",
       "22     ZJL12        goose  [0.23565999999999998, 0.30683, 0.18506, 0.2682...   \n",
       "23    ZJL120      uniform  [-0.44696, -0.13147999999999999, 0.11385, -0.1...   \n",
       "24    ZJL121    miniskirt  [0.041488, -0.012031, -0.75962, 0.084367, -0.0...   \n",
       "25    ZJL122          van  [0.16976, -0.49576000000000003, -0.4419, 0.275...   \n",
       "26    ZJL123         nail  [0.37637, 0.36164, -0.38082, 0.128229999999999...   \n",
       "27    ZJL124        brace  [0.23373000000000002, -0.29489, -0.14131, -0.2...   \n",
       "28    ZJL125      obelisk  [-0.12885, -0.19426, -0.12495999999999999, -0....   \n",
       "29    ZJL126         oboe  [-0.48135, 0.28976999999999997, -0.62751, 0.45...   \n",
       "..       ...          ...                                                ...   \n",
       "195   ZJL491      admiral  [0.053237, -0.3005, 0.53386, -0.84404, -0.3232...   \n",
       "196   ZJL492        cello  [-0.29549000000000003, -0.2104, -0.64006000000...   \n",
       "197   ZJL493       puffer  [0.60475, 0.37146999999999997, 0.4640399999999...   \n",
       "198   ZJL494       coyote  [0.23928000000000002, 0.39632, -0.032925, 0.18...   \n",
       "199   ZJL495       diaper  [-0.48094, -0.47023000000000004, -0.066839, 0....   \n",
       "200   ZJL496      lasagna  [0.12351, 0.20221, 0.12410999999999998, 0.0591...   \n",
       "201   ZJL497      ravioli  [0.090012, -0.27005999999999997, 0.73474, 0.52...   \n",
       "202   ZJL498        crepe  [0.31296999999999997, 0.21861999999999998, 0.0...   \n",
       "203   ZJL499       hotpot  [-0.21215, 0.11823, 0.29638000000000003, -0.00...   \n",
       "204   ZJL500       waffle  [-0.069418, 0.056188, -0.13912, 0.54079, 0.017...   \n",
       "205   ZJL501    badminton  [0.19937, -0.38802, 0.45327, -0.23504, 0.6048,...   \n",
       "206   ZJL502      battery  [0.017086, 0.13187000000000001, -0.74831, -0.3...   \n",
       "207   ZJL503    chameleon  [0.2041, -0.060529, -0.00031101, 0.01189099999...   \n",
       "208   ZJL504    chihuahua  [0.089863, 0.6418699999999999, -0.156679999999...   \n",
       "209   ZJL505        cloak  [-0.19025999999999998, -0.32337, 0.14146, -0.1...   \n",
       "210   ZJL506     crayfish  [0.31388, 0.10839000000000001, 0.34991, 0.6145...   \n",
       "211   ZJL507         drum  [-0.1569, 0.41031999999999996, -0.885320000000...   \n",
       "212   ZJL508      earplug  [-0.25473, -0.10052, -0.024631999999999998, 0....   \n",
       "213   ZJL509     football  [0.010569, 1.4321, 0.43626000000000004, 0.3217...   \n",
       "214   ZJL510        grand  [-0.49588000000000004, -0.018563, -0.086023, -...   \n",
       "215   ZJL511       hammer  [0.24715, 0.19080999999999998, -0.226809999999...   \n",
       "216   ZJL512  leatherback  [0.19159, -0.30508, 0.085977, -0.22526, 0.1997...   \n",
       "217   ZJL513        lotus  [-0.44666000000000006, -0.34273000000000003, -...   \n",
       "218   ZJL514      mailbox  [-0.5008100000000001, -0.12595, -0.71704, 0.11...   \n",
       "219   ZJL515   microphone  [-0.53493, 0.079057, 0.05472899999999999, -0.5...   \n",
       "220   ZJL516      pumpkin  [-0.25501, -0.36505, 0.33899999999999997, 0.23...   \n",
       "221   ZJL517          sax  [0.11925999999999999, -0.24611999999999998, -0...   \n",
       "222   ZJL518       shield  [0.049525, 0.19760999999999998, -0.16082, -0.5...   \n",
       "223   ZJL519       soccer  [0.3012, 0.9056200000000001, 0.387240000000000...   \n",
       "224   ZJL520     zucchini  [-0.037667, -0.37043000000000004, 0.60163, -0....   \n",
       "\n",
       "                                                  attr  \\\n",
       "0    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...   \n",
       "1    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.1, 0.3, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, ...   \n",
       "5    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, ...   \n",
       "6    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.4, ...   \n",
       "7    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...   \n",
       "8    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6, 0.5, 0.3, ...   \n",
       "9    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7, 0.3, 0.0, ...   \n",
       "10   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.2, 0.5, ...   \n",
       "11   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, ...   \n",
       "12   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "13   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3, 0.6, 0.1, ...   \n",
       "14   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.5, ...   \n",
       "15   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, 0.3, 0.3, ...   \n",
       "16   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.8, 0.5, ...   \n",
       "17   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.2, 0.5, ...   \n",
       "18   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "19   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.4, ...   \n",
       "20   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.2, 0.0, ...   \n",
       "21   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "22   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, ...   \n",
       "23   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, ...   \n",
       "24   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, ...   \n",
       "25   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.3, ...   \n",
       "26   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...   \n",
       "27   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...   \n",
       "28   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "29   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "195  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "196  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...   \n",
       "197  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "198  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "199  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "200  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.1, ...   \n",
       "201  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.1, 0.8, ...   \n",
       "202  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, ...   \n",
       "203  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, ...   \n",
       "204  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "205  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, ...   \n",
       "206  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, ...   \n",
       "207  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, ...   \n",
       "208  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, ...   \n",
       "209  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, ...   \n",
       "210  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, ...   \n",
       "211  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...   \n",
       "212  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...   \n",
       "213  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.3, ...   \n",
       "214  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...   \n",
       "215  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, ...   \n",
       "216  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...   \n",
       "217  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3, 0.0, 1.0, ...   \n",
       "218  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...   \n",
       "219  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...   \n",
       "220  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, ...   \n",
       "221  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.5, ...   \n",
       "222  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, ...   \n",
       "223  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.8, ...   \n",
       "224  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2, ...   \n",
       "\n",
       "                                                    FT  \\\n",
       "0    [0.1149646, -0.3140999, 0.57411474, 0.29630047...   \n",
       "1    [0.3325013, 0.42204815, -0.1808518, 0.14346813...   \n",
       "2    [0.013632397, 0.71902895, 0.032226212, 0.19544...   \n",
       "3    [-0.035188664, 0.1517249, -0.46055317, 0.46080...   \n",
       "4    [0.0317293, 0.31921837, -0.18210353, 0.6699042...   \n",
       "5    [-0.23176979, -0.12205786, 0.00305963, 0.26619...   \n",
       "6    [-0.21230997, 0.41450253, -0.21426353, 0.46661...   \n",
       "7    [0.15048559, 0.12888779, 0.37634444, 0.0908370...   \n",
       "8    [0.010170216, 0.33417445, -0.3893057, -0.24055...   \n",
       "9    [-0.07759988, 0.36443108, 0.043521836, -0.0222...   \n",
       "10   [-0.33664346, -0.37547466, -0.5528914, 0.51069...   \n",
       "11   [-0.6426963, 0.36929774, -0.4408595, 0.1827686...   \n",
       "12   [-0.052038547, 0.056434162, -0.1277771, 0.1577...   \n",
       "13   [-0.25765032, 0.63641816, -0.121435784, 0.6777...   \n",
       "14   [-0.10160401, 0.14163275, -0.1724485, 1.085940...   \n",
       "15   [-0.011755146, -0.2673759, 0.023946565, 0.7359...   \n",
       "16   [-0.037145477, -0.0045884233, -0.07360987, 0.3...   \n",
       "17   [-0.3025545, 0.121124536, 0.028867356, 0.74450...   \n",
       "18   [-0.060779553, 0.07980797, -0.23010205, 0.4668...   \n",
       "19   [0.062857, 0.117792785, -0.15806851, 0.734136,...   \n",
       "20   [-0.11611738, 0.16444671, 0.0868177, 0.4125058...   \n",
       "21   [0.046605904, 0.1282926, -0.11414856, 0.548003...   \n",
       "22   [-0.08899335, -0.2932532, -0.3248234, 0.475758...   \n",
       "23   [0.004506878, 0.25240996, -0.34056577, 0.68645...   \n",
       "24   [0.09917627, 0.43743566, -0.46129873, 0.450758...   \n",
       "25   [0.2220249, -0.034862347, 0.10664205, -0.14505...   \n",
       "26   [-0.31332412, 0.42760685, -0.34325367, 0.22245...   \n",
       "27   [0.3792209, 0.29244858, 0.1584825, 0.44065905,...   \n",
       "28   [-0.21504942, 0.40321508, 0.028531153, 0.31103...   \n",
       "29   [-0.440083, 0.31476673, 0.122580215, 0.3073282...   \n",
       "..                                                 ...   \n",
       "195  [-0.11922852, 0.3325219, -0.14239976, 0.601296...   \n",
       "196  [-0.093174115, -0.31567568, -0.4065277, -0.042...   \n",
       "197  [-0.18508978, -0.31733564, -0.05878605, 0.4287...   \n",
       "198  [0.3189742, -0.006816056, -0.15609737, 0.12310...   \n",
       "199  [-0.105973884, -0.091514, -0.049738236, 0.2206...   \n",
       "200  [-0.009666942, -0.20590933, 0.2226488, 0.03433...   \n",
       "201  [0.39993888, 0.47747856, -0.22309932, 0.171701...   \n",
       "202  [-0.24545537, -0.15801899, 0.26520485, 0.43122...   \n",
       "203  [-0.42331988, 0.024399038, 0.22775544, 0.34899...   \n",
       "204  [0.07989164, -0.3023921, -0.22021729, 0.513184...   \n",
       "205  [0.04777963, 0.043404106, -0.4154273, -0.07342...   \n",
       "206  [0.0069941445, -0.29150572, 0.02395971, 0.4950...   \n",
       "207  [0.25405788, 0.073970295, 0.12269053, 0.228917...   \n",
       "208  [-0.53531253, -0.16149446, -0.27518195, 0.4222...   \n",
       "209  [-0.21465665, 0.19575532, -0.1632817, 0.403134...   \n",
       "210  [-0.7809395, 0.044108216, 0.007919356, -0.0367...   \n",
       "211  [-0.23050831, 0.14783262, -0.16996807, 0.51367...   \n",
       "212  [-0.14431265, 0.4027606, -0.0016577862, 0.3109...   \n",
       "213  [-0.1689589, 0.15929084, -0.37840053, 0.595256...   \n",
       "214  [-0.25950426, -0.023487244, -0.1318331, 0.7389...   \n",
       "215  [-0.2360338, -0.07647127, 0.22583552, -0.25658...   \n",
       "216  [-0.416903, -0.11632784, -0.34706607, 0.544050...   \n",
       "217  [-0.21619053, -0.3781762, 0.20469096, 0.298500...   \n",
       "218  [0.28955215, -0.27664104, -0.26771367, 0.11479...   \n",
       "219  [0.15795235, -0.26573238, -0.21733414, 0.18031...   \n",
       "220  [0.32753658, -0.30290368, -0.4469345, 0.128047...   \n",
       "221  [-0.08635518, -0.003806839, -0.07592675, -0.00...   \n",
       "222  [0.10307992, -0.118768804, -0.14841947, 0.4026...   \n",
       "223  [0.45865828, -0.03809078, -0.3546014, 0.187495...   \n",
       "224  [0.23991816, -0.3139726, -0.11096571, 0.024766...   \n",
       "\n",
       "                                          encoded_attr  \\\n",
       "0    [17, 148, 10, 134, 239, 202, 279, 163, 22, 82,...   \n",
       "1    [17, 148, 10, 134, 239, 202, 279, 8, 22, 181, ...   \n",
       "2    [165, 148, 10, 134, 239, 202, 100, 8, 22, 220,...   \n",
       "3    [165, 148, 10, 134, 239, 202, 267, 2, 0, 252, ...   \n",
       "4    [165, 148, 10, 134, 239, 202, 100, 2, 22, 82, ...   \n",
       "5    [165, 148, 10, 134, 239, 202, 100, 50, 88, 265...   \n",
       "6    [165, 7, 10, 134, 239, 202, 279, 50, 144, 162,...   \n",
       "7    [165, 148, 10, 134, 239, 221, 67, 8, 22, 82, 2...   \n",
       "8    [165, 148, 90, 134, 239, 202, 24, 50, 0, 162, ...   \n",
       "9    [165, 148, 10, 134, 239, 221, 278, 286, 22, 82...   \n",
       "10   [165, 7, 10, 134, 239, 202, 279, 249, 88, 287,...   \n",
       "11   [165, 7, 10, 134, 239, 202, 267, 8, 22, 265, 1...   \n",
       "12   [17, 148, 10, 134, 239, 202, 100, 8, 22, 220, ...   \n",
       "13   [165, 148, 10, 134, 239, 221, 51, 102, 36, 82,...   \n",
       "14   [165, 148, 10, 134, 239, 221, 279, 50, 88, 82,...   \n",
       "15   [165, 148, 90, 134, 239, 202, 51, 286, 0, 287,...   \n",
       "16   [165, 148, 10, 134, 239, 221, 279, 180, 88, 26...   \n",
       "17   [165, 148, 10, 134, 239, 221, 279, 249, 88, 26...   \n",
       "18   [165, 7, 10, 134, 239, 202, 100, 8, 22, 82, 19...   \n",
       "19   [165, 7, 10, 134, 239, 202, 279, 50, 144, 162,...   \n",
       "20   [165, 148, 10, 134, 239, 221, 267, 249, 22, 82...   \n",
       "21   [165, 148, 10, 134, 239, 202, 100, 8, 22, 220,...   \n",
       "22   [17, 148, 10, 134, 239, 202, 100, 164, 22, 252...   \n",
       "23   [165, 148, 90, 134, 239, 202, 279, 50, 88, 265...   \n",
       "24   [165, 148, 90, 134, 239, 202, 279, 50, 88, 265...   \n",
       "25   [165, 7, 10, 134, 239, 202, 279, 50, 0, 262, 5...   \n",
       "26   [165, 148, 10, 134, 239, 221, 67, 8, 22, 82, 2...   \n",
       "27   [165, 148, 10, 134, 239, 221, 67, 8, 22, 82, 2...   \n",
       "28   [165, 148, 10, 134, 239, 202, 67, 8, 22, 82, 2...   \n",
       "29   [165, 148, 10, 134, 239, 202, 278, 8, 22, 82, ...   \n",
       "..                                                 ...   \n",
       "195  [17, 148, 10, 134, 129, 202, 267, 2, 22, 245, ...   \n",
       "196  [165, 148, 10, 134, 129, 202, 81, 8, 22, 245, ...   \n",
       "197  [17, 148, 10, 134, 129, 202, 279, 8, 22, 245, ...   \n",
       "198  [17, 148, 10, 134, 129, 202, 66, 50, 22, 245, ...   \n",
       "199  [165, 148, 90, 134, 129, 202, 100, 50, 88, 245...   \n",
       "200  [165, 148, 10, 134, 129, 202, 67, 102, 22, 245...   \n",
       "201  [165, 148, 10, 134, 129, 202, 100, 126, 22, 24...   \n",
       "202  [165, 148, 10, 134, 129, 202, 198, 163, 22, 24...   \n",
       "203  [165, 148, 10, 134, 129, 202, 279, 50, 22, 245...   \n",
       "204  [165, 148, 10, 134, 129, 202, 100, 163, 22, 24...   \n",
       "205  [165, 148, 10, 134, 129, 202, 198, 29, 22, 245...   \n",
       "206  [165, 148, 10, 134, 129, 202, 279, 50, 22, 245...   \n",
       "207  [17, 148, 10, 134, 129, 202, 279, 50, 275, 245...   \n",
       "208  [17, 148, 10, 134, 129, 202, 81, 163, 22, 245,...   \n",
       "209  [165, 148, 90, 134, 129, 202, 67, 163, 22, 245...   \n",
       "210  [17, 148, 10, 134, 129, 202, 100, 8, 275, 245,...   \n",
       "211  [165, 148, 10, 134, 129, 221, 279, 50, 22, 245...   \n",
       "212  [165, 148, 10, 134, 129, 221, 279, 50, 22, 245...   \n",
       "213  [165, 148, 10, 134, 129, 202, 66, 163, 22, 245...   \n",
       "214  [165, 148, 10, 134, 129, 221, 279, 50, 22, 245...   \n",
       "215  [165, 148, 10, 134, 129, 221, 279, 50, 22, 245...   \n",
       "216  [17, 148, 10, 134, 129, 202, 100, 8, 22, 245, ...   \n",
       "217  [165, 148, 10, 250, 129, 202, 100, 249, 22, 24...   \n",
       "218  [165, 148, 10, 134, 129, 221, 279, 50, 22, 245...   \n",
       "219  [165, 148, 10, 134, 129, 221, 279, 50, 22, 245...   \n",
       "220  [165, 148, 10, 250, 129, 202, 67, 50, 22, 245,...   \n",
       "221  [165, 148, 10, 134, 129, 202, 100, 163, 22, 24...   \n",
       "222  [165, 148, 10, 134, 129, 221, 279, 50, 22, 245...   \n",
       "223  [165, 148, 10, 134, 129, 202, 81, 249, 22, 245...   \n",
       "224  [165, 148, 10, 250, 129, 202, 100, 8, 22, 245,...   \n",
       "\n",
       "                                                  desc  \n",
       "0    [6, 1564, 1, 4360, 1367, 13391, 1821, 408, 151...  \n",
       "1    [413, 6, 32, 6540, 3, 277, 4368, 13, 70, 11, 3...  \n",
       "2    [2557, 1, 2964, 2, 37, 31, 41, 8945, 51, 188, ...  \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4    [8452, 6549, 12, 4042, 22, 33, 19, 41, 10213, ...  \n",
       "5    [1739, 1888, 23, 2982, 2566, 461, 11, 29361, 2...  \n",
       "6    [4, 4, 986, 122, 4, 2263, 574, 5, 1555, 612, 4...  \n",
       "7    [29388, 679, 4, 2701, 174, 4, 234, 5264, 10, 8...  \n",
       "8    [79, 781, 30, 3587, 4, 321, 59, 237, 1368, 10,...  \n",
       "9    [3506, 2, 5, 4395, 1679, 9, 38, 16, 13471, 13,...  \n",
       "10   [5, 9593, 331, 1, 15373, 1, 3, 1, 21915, 29760...  \n",
       "11   [12, 3872, 7225, 33, 2869, 16, 1, 135, 2, 3872...  \n",
       "12   [15, 870, 28, 3212, 460, 1849, 19, 1636, 6, 20...  \n",
       "13   [1582, 1, 191, 7245, 2, 1, 11183, 29944, 7, 16...  \n",
       "14   [64, 135, 16, 1, 217, 8, 1474, 5, 3150, 2, 1, ...  \n",
       "15   [1058, 45, 1, 688, 7, 953, 100, 160, 12, 553, ...  \n",
       "16   [51, 67, 3, 1516, 10, 5, 6612, 2165, 4, 842, 2...  \n",
       "17   [5430, 59, 2, 1, 1216, 4, 2439, 57, 154, 21, 9...  \n",
       "18   [280, 12, 770, 299, 13, 120, 1, 6939, 7, 79, 1...  \n",
       "19   [416, 1014, 1137, 998, 27, 315, 43, 2852, 2, 3...  \n",
       "20   [11, 1, 3520, 2498, 312, 2, 5, 4694, 602, 6944...  \n",
       "21   [105, 1448, 3, 1, 2250, 4, 34, 393, 1, 3480, 8...  \n",
       "22   [9, 31, 459, 16, 1, 1837, 4, 5329, 12, 6837, 7...  \n",
       "23   [8, 14, 13681, 5838, 9650, 3, 1442, 70, 4, 1, ...  \n",
       "24   [3, 4496, 6374, 28, 129, 180, 1, 440, 3, 442, ...  \n",
       "25   [632, 12, 15542, 11247, 9, 26, 6, 1798, 1, 195...  \n",
       "26   [15, 5, 360, 135, 2, 307, 33, 18, 4106, 14, 5,...  \n",
       "27   [18189, 10, 85, 10, 1, 8035, 3, 8477, 8627, 1,...  \n",
       "28   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "29   [15611, 880, 596, 5, 127, 2, 821, 8, 17, 22327...  \n",
       "..                                                 ...  \n",
       "195  [2298, 1, 4882, 11, 148, 88, 21, 33, 380, 1851...  \n",
       "196  [4467, 9, 31, 5, 135, 2, 8, 12, 12957, 8, 9, 3...  \n",
       "197  [2, 770, 8, 30, 26, 6, 567, 4427, 4464, 13, 12...  \n",
       "198  [1332, 730, 2397, 1144, 46, 959, 252, 14, 2511...  \n",
       "199  [18, 996, 10, 3068, 6051, 28, 33, 31, 18, 26, ...  \n",
       "200  [10232, 547, 1, 123, 126, 1084, 356, 26, 268, ...  \n",
       "201  [0, 0, 0, 0, 0, 0, 0, 15168, 15168, 7, 5, 281,...  \n",
       "202  [439, 6, 457, 1173, 6418, 7, 56, 14, 5, 6002, ...  \n",
       "203  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "204  [19, 6, 18, 3314, 84, 9, 996, 14, 24, 1727, 2,...  \n",
       "205  [3, 3913, 12, 268, 51, 1845, 4, 245, 1, 586, 9...  \n",
       "206  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "207  [20240, 7, 65, 124, 353, 47, 1, 32, 19159, 107...  \n",
       "208  [232, 5, 653, 1675, 3, 5270, 6308, 11, 5, 1050...  \n",
       "209  [7, 64, 3, 1188, 6149, 53, 82, 53583, 12, 128,...  \n",
       "210  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "211  [1, 4574, 53602, 214, 6, 2527, 1, 11786, 3, 18...  \n",
       "212  [17984, 12, 1356, 6, 13164, 13388, 567, 2, 133...  \n",
       "213  [655, 2, 1, 655, 45, 21, 752, 6, 1102, 13655, ...  \n",
       "214  [75, 718, 238, 27, 1, 677, 22, 456, 34, 2, 1, ...  \n",
       "215  [4, 2843, 1, 16284, 945, 223, 648, 3, 5, 194, ...  \n",
       "216  [129, 18, 2923, 3079, 109, 8, 9, 145, 1782, 10...  \n",
       "217  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "218  [3, 5, 10170, 18285, 124, 6, 13310, 189, 11, 1...  \n",
       "219  [11324, 1, 230, 11, 8, 6, 232, 161, 12151, 27,...  \n",
       "220  [1298, 62, 1589, 6, 679, 77, 59, 2943, 1016, 6...  \n",
       "221  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "222  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "223  [189, 30, 182, 11, 1418, 57, 1332, 1868, 16, 1...  \n",
       "224  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[510 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_id_emb_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_array_from_series(s):\n",
    "    return np.asarray(list(s))\n",
    "\n",
    "attr1_list = pd.read_csv(path + '/DatasetA/attribute_list.txt', index_col = 0, sep = '\\t', header = None)\n",
    "attr2_list = pd.read_csv(path + '/semifinal_image_phase2/attribute_list.txt', index_col = 0, sep = '\\t', header = None)\n",
    "attr_dict ={}\n",
    "round2_class_id = ['ZJL' + str(i) for i in range(296, 521)]\n",
    "round1_class_id = list(set(class_id_emb_attr.class_id.unique()) - set(round2_class_id))\n",
    "round1_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(round1_class_id)]\n",
    "round2_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(round2_class_id)]\n",
    "round1_attr_value = extract_array_from_series(round1_class_id_emb_attr['attr'])\n",
    "round2_attr_value = extract_array_from_series(round2_class_id_emb_attr['attr'])\n",
    "round1_attrs = attr1_list[1].values\n",
    "round2_attrs = attr2_list[1].values\n",
    "# round1_attr_to_ind = {}\n",
    "# round2_attr_to_ind = {}\n",
    "for i, attr in enumerate(list(attr1_list[1].values)):\n",
    "    if attr not in attr_dict:\n",
    "        attr_dict[attr] = set()\n",
    "    attr_dict[attr].update(set(round1_attr_value[:, i]))\n",
    "#     round1_attr_to_ind[attr] = i\n",
    "    \n",
    "for i, attr in enumerate(list(attr2_list[1].values)):\n",
    "    if attr not in attr_dict:\n",
    "        attr_dict[attr] = set()\n",
    "    attr_dict[attr].update(set(round2_attr_value[:, i]))\n",
    "#     round2_attr_to_ind[attr] = i\n",
    "    \n",
    "consant_attr = [attr for attr in attr_dict if len(attr_dict[attr]) == 1 ]\n",
    "# merge_attr_value = np.zeros((class_id_emb_attr.shape[0], len(attr_dict) - len(consant_attr)))\n",
    "# attr_ind = 0\n",
    "# attrs = []\n",
    "# for attr in attr_dict:\n",
    "#     if attr in consant_attr:\n",
    "#         continue\n",
    "#     if attr in attr1_list[1].values:\n",
    "#         round1_attr_array = round1_attr_value[:, round1_attr_to_ind[attr]]\n",
    "#     else:\n",
    "#         round1_attr_array = np.zeros(round1_attr_value.shape[0])\n",
    "#         round1_attr_array[:] = -1\n",
    "#     if attr in attr2_list[1].values:\n",
    "#         round2_attr_array = round2_attr_value[:, round2_attr_to_ind[attr]]\n",
    "#     else:\n",
    "#         round2_attr_array = np.zeros(round2_attr_value.shape[0])\n",
    "#         round2_attr_array[:] = -1\n",
    "#     merge_attr_value[:, attr_ind] = np.c_[round1_attr_array, round2_attr_array]\n",
    "#     attrs.append(attr)\n",
    "# sum(len(attr_dict[attr]) for attr in attr_dict)\n",
    "# merge_attr_value\n",
    "# sum(len(attr_dict[attr]) == 1 for attr in attr_dict)\n",
    "\n",
    "intersect_attrs = np.intersect1d(round1_attrs, round2_attrs)\n",
    "round1_attr_df = pd.DataFrame(round1_attr_value, columns = round1_attrs)\n",
    "round2_attr_df = pd.DataFrame(round2_attr_value, columns = round2_attrs)\n",
    "attr_df = pd.concat([round1_attr_df, round2_attr_df], sort = False).fillna(-1).drop(columns = consant_attr) #.astype('str')\n",
    "class_id_emb_attr['intersect_attr'] = list(attr_df[intersect_attrs].values)\n",
    "only_round2_attrs = list(set(round2_attrs) - set(intersect_attrs) - set(consant_attr))\n",
    "class_id_emb_attr['only_round2_attr'] = list(attr_df[only_round2_attrs].values)\n",
    "# attr_df = attr_df.apply(lambda s: s.name + '_' + s, axis = 0)\n",
    "# attr_df.values\n",
    "# attr_value_dict = dict((v, i) for i, v in enumerate(set(attr_df.values.flatten())))\n",
    "# encoded_attr_df = attr_df.apply(lambda s: [attr_value_dict[v] for v in s], axis = 0)\n",
    "# class_id_emb_attr['encoded_attr'] = list(encoded_attr_df[intersect_attrs].values)\n",
    "# encode_attr_min = encoded_attr_df.min().min()\n",
    "# encode_attr_max = encoded_attr_df.max().max()\n",
    "# encode_attr_min, encode_attr_max\n",
    "# class_id_emb_attr\n",
    "# encoded_attr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + '/pix72/round2B_class_id_emb_attr_ft2500_encodedAttr_intersectAttr.pkl', 'wb') as handle:\n",
    "    pickle.dump(class_id_emb_attr, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_id_emb_attr.iloc[0].only_round2_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intersect_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_data\n",
    "gc.collect()\n",
    "\n",
    "with open(path + '/pix72/round1A_train_img.pkl', 'rb') as handle:\n",
    "    round1_train_img_part0 = pickle.load(handle)\n",
    "with open(path + '/pix72/round1B_train_img.pkl', 'rb') as handle:\n",
    "    round1_train_img_part1 = pickle.load(handle)\n",
    "with open(path + '/pix72/round2A_train_img.pkl', 'rb') as handle:\n",
    "    round2_train_img = pickle.load(handle)\n",
    "with open(path + '/pix72/round2B_train_img.pkl', 'rb') as handle:\n",
    "    round2B_train_img = pickle.load(handle)\n",
    "# with open(path + '/round2B_test_img.pkl', 'rb') as handle:\n",
    "#     test_data = pickle.load(handle)\n",
    "with open(path + '/pix72/merge_train_img_pred.pkl', 'rb') as handle:\n",
    "    train_img_flat = pickle.load(handle)[:, :1032]\n",
    "# with open(path + '/test_data_img_flat_20181025_175212.pickle', 'rb') as handle:\n",
    "#     test_img_flat = pickle.load(handle)\n",
    "round2_class_id = ['ZJL' + str(i) for i in range(296, 521)]\n",
    "round2_train_class_id = round2_train_img.class_id.unique()\n",
    "train_data = pd.concat([\n",
    "            round1_train_img_part0, \n",
    "            round1_train_img_part1, \n",
    "            round2_train_img, \n",
    "            round2B_train_img,\n",
    "            ], axis = 0, sort = False).drop(columns = 'img')\n",
    "train_data['target'] = list(train_img_flat)\n",
    "# test_data['target'] = list(test_img_flat)\n",
    "del round1_train_img_part0, round1_train_img_part1, round2_train_img , train_img_flat, round2B_train_img, \n",
    "gc.collect()\n",
    "train_data = train_data.merge(class_id_emb_attr, how = 'left', on = 'class_id')\n",
    "# , test_img_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + '/LatestCorpus_skipgram_300_10Epoch/ft', 'rb') as handle:\n",
    "    x = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abacus 1\n",
      "ant 2\n",
      "brass 3\n",
      "bridge 4\n",
      "beacon 5\n",
      "book 6\n",
      "banjo 7\n",
      "basketball 8\n",
      "ball 9\n",
      "badminton 10\n",
      "bicycle 11\n",
      "boat 12\n",
      "bus 13\n",
      "bassoon 14\n",
      "boxing 15\n",
      "beagle 16\n",
      "beaver 17\n",
      "bear 18\n",
      "ballpoint pen 19\n",
      "bikini 20\n",
      "bison 21\n",
      "bee 22\n",
      "borzoi 23\n",
      "basenji 24\n",
      "central processing unit 25\n",
      "cinema 26\n",
      "clock 27\n",
      "cello 28\n",
      "camel 29\n",
      "cat 30\n",
      "coyote 31\n",
      "cannon 32\n",
      "cairn 33\n",
      "cactus 34\n",
      "drum 35\n",
      "dock 36\n",
      "dhole 37\n",
      "elephant 38\n",
      "flute 39\n",
      "association football 40\n",
      "fox 41\n",
      "flag 42\n",
      "file 43\n",
      "grape 44\n",
      "gorilla 45\n",
      "giant panda 46\n",
      "giraffe 47\n",
      "horse 48\n",
      "hammer 49\n",
      "harp 50\n",
      "hyena 51\n",
      "jeep 52\n",
      "jersey 53\n",
      "jaguar 54\n",
      "joystick 55\n",
      "keyboard 56\n",
      "kimono 57\n",
      "koala 58\n",
      "kiwi 59\n",
      "library 60\n",
      "lotus 61\n",
      "mouse 62\n",
      "monarch 63\n",
      "missile 64\n",
      "maze 65\n",
      "mushroom 66\n",
      "minivan 67\n",
      "oboe 68\n",
      "orange 69\n",
      "orangutan 70\n",
      "olive 71\n",
      "potato 72\n",
      "platypus 73\n",
      "penguin 74\n",
      "pickup 75\n",
      "plough 76\n",
      "flatworm 77\n",
      "pizza 78\n",
      "puck 79\n",
      "pig 80\n",
      "quail 81\n",
      "rocket 82\n",
      "rat 83\n",
      "rose 84\n",
      "rabbit 85\n",
      "snooker 86\n",
      "switch 87\n",
      "ski 88\n",
      "stethoscope 89\n",
      "submarine 90\n",
      "scorpion 91\n",
      "scale 92\n",
      "scabbard 93\n",
      "sapphire 94\n",
      "salamander 95\n",
      "television 96\n",
      "tank 97\n",
      "telephone 98\n",
      "torch 99\n",
      "tiger 100\n",
      "train 101\n",
      "tram 102\n",
      "transformer 103\n",
      "thresher 104\n",
      "truck 105\n",
      "valley 106\n",
      "violin 107\n",
      "vulture 108\n",
      "volleyball 109\n",
      "volcano 110\n",
      "web 111\n",
      "wheel 112\n",
      "wok 113\n",
      "wombat 114\n",
      "whippet 115\n",
      "lion 116\n",
      "chicken 117\n",
      "eagle 118\n",
      "turtle 119\n",
      "hippopotamus 120\n",
      "uniform 121\n",
      "deer 122\n",
      "frog 123\n",
      "banana 124\n",
      "pepper 125\n",
      "compass 126\n",
      "oak 127\n",
      "shield 128\n",
      "pot 129\n",
      "fence 130\n",
      "whistle 131\n",
      "crayfish 132\n",
      "shark 133\n",
      "beaker 134\n",
      "candle 135\n",
      "porcupine 136\n",
      "confectionery 137\n",
      "eggplant 138\n",
      "loudspeaker 139\n",
      "lobster 140\n",
      "asparagus 141\n",
      "hedgehog 142\n",
      "hay 143\n",
      "spoon 144\n",
      "barometer 145\n",
      "espresso 146\n",
      "coral 147\n",
      "ice cream 148\n",
      "pcb 149\n",
      "butterfly 150\n",
      "parachute 151\n",
      "pole 152\n",
      "jellyfish 153\n",
      "goose 154\n",
      "ferry 155\n",
      "swan 156\n",
      "crane 157\n",
      "airliner 158\n",
      "peach 159\n",
      "machine 160\n",
      "dam 161\n",
      "arch 162\n",
      "hog 163\n",
      "camera 164\n",
      "marimba 165\n",
      "stingray 166\n",
      "diaper 167\n",
      "unicycle 168\n",
      "trolleybus 169\n",
      "apron 170\n",
      "cicada 171\n",
      "cherry 172\n",
      "snail 173\n",
      "donkey 174\n",
      "mango 175\n",
      "apricot 176\n",
      "pineapple 177\n",
      "hvac 178\n",
      "rambutan 179\n",
      "dragonfly 180\n",
      "honeycomb 181\n",
      "broom 182\n",
      "dugong 183\n",
      "planetarium 184\n",
      "watch 185\n",
      "carp 186\n",
      "trilobite 187\n",
      "trifle 188\n",
      "peafowl 189\n",
      "fly 190\n",
      "van 191\n",
      "match 192\n",
      "admiral 193\n",
      "microphone 194\n",
      "broccoli 195\n",
      "umbrella 196\n",
      "chihuahua 197\n",
      "altar 198\n",
      "hornbill 199\n",
      "odometer 200\n",
      "sundial 201\n",
      "lilium 202\n",
      "stork 203\n",
      "pomegranate 204\n",
      "cougar 205\n",
      "longan 206\n",
      "carambola 207\n",
      "chimpanzee 208\n",
      "skunk 209\n",
      "lychee 210\n",
      "binoculars 211\n",
      "greenhouse 212\n",
      "barbershop 213\n",
      "quilt 214\n",
      "ipod 215\n",
      "muzzle 216\n",
      "perfume 217\n",
      "canoe 218\n",
      "obelisk 219\n",
      "thatching 220\n",
      "remote control 221\n",
      "gibbon 222\n",
      "barn 223\n",
      "maraca 224\n",
      "lasagne 225\n",
      "magpie 226\n",
      "projectile 227\n",
      "sports car 228\n",
      "ambulance 229\n",
      "syringe 230\n",
      "sink 231\n",
      "nipple 232\n",
      "persimmon 233\n",
      "bulbul 234\n",
      "hovercraft 235\n",
      "tractor 236\n",
      "alp 237\n",
      "pajamas 238\n",
      "cloak 239\n",
      "swing 240\n",
      "yawl 241\n",
      "sax 242\n",
      "racket 243\n",
      "buckeye 244\n",
      "cauliflower 245\n",
      "cucumber 246\n",
      "flamingo 247\n",
      "dome 248\n",
      "tick 249\n",
      "stocking 250\n",
      "sunglasses 251\n",
      "maypole 252\n",
      "chameleon 253\n",
      "terrier 254\n",
      "cliff 255\n",
      "backpack 256\n",
      "centipede 257\n",
      "convertible 258\n",
      "headphones 259\n",
      "tripod 260\n",
      "screwdriver 261\n",
      "laptop 262\n",
      "miniskirt 263\n",
      "meerkat 264\n",
      "swallow 265\n",
      "waffle 266\n",
      "woodpecker 267\n",
      "pretzel 268\n",
      "bagel 269\n",
      "tower 270\n",
      "teapot 271\n",
      "shop 272\n",
      "cuirass 273\n",
      "starfish 274\n",
      "bottle 275\n",
      "chiton 276\n",
      "wig 277\n",
      "forklift 278\n",
      "viaduct 279\n",
      "reef 280\n",
      "rule 281\n",
      "toaster 282\n",
      "mask 283\n",
      "widow 284\n",
      "throne 285\n",
      "limpkin 286\n",
      "thorax 287\n",
      "jacamar 288\n",
      "chair 289\n",
      "patas monkey 290\n",
      "alligator 291\n",
      "redshank 292\n",
      "table 293\n",
      "lolly 294\n",
      "conch 295\n",
      "hazelnut 296\n",
      "shoal 297\n",
      "impala 298\n",
      "jujube 299\n",
      "pan 300\n",
      "excavator 301\n",
      "square academic cap 302\n",
      "cradle 303\n",
      "brace 304\n",
      "purple mangosteen 305\n",
      "leatherback sea turtle 306\n",
      "harvester 307\n",
      "snowmobile 308\n",
      "crêpe 309\n",
      "lakeside 310\n",
      "dough 311\n",
      "jackfruit 312\n",
      "weevil 313\n",
      "desk 314\n",
      "apiary 315\n",
      "pitcher 316\n",
      "bucket 317\n",
      "vestment 318\n",
      "spindle 319\n",
      "clog 320\n",
      "cock 321\n",
      "battery 322\n",
      "gondola 323\n",
      "turnstile 324\n",
      "walkie-talkie 325\n",
      "barrow 326\n",
      "retriever 327\n",
      "fighter 328\n",
      "plate 329\n",
      "fountain 330\n",
      "trimaran 331\n",
      "nail 332\n",
      "marmot 333\n",
      "wagon 334\n",
      "menu 335\n",
      "sieve 336\n",
      "plunger 337\n",
      "chow 338\n",
      "yurt 339\n",
      "carbonara 340\n",
      "zucchini 341\n",
      "bow tie 342\n",
      "hatchet 343\n",
      "dog sled 344\n",
      "chain 345\n",
      "neuroptera 346\n",
      "spatula 347\n",
      "guacamole 348\n",
      "printer 349\n",
      "stick 350\n",
      "guenon 351\n",
      "sombrero 352\n",
      "poodle 353\n",
      "thimble 354\n",
      "bighorn sheep 355\n",
      "dwelling 356\n",
      "mousetrap 357\n",
      "patio 358\n",
      "stove 359\n",
      "loquat 360\n",
      "mower 361\n",
      "motorboat 362\n",
      "player 363\n",
      "cup 364\n",
      "stage 365\n",
      "ravioli 366\n",
      "barrel 367\n",
      "wallet 368\n",
      "safe 369\n",
      "church 370\n",
      "sarong 371\n",
      "isopoda 372\n",
      "ear 373\n",
      "freighter 374\n",
      "puma 375\n",
      "rhinoceros 376\n",
      "bathtub 377\n",
      "cardigan 378\n",
      "mailbox 379\n",
      "go-kart 380\n",
      "wreck 381\n",
      "sandal 382\n",
      "vault 383\n",
      "corkscrew 384\n",
      "washer 385\n",
      "acorn 386\n",
      "redbone 387\n",
      "poncho 388\n",
      "hartebeest 389\n",
      "earplug 390\n",
      "stopwatch 391\n",
      "gazelle 392\n",
      "bearskin 393\n",
      "crate 394\n",
      "grasshopper 395\n",
      "barbell 396\n",
      "spotlight 397\n",
      "refrigerator 398\n",
      "pinwheel 399\n",
      "coat 400\n",
      "minibus 401\n",
      "artichoke 402\n",
      "trunks 403\n",
      "reel 404\n",
      "capra (genus) 405\n",
      "dumbbell 406\n",
      "titi 407\n",
      "liner 408\n",
      "teddy 409\n",
      "handgun holster 410\n",
      "scoreboard 411\n",
      "swab 412\n",
      "pot pie 413\n",
      "puffer 414\n",
      "breastplate 415\n",
      "drumstick 416\n",
      "binder 417\n",
      "diamondback 418\n",
      "tub 419\n",
      "nest box 420\n",
      "lipstick 421\n",
      "ringlet 422\n",
      "hip 423\n",
      "seashore 424\n",
      "strawberry 425\n",
      "toy store 426\n",
      "cockroach 427\n",
      "ox 428\n",
      "crib 429\n",
      "lampshade 430\n",
      "polecat 431\n",
      "bookselling 432\n",
      "jug 433\n",
      "monkey 434\n",
      "bannister 435\n",
      "loaf 436\n",
      "hawthorn 437\n",
      "grand 438\n",
      "cleaver 439\n",
      "hourglass 440\n",
      "gown 441\n",
      "dog 442\n",
      "bullfrog 443\n",
      "agama 444\n",
      "albatross 445\n",
      "slot 446\n",
      "whiptail 447\n",
      "charger 448\n",
      "tabby cat 449\n",
      "bib 450\n",
      "hotdog 451\n",
      "tomato 452\n",
      "dishcloth 453\n",
      "promontory 454\n",
      "loupe 455\n",
      "phacochoerus 456\n",
      "organ 457\n",
      "terrapin 458\n",
      "packet 459\n",
      "car 460\n",
      "fig 461\n",
      "radio 462\n",
      "blueberry 463\n",
      "durian 464\n",
      "jean 465\n",
      "tarantula 466\n",
      "plane 467\n",
      "slug 468\n",
      "goldfish 469\n",
      "coccinellidae 470\n",
      "apple 471\n",
      "sock 472\n",
      "prison 473\n",
      "goat 474\n",
      "nematode 475\n",
      "basset 476\n",
      "watermelon 477\n",
      "baboon 478\n",
      "eel 479\n",
      "parrot 480\n",
      "lifeboat 481\n",
      "shepherd 482\n",
      "burrito 483\n",
      "pumpkin 484\n",
      "pug 485\n",
      "lemon 486\n",
      "snorkel 487\n",
      "limousine 488\n",
      "football 489\n",
      "chopsticks 490\n",
      "purse 491\n",
      "cricket 492\n",
      "cattle 493\n",
      "mantis 494\n",
      "projector 495\n",
      "constrictor 496\n",
      "balloon 497\n",
      "coho 498\n",
      "walnut 499\n",
      "mailbag 500\n",
      "rickshaw 501\n",
      "bag 502\n",
      "pterois 503\n",
      "grille 504\n",
      "hook 505\n",
      "hot pot 506\n"
     ]
    }
   ],
   "source": [
    "class_names = list(class_id_emb_attr.class_name.values)\n",
    "redirect_classs = ['ballpoint', 'bighorn', 'birdhouse', 'bookshop', 'bowtie', 'boxer', 'chest', \n",
    "                   'chopstick', 'cpu', 'crepe', 'dishrag', 'dogsled', 'earphones', 'flagpole', \n",
    "                   'heater', 'holster', 'hotpot', 'ibex', 'icecream', 'isopod', 'lacewing', \n",
    "                   'ladybug', 'lasagna', 'leatherback', 'lily', 'lionfish', 'mangosteen', \n",
    "                   'matchstick', 'meter', 'mortarboard', 'ostrich', 'pajama', 'panda', \n",
    "                   'patas', 'peacock', 'phone', 'plow', 'potpie', 'remote-control', 'sandbar', \n",
    "                   'soccer', 'speedboat', 'sportscar', 'strainer', 'tabby', 'thatch', 'toyshop', \n",
    "                   'upright', 'warthog', 'washbasin']\n",
    "\n",
    "alter_redirect_names = [\n",
    "    'Ballpoint pen', 'Bighorn sheep', 'Bookselling', 'Nest box', 'Bow tie', 'Boxing', 'Thorax',  'Chopsticks', \n",
    "    'Central processing unit', 'Crêpe', 'Dishcloth', 'Dog sled', 'Headphones', 'Flag', 'HVAC', 'Handgun holster',\n",
    "    'Hot pot', 'Capra (genus)', 'Ice cream', 'Isopoda', 'Neuroptera', 'Coccinellidae', 'Lasagne', 'Leatherback sea turtle',\n",
    "    'Lilium', 'Pterois', 'Purple mangosteen', 'Match', '?', 'Square academic cap', 'Ostrich', 'Pajamas', 'Giant panda', \n",
    "    'Patas monkey', 'Peafowl', 'Telephone', 'Plough', 'Pot pie', 'Remote control', 'Shoal', 'Association football',\n",
    "    'Motorboat', 'Sports car', 'Sieve', 'Tabby cat', 'Thatching', 'Toy store', '?', 'Phacochoerus', 'Sink'\n",
    "]\n",
    "alter_redirect_names = [s.lower() for s in alter_redirect_names]\n",
    "redirect_classs_pair = dict(zip(redirect_classs, alter_redirect_names))\n",
    "reverse_redirect_classs_pair = dict((redirect_classs_pair[k], k) for k in redirect_classs_pair.keys())\n",
    "dirs = glob.glob('/mnt/d/personal/wiki_text/*')\n",
    "class_corpus = {}\n",
    "cur_title = ''\n",
    "for dir in dirs:\n",
    "    files = glob.glob(dir + '/*')\n",
    "    for f in files:\n",
    "        with open(f, 'r') as h:\n",
    "            for line in h:\n",
    "                if line.startswith('<doc id='):\n",
    "                    ls = line.split('\"')\n",
    "                    title = ls[-2].lower()\n",
    "                    if title in class_names or title in redirect_classs_pair.values():\n",
    "                        if title in class_names:\n",
    "                            cur_title = title\n",
    "                        else:\n",
    "                            cur_title = reverse_redirect_classs_pair[title]\n",
    "                        if cur_title not in class_corpus:\n",
    "                            class_corpus[cur_title] = ''\n",
    "                            print (title, len(class_corpus))\n",
    "                        if len(class_corpus) == len(class_names):\n",
    "                            print ('done')\n",
    "                    else:\n",
    "                        cur_title = ''\n",
    "                elif cur_title != '':\n",
    "                    class_corpus[cur_title] += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_corpus_norm = dict((k, v.replace('\\n', ' ').replace(',', ' ')) for k, v in class_corpus.items())\n",
    "pd.DataFrame(class_corpus_norm, index = [0]).T.to_csv(path + 'class_desc', header = None)\n",
    "# pd.DataFrame(class_corpus_norm, index = [0]).T.index.to_csv(path + 'class_desc.index', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meter', 'ostrich', 'phone', 'upright'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(class_names) - set(class_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12],\n",
       "       [13],\n",
       "       [24]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(class_id_emb_attr[class_id_emb_attr.class_id == 'ZJL195'].attr.values[0] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../data//semifinal_image_phase2/class_wordembeddings.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c5aa2f399ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclass_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mglove_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_class_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/semifinal_image_phase2/class_wordembeddings.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# fasttext_emb =  read_class_emb(path + '/External/class_wordembeddings_fasttext')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-c5aa2f399ec6>\u001b[0m in \u001b[0;36mread_class_emb\u001b[0;34m(class_emb_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_class_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_emb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclass_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_emb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mclass_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclass_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclass_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../data//semifinal_image_phase2/class_wordembeddings.txt' does not exist"
     ]
    }
   ],
   "source": [
    "def read_class_emb(class_emb_path):\n",
    "    class_emb = pd.read_csv(class_emb_path, index_col = 0, sep = ' ', header = None)\n",
    "    class_emb.index.name = 'class_name'\n",
    "    class_emb = class_emb.apply(lambda s: np.array([float(x) for x in s])[:300], axis = 1)\n",
    "    return class_emb\n",
    "\n",
    "glove_emb = read_class_emb(path + '/semifinal_image_phase2/class_wordembeddings.txt')\n",
    "# fasttext_emb =  read_class_emb(path + '/External/class_wordembeddings_fasttext')\n",
    "\n",
    "class_id_to_name = pd.read_csv(path + '/semifinal_image_phase2/label_list.txt', \n",
    "                            index_col = 'class_name', sep = '\\t', header = None, names = ['class_id', 'class_name'])\n",
    "\n",
    "attr_list = pd.read_csv(path + '/semifinal_image_phase2/attribute_list.txt', index_col = 0, sep = '\\t', header = None)\n",
    "\n",
    "attributes_per_class = pd.read_csv(path + '/semifinal_image_phase2/attributes_per_class.txt', \n",
    "                                index_col = 0, sep = '\\t', header = None)\n",
    "attributes_per_class.index.name = 'class_id'\n",
    "attributes_per_class = attributes_per_class.apply(lambda s: np.array([float(x) for x in s]), axis = 1)\n",
    "\n",
    "round2B_class_id_emb_attr = class_id_to_name.copy()\n",
    "round2B_class_id_emb_attr['emb'] = glove_emb\n",
    "# class_id_emb_attr['emb_fasttext'] = fasttext_emb\n",
    "# class_id_emb_attr['emb'] = class_id_emb_attr.apply(lambda s: np.hstack([s['emb_glove'], s['emb_fasttext']]), axis = 1)\n",
    "round2B_class_id_emb_attr.reset_index(inplace = True)\n",
    "round2B_class_id_emb_attr.set_index('class_id', inplace = True)\n",
    "round2B_class_id_emb_attr['attr'] = attributes_per_class\n",
    "round2B_class_id_emb_attr.reset_index(inplace = True)\n",
    "# print ('Load class_id_emb_attr Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round2_class_id = ['ZJL' + str(i) for i in range(296, 501)]\n",
    "# round2B_class_id_emb_attr = pd.concat([class_id_emb_attr[~class_id_emb_attr.class_id.isin(round2_class_id)], round2B_class_id_emb_attr])\n",
    "with open(path + 'round2_class_id_emb_attr_ft1300.pkl', 'wb') as handle:\n",
    "    pickle.dump(class_id_emb_attr, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31979"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(round2B_train_img.img_id) - set(round2_train_img.img_id))\n",
    "# round2B_train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 83/38221 [00:00<00:46, 824.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load setA_train_data ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38221/38221 [00:39<00:00, 973.13it/s]\n",
      "  0%|          | 104/49028 [00:00<00:47, 1039.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load setB_train_data ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49028/49028 [00:48<00:00, 1003.10it/s]\n",
      "100%|██████████| 31896/31896 [04:28<00:00, 118.90it/s]\n",
      "100%|██████████| 31979/31979 [01:54<00:00, 164.07it/s]\n",
      "  0%|          | 20/8024 [00:00<00:42, 190.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load test_data ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8024/8024 [00:59<00:00, 135.04it/s]\n"
     ]
    }
   ],
   "source": [
    "def read_image_data(img_id_path, imag_path, cols, debug):\n",
    "    img_data = pd.read_csv(img_id_path, sep = '\\t', header = None, names = cols)\n",
    "    if debug:\n",
    "        img_data = img_data[:2000]\n",
    "    img_data['img'] = img_data['img_id'].progress_apply(lambda id: \n",
    "                            image.img_to_array(image.load_img(imag_path + id, target_size = (72, 72, 3))))\n",
    "    return img_data\n",
    "\n",
    "print ('Load setA_train_data ----')\n",
    "setA_train_data = read_image_data(img_id_path = path + '/DatasetA/train.txt', imag_path = path + '/DatasetA/train/', \n",
    "            cols = ['img_id', 'class_id'], debug = False)\n",
    "print ('Load setB_train_data ----')\n",
    "setB_train_data = read_image_data(img_id_path = path + '/DatasetB/train.txt', imag_path = path + '/DatasetB/train/', \n",
    "            cols = ['img_id', 'class_id'], debug = False)\n",
    "                                  \n",
    "round2A_train_data = read_image_data(img_id_path = path + '/round2_DatasetA_20180927/train.txt', \n",
    "                                  imag_path = path + '/round2_DatasetA_20180927/train/', \n",
    "            cols = ['img_id', 'class_id'], debug = False)\n",
    "\n",
    "round2B_train_data = read_image_data(img_id_path = path + '/semifinal_image_phase2/train.txt', \n",
    "                                  imag_path = path + '/semifinal_image_phase2/train/', \n",
    "            cols = ['img_id', 'class_id'], debug = False)\n",
    "# print ('Load setB_train_data ----')\n",
    "# setB_train_data = read_image_data(img_id_path = path + '/DatasetB/train.txt', imag_path = path + '/DatasetB/train/', \n",
    "#             cols = ['img_id', 'class_id'], debug = FLAGS.debug)\n",
    "# train_data = setA_train_data.append(setB_train_data)\n",
    "# del setA_train_data, setB_train_data\n",
    "# train_data = train_data.merge(class_id_emb_attr, how = 'left', on = 'class_id')\n",
    "\n",
    "print ('Load test_data ----')\n",
    "test_data = read_image_data(img_id_path = path + '/semifinal_image_phase2/image.txt', \n",
    "                            imag_path = path + '/semifinal_image_phase2/test/', \n",
    "            cols = ['img_id'], debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'pix72/round1A_train_img.pkl', 'wb') as handle:\n",
    "    pickle.dump(setA_train_data, handle)\n",
    "with open(path + 'pix72/round1B_train_img.pkl', 'wb') as handle:\n",
    "    pickle.dump(setB_train_data, handle)\n",
    "with open(path + 'pix72/round2A_train_img.pkl', 'wb') as handle:\n",
    "    pickle.dump(round2A_train_data, handle)\n",
    "with open(path + 'pix72/round2B_train_data.pkl', 'wb') as handle:\n",
    "    pickle.dump(round2B_train_data, handle)\n",
    "with open(path + 'pix72/round2B_test_data.pkl', 'wb') as handle:\n",
    "    pickle.dump(test_data, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text =  pd.read_csv(path + '/External/class_wordembeddings_glove', \n",
    "                        index_col = 0, sep = ' ', header = None)\n",
    "fast_text.index.name = 'class_name'\n",
    "fast_text_df = pd.DataFrame(index = fast_text.index)\n",
    "fast_text_df['emb_glove_crawl'] = fast_text.apply(lambda s: np.array([float(x) for x in s])[:300], axis = 1)\n",
    "\n",
    "train_data = train_data.merge(fast_text_df, how = 'left', on = 'class_name')\n",
    "\n",
    "# train_data['emb'] = train_data.apply(lambda s: np.hstack([s['emb'], s['emb_glove_crawl']]), axis = 1)\n",
    "\n",
    "class_id_emb_attr = class_id_emb_attr.merge(fast_text_df, how = 'left', on = 'class_name')\n",
    "class_id_emb_attr['emb'] = class_id_emb_attr.apply(lambda s: np.hstack([s['emb'], s['emb_glove_crawl']]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "class_names = class_id_emb_attr.class_name.values\n",
    "glove_dict = {}\n",
    "with open(path + 'GloveEmb/vectors_200.txt') as f:\n",
    "    for line in f:\n",
    "        ls = line.split(' ')\n",
    "        if ls[0] in class_names:\n",
    "            glove_dict[ls[0]] = np.array([float(x) for x in ls[1:]])\n",
    "            if len(glove_dict) == class_names.shape[0]:\n",
    "                print ('done')\n",
    "\n",
    "class_id_emb_attr['Glove'] = class_id_emb_attr['class_name'].apply(lambda c: ' '.join(str(x) for x in glove_dict[c]))\n",
    "class_id_emb_attr[['class_name', 'Glove']].to_csv(path + '/ft_class_embs/LatestCorpus_glove_200', index = False)\n",
    "# class_id_emb_attr['emb'] = class_id_emb_attr.apply(lambda s: np.hstack([s['emb'], s['Glove']]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2472222222222222"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "167 * 70 /3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'round2_class_id_emb_attr_add_golve_crawl.pkl', 'wb') as handle:\n",
    "    pickle.dump(class_id_emb_attr, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, scores = None, cat_max = None, flags = None, model_type = None):\n",
    "        self.batch_size = 128 #flags.densenet_batch_size\n",
    "        self.epochs = 200 #flags.densenet_epochs\n",
    "        self.patience = 30 #flags.densenet_patience\n",
    "        self.scores = scores\n",
    "        self.cat_max = cat_max\n",
    "        self.model_type = model_type\n",
    "        self.aug_data = True #flags.aug_data\n",
    "        self.lr = 1e-3 #flags.lr\n",
    "        self.verbose = 2 #flags.train_verbose\n",
    "        self.OneHotEncoder = sklearn.preprocessing.OneHotEncoder()\n",
    "        self.model = self.small_densenet(\n",
    "                blocks = [6, 12, 24, 16], #[int(b.strip()) for b in flags.blocks.strip().split(',')], \n",
    "                weight_decay = 1e-4, #flags.weight_decay, \n",
    "                kernel_initializer = 'he_normal', #flags.kernel_initializer,\n",
    "                init_filters = 128, #flags.init_filters,\n",
    "                reduction = 0.5, #flags.reduction,\n",
    "                growth_rate = 32, #flags.growth_rate,\n",
    "                init_stride = 1 #flags.init_stride\n",
    "                )\n",
    "\n",
    "    def dense_block(self, x, blocks, name, \n",
    "            weight_decay = 1e-4, \n",
    "            kernel_initializer = 'he_normal',\n",
    "            growth_rate = None):\n",
    "        \"\"\"A dense block.\n",
    "        # Arguments\n",
    "            x: input tensor.\n",
    "            blocks: integer, the number of building blocks.\n",
    "            name: string, block label.\n",
    "        # Returns\n",
    "            output tensor for the block.\n",
    "        \"\"\"\n",
    "        for i in range(blocks):\n",
    "            x = self.conv_block(x, growth_rate, name=name + '_block' + str(i + 1), \n",
    "                weight_decay = weight_decay,\n",
    "                kernel_initializer = kernel_initializer)\n",
    "        return x\n",
    "\n",
    "    def transition_block(self, x, reduction, name, weight_decay = 1e-4, kernel_initializer = 'he_normal'):\n",
    "        \"\"\"A transition block.\n",
    "        # Arguments\n",
    "            x: input tensor.\n",
    "            reduction: float, compression rate at transition layers.\n",
    "            name: string, block label.\n",
    "        # Returns\n",
    "            output tensor for the block.\n",
    "        \"\"\"\n",
    "        bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "        x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                    name=name + '_bn')(x)\n",
    "        x = layers.Activation('relu', name=name + '_relu')(x)\n",
    "        x = layers.Conv2D(int(K.int_shape(x)[bn_axis] * reduction), 1,\n",
    "                        use_bias=False,\n",
    "                        kernel_initializer = kernel_initializer,\n",
    "                        kernel_regularizer = l2(weight_decay),\n",
    "                        name=name + '_conv')(x)\n",
    "        x = layers.AveragePooling2D(2, strides=2, name=name + '_pool')(x)\n",
    "        return x\n",
    "\n",
    "    def conv_block(self, x, growth_rate, name, weight_decay = 1e-4, kernel_initializer = 'he_normal'):\n",
    "        \"\"\"A building block for a dense block.\n",
    "        # Arguments\n",
    "            x: input tensor.\n",
    "            growth_rate: float, growth rate at dense layers.\n",
    "            name: string, block label.\n",
    "        # Returns\n",
    "            Output tensor for the block.\n",
    "        \"\"\"\n",
    "        bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "        x1 = layers.BatchNormalization(axis=bn_axis,\n",
    "                                    epsilon=1.001e-5,\n",
    "                                    name=name + '_0_bn')(x)\n",
    "        x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
    "        x1 = layers.Conv2D(4 * growth_rate, 1,\n",
    "                        use_bias=False,\n",
    "                        kernel_initializer = kernel_initializer,\n",
    "                        kernel_regularizer = l2(weight_decay),\n",
    "                        name=name + '_1_conv')(x1)\n",
    "        x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                    name=name + '_1_bn')(x1)\n",
    "        x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
    "        x1 = layers.Conv2D(growth_rate, 3,\n",
    "                        padding='same',\n",
    "                        use_bias=False,\n",
    "                        kernel_initializer = kernel_initializer,\n",
    "                        kernel_regularizer = l2(weight_decay),\n",
    "                        name=name + '_2_conv')(x1)\n",
    "        x = layers.Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
    "        return x\n",
    "\n",
    "    def small_densenet(self, img_input_shape = (72, 72, 3), \n",
    "        blocks = [6, 12, 24, 16], \n",
    "        weight_decay = 1e-4, \n",
    "        kernel_initializer = 'he_normal',\n",
    "        init_filters = None,\n",
    "        reduction = None,\n",
    "        growth_rate = None,\n",
    "        init_stride = None\n",
    "        ):\n",
    "        img_input = layers.Input(shape = (img_input_shape))\n",
    "\n",
    "        x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n",
    "        x = layers.Conv2D(init_filters, 3, strides=init_stride, use_bias=False, \n",
    "            kernel_initializer = kernel_initializer, \n",
    "            kernel_regularizer = l2(weight_decay),\n",
    "            name='conv1/conv')(x)\n",
    "        x = layers.BatchNormalization(\n",
    "            axis=3, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "        x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "        x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
    "        x = layers.AveragePooling2D(3, strides=2, name='pool1')(x)\n",
    "        \n",
    "        for i, block in enumerate(blocks):\n",
    "            scope_num_str = str(i + 2)\n",
    "            x = self.dense_block(x, block, name='conv' + scope_num_str, \n",
    "                                 growth_rate = growth_rate,\n",
    "                                 weight_decay = weight_decay, \n",
    "                                 kernel_initializer = kernel_initializer)\n",
    "            if i != len(blocks) - 1:\n",
    "                x = self.transition_block(x, reduction, name='pool' + scope_num_str, \n",
    "                                          weight_decay = weight_decay, kernel_initializer = kernel_initializer)\n",
    "        x = layers.BatchNormalization(\n",
    "            axis=3, epsilon=1.001e-5, name='bn')(x)\n",
    "        x = layers.Activation('relu', name='relu')(x)\n",
    "\n",
    "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = layers.Dense(self.cat_max, activation='softmax',\n",
    "            kernel_initializer = kernel_initializer, \n",
    "            name='fc')(x)\n",
    "        \n",
    "        model = Model(img_input, x)\n",
    "        model.compile(optimizer = Adam(lr=self.lr), loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def DNN_DataSet(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return preprocess_img(df['img'])\n",
    "\n",
    "    def train(self, train_part_df, train_part_label, validate_part_df, validate_part_label):\n",
    "        \"\"\"\n",
    "        Keras Training\n",
    "        \"\"\"\n",
    "        print(\"-----DNN training-----\")\n",
    "\n",
    "        DNN_Train_Data = self.DNN_DataSet(train_part_df)\n",
    "        DNN_validate_Data = self.DNN_DataSet(validate_part_df)\n",
    "\n",
    "        callbacks = [\n",
    "                EarlyStopping(monitor='val_categorical_accuracy', patience=self.patience, verbose=0),\n",
    "                ]\n",
    "        if self.aug_data:\n",
    "            datagen = preprocessing.image.ImageDataGenerator(\n",
    "                    rotation_range=45,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True)\n",
    "\n",
    "            datagen.fit(DNN_Train_Data)\n",
    "\n",
    "            h = self.model.fit_generator(datagen.flow(DNN_Train_Data, train_part_label, batch_size=self.batch_size), \n",
    "                    validation_data=(DNN_validate_Data, validate_part_label), steps_per_epoch = DNN_Train_Data.shape[0]//self.batch_size,\n",
    "                    epochs=self.epochs, shuffle=True, verbose = self.verbose, workers=1, use_multiprocessing=False, \n",
    "                    callbacks=callbacks)\n",
    "        else:\n",
    "            h = self.model.fit(DNN_Train_Data, train_part_label, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                        shuffle=True, verbose=self.verbose,\n",
    "                        validation_data=(DNN_validate_Data, validate_part_label)\n",
    "                        , callbacks=callbacks\n",
    "                        )\n",
    "        self.scores.append(pd.DataFrame(h.history))\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, test_part, batch_size = 1024, verbose=2):\n",
    "        \"\"\"\n",
    "        Keras Training\n",
    "        \"\"\"\n",
    "        print(\"-----DNN Test-----\")\n",
    "        pred = self.model.predict(self.DNN_DataSet(test_part), verbose=verbose)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119145/119145 [==============================] - 12271s \n",
      "7817/7817 [==============================] - 817s   \n"
     ]
    }
   ],
   "source": [
    "img_model = DenseNet(scores = None, \n",
    "                     cat_max = 205, #flags.cat_max, \n",
    "                     flags = None, \n",
    "                     model_type = 'DenseNet').model\n",
    "# model_file_name = glob.glob(model_path + '/imgmodel_*.h5')[0]\n",
    "# print ('Model file name: ', model_file_name)\n",
    "# img_model.load_weights(model_file_name)\n",
    "img_model.load_weights('../../Data/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/model_0_2018_09_24_03_07_15.h5')\n",
    "# img_model_flat = Model(input = img_model.input, output = img_model.get_layer(name = 'avg_pool').output)\n",
    "train_data['target'] = list(model_eval(img_model, 'DenseNet', train_data))\n",
    "test_data['target'] = list(model_eval(img_model, 'DenseNet', test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 5, 7],\n",
       "       [2, 8, 6],\n",
       "       [7, 6, 4]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(1, 10, (3, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-724ce3e98dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "np.random.choice(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_id_emb_attr.iloc[0].emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def extract_array_from_series(s):\n",
    "    return np.asarray(list(s))\n",
    "\n",
    "def create_dem_data(df, only_emb = False, emb_len = 1200, model_type = 'DEM'):\n",
    "    dem_data = []\n",
    "    emb = extract_array_from_series(df['emb'])[:, :]\n",
    "    if only_emb:\n",
    "        attr = np.zeros((emb.shape[0], 22))\n",
    "        dem_data.append(attr)\n",
    "    else:\n",
    "        attr = extract_array_from_series(df['attr'])[:, :]\n",
    "    dem_data.append(emb)\n",
    "    if model_type == 'RCNN_DEM_BC':\n",
    "        desc = extract_array_from_series(df['desc'])\n",
    "        dem_data.append(desc)\n",
    "    return dem_data\n",
    "\n",
    "def create_gcn_data(df, class_to_id):\n",
    "    return np.array([class_to_id[c] for c in df['class_id'].values]).astype('int32')\n",
    "\n",
    "def create_qfsl_data(df, class_to_id, categories = 205, ns = None):\n",
    "    OneHotEncoder = sklearn.preprocessing.OneHotEncoder(n_values = categories )\n",
    "    train_target = df['class_id'].apply(lambda c: class_to_id[c]).values\n",
    "    train_target = OneHotEncoder.fit_transform(np.reshape(train_target, (-1, 1))).toarray()\n",
    "    class_weight = np.ones(train_target.shape)\n",
    "    if ns is not None:\n",
    "#         in_loss_proba = (ns + 1)/ train_target.shape[1]\n",
    "        class_weight = np.random.choice([0, 1], train_target.shape, p = [1 - ns, ns])\n",
    "        class_weight[np.argwhere(train_target == 1).T] = 1\n",
    "    return [extract_array_from_series(df['target']), train_target, class_weight]\n",
    "\n",
    "def neg_aug_data(pos_data, train_class_id, class_id_emb_attr = None, c2c_neg_cnt = None, only_emb = False):\n",
    "    if c2c_neg_cnt is None:\n",
    "        pos_len = pos_data[0].shape[0]\n",
    "        ind_array = np.array(range(pos_len))\n",
    "    #     rs = np.random.RandomState(seed=420)\n",
    "    #     perm_ind_array = rs.permutation(rs.permutation(rs.permutation(rs.permutation(ind_array))))\n",
    "        perm_ind_array = np.random.permutation(ind_array)\n",
    "        perm_label = np.zeros(pos_len, dtype = 'float')\n",
    "        perm_label[train_class_id == train_class_id[perm_ind_array]] = 1\n",
    "        perm_img_feature_map = pos_data[0][perm_ind_array] \n",
    "        perm_label = perm_label.reshape((pos_len, 1))\n",
    "#         print (perm_label.shape)\n",
    "        neg_data = [perm_img_feature_map] + pos_data[1:-1] + [perm_label]\n",
    "    else:\n",
    "        train_class_id_uniq = np.unique(train_class_id)\n",
    "        attr_embs = create_dem_data(class_id_emb_attr, only_emb)\n",
    "        neg_attrs = []\n",
    "        neg_embs = []\n",
    "        neg_imgs = []\n",
    "        img_origin_ind = np.array(range(train_class_id.shape[0]))\n",
    "        for i, class_id in enumerate(list(class_id_emb_attr.class_id)):\n",
    "            per_class_attrs,  per_class_embs = attr_embs[0][i], attr_embs[1][i]\n",
    "            per_class_neg_imgs = []\n",
    "            for have_img_clas_id in train_class_id_uniq:\n",
    "                if class_id == have_img_clas_id:\n",
    "                    continue\n",
    "                imgs_ind = img_origin_ind[train_class_id == have_img_clas_id]\n",
    "                per_class_neg_imgs.extend(list(pos_data[0][np.random.choice(imgs_ind, c2c_neg_cnt)]))\n",
    "            per_class_neg_len = len(per_class_neg_imgs)\n",
    "#             print ('Class_id, neg', class_id, per_class_neg_len)\n",
    "            neg_attrs.extend([per_class_attrs] * per_class_neg_len)\n",
    "            neg_embs.extend([per_class_embs] * per_class_neg_len)\n",
    "            neg_imgs.extend(per_class_neg_imgs)\n",
    "        neg_data = [np.array(neg_imgs), np.array(neg_attrs), np.array(neg_embs), np.zeros(len(neg_imgs))]\n",
    "    return neg_data\n",
    "        \n",
    "def create_dem_bc_data(df, neg_aug = 0, only_emb = False, class_id_emb_attr = None, \n",
    "                       c2c_neg_cnt = None, model_type = 'DEM_BC'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    train_data = [extract_array_from_series(df['target'])[:, :]] + create_dem_data(df, only_emb, model_type = model_type)\n",
    "    train_len = train_data[0].shape[0]\n",
    "    train_data = train_data + [np.ones((train_len, 1), dtype = 'float')]\n",
    "    merge_data = train_data\n",
    "    if neg_aug > 0:\n",
    "        train_class_id = extract_array_from_series(df['class_id'])\n",
    "        for i in range(neg_aug):\n",
    "            neg_data = neg_aug_data(train_data, train_class_id, \n",
    "                                    class_id_emb_attr = class_id_emb_attr, \n",
    "                                    c2c_neg_cnt = c2c_neg_cnt,\n",
    "                                    only_emb = only_emb)\n",
    "            merge_data = [np.r_[merge_data[i], neg_data[i]] for i in range(len(merge_data))]\n",
    "        print ('DEM BC Data Train Len, Pos, Neg:', train_len, np.sum(merge_data[-1]), np.sum(merge_data[-1] == 0))\n",
    "        return merge_data\n",
    "    else:\n",
    "        return train_data\n",
    "    \n",
    "def create_dem_bc_aug_data(df, neg_aug = 0, only_emb = False, class_id_emb_attr = None, c2c_neg_cnt = None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    train_data = [extract_array_from_series(df['img'])] + create_dem_data(df, only_emb)\n",
    "    train_len = train_data[0].shape[0]\n",
    "    train_data = train_data + [np.ones((train_len, 1), dtype = 'float')]\n",
    "    merge_data = train_data\n",
    "    if neg_aug > 0:\n",
    "        train_class_id = extract_array_from_series(df['class_id'])\n",
    "        for i in range(neg_aug):\n",
    "            neg_data = neg_aug_data(train_data, train_class_id, \n",
    "                                    class_id_emb_attr = class_id_emb_attr, \n",
    "                                    c2c_neg_cnt = c2c_neg_cnt,\n",
    "                                    only_emb = only_emb)\n",
    "            merge_data = [np.r_[merge_data[i], neg_data[i]] for i in range(len(merge_data))]\n",
    "        print ('DEM BC Data Train Len, Pos, Neg:', train_len, np.sum(merge_data[-1]), np.sum(merge_data[-1] == 0))\n",
    "        return merge_data\n",
    "    else:\n",
    "        return train_data\n",
    "    \n",
    "def preprocess_numpy_input(x, data_format = 'channels_last', mode = 'torch', **kwargs):\n",
    "    \"\"\"Preprocesses a Numpy array encoding a batch of images.\n",
    "\n",
    "    # Arguments\n",
    "        x: Input array, 3D or 4D.\n",
    "        data_format: Data format of the image array.\n",
    "        mode: One of \"caffe\", \"tf\" or \"torch\".\n",
    "            - caffe: will convert the images from RGB to BGR,\n",
    "                then will zero-center each color channel with\n",
    "                respect to the ImageNet dataset,\n",
    "                without scaling.\n",
    "            - tf: will scale pixels between -1 and 1,\n",
    "                sample-wise.\n",
    "            - torch: will scale pixels between 0 and 1 and then\n",
    "                will normalize each channel with respect to the\n",
    "                ImageNet dataset.\n",
    "\n",
    "    # Returns\n",
    "        Preprocessed Numpy array.\n",
    "    \"\"\"\n",
    "    if not issubclass(x.dtype.type, np.floating):\n",
    "        x = x.astype(K.floatx(), copy=False)\n",
    "\n",
    "    if mode == 'tf':\n",
    "        x /= 127.5\n",
    "        x -= 1.\n",
    "        return x\n",
    "\n",
    "    if mode == 'torch':\n",
    "        x /= 255.\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    else:\n",
    "        if data_format == 'channels_first':\n",
    "            # 'RGB'->'BGR'\n",
    "            if x.ndim == 3:\n",
    "                x = x[::-1, ...]\n",
    "            else:\n",
    "                x = x[:, ::-1, ...]\n",
    "        else:\n",
    "            # 'RGB'->'BGR'\n",
    "            x = x[..., ::-1]\n",
    "        mean = [103.939, 116.779, 123.68]\n",
    "        std = None\n",
    "\n",
    "    # Zero-center by mean pixel\n",
    "    if data_format == 'channels_first':\n",
    "        if x.ndim == 3:\n",
    "            x[0, :, :] -= mean[0]\n",
    "            x[1, :, :] -= mean[1]\n",
    "            x[2, :, :] -= mean[2]\n",
    "            if std is not None:\n",
    "                x[0, :, :] /= std[0]\n",
    "                x[1, :, :] /= std[1]\n",
    "                x[2, :, :] /= std[2]\n",
    "        else:\n",
    "            x[:, 0, :, :] -= mean[0]\n",
    "            x[:, 1, :, :] -= mean[1]\n",
    "            x[:, 2, :, :] -= mean[2]\n",
    "            if std is not None:\n",
    "                x[:, 0, :, :] /= std[0]\n",
    "                x[:, 1, :, :] /= std[1]\n",
    "                x[:, 2, :, :] /= std[2]\n",
    "    else:\n",
    "        x[..., 0] -= mean[0]\n",
    "        x[..., 1] -= mean[1]\n",
    "        x[..., 2] -= mean[2]\n",
    "        if std is not None:\n",
    "            x[..., 0] /= std[0]\n",
    "            x[..., 1] /= std[1]\n",
    "            x[..., 2] /= std[2]\n",
    "    return x\n",
    "\n",
    "def preprocess_img(img_series):\n",
    "    return preprocess_numpy_input(extract_array_from_series(img_series))\n",
    "\n",
    "def multi_labels_cross_entropy(y_true, y_preds, eps = 1e-6):\n",
    "#     multi_labels_loss = [sklearn.metrics.log_loss]\n",
    "#     y_preds_clip = max(eps, min(1 - eps, y_preds))\n",
    "    y_preds_clip = np.clip(y_preds, eps, 1- eps)\n",
    "    multi_loss = -(y_true * np.log(y_preds_clip) + (1 - y_true) * np.log(1 - y_preds_clip))\n",
    "    return np.mean(multi_loss, axis = -1)\n",
    "\n",
    "def find_nearest_class(class_id_emb_attr, eval_df, cand_feature_map = None, img_feature_map = None,\n",
    "                      model_type = None, attr_preds = None, zs_model = None, dis_arr = None):\n",
    "    nearest_class_id = ['ZJL'] * img_feature_map.shape[0]\n",
    "#     norm_img_feature_map = sklearn.preprocessing.normalize(img_feature_map)\n",
    "    for i in range(img_feature_map.shape[0]):\n",
    "        if model_type == 'I2A':\n",
    "            dis = multi_labels_cross_entropy(extract_array_from_series(class_id_emb_attr['attr'])[:, :50],\n",
    "                                            attr_preds[i])\n",
    "        elif model_type == 'DEM_BC' or model_type == 'DEM_BC_AUG' or model_type == 'RCNN_DEM_BC':\n",
    "            img = img_feature_map[i]\n",
    "            pred_data = cand_feature_map * img\n",
    "#             pred_data = np.c_[cand_feature_map, [img] * cand_feature_map.shape[0]]\n",
    "            dis = 1 - zs_model.predict(pred_data)\n",
    "        elif model_type == 'lgb':\n",
    "            img = img_feature_map[i]\n",
    "            pred_data = cand_feature_map * img\n",
    "            dis = 1 - zs_model.predict(pred_data, num_iteration = -1)\n",
    "        elif model_type == 'QFSL':\n",
    "            dis = 1 - dis_arr[i]\n",
    "        else:\n",
    "#             print ('find')\n",
    "            img = img_feature_map[i]\n",
    "            dis = np.linalg.norm(img - cand_feature_map, axis = 1)\n",
    "        min_ind = np.where(dis == np.amin(dis))[0]\n",
    "        nearest_class_id[i] = class_id_emb_attr.iloc[min_ind[0]]['class_id']\n",
    "    return np.asarray(nearest_class_id)\n",
    "        \n",
    "def calc_accuracy(eval_df, eval_class, preds):\n",
    "    eval_mask = eval_df.class_id.isin(eval_class)\n",
    "    eval_num = np.sum(eval_mask)\n",
    "    right_num = np.sum(preds[eval_mask] == eval_df.class_id[eval_mask])\n",
    "    return right_num / np.sum(eval_mask), right_num, eval_num\n",
    "    \n",
    "def calc_detailed_accuracy(eval_df, preds, class_id_dict):\n",
    "    print (\"\\n\")\n",
    "    for class_set_name in sorted(class_id_dict):\n",
    "        class_set = class_id_dict[class_set_name]\n",
    "        re = calc_accuracy(eval_df, class_set, preds)\n",
    "        print(\"%s: \\t%.6f\\t%.0f\\t%.0f\" % ((class_set_name,) + re))\n",
    "#     print (\"\\n\")\n",
    "\n",
    "def multi_preds_vote(preds):\n",
    "    vote_preds = []\n",
    "    for single_img_vote in preds:\n",
    "        uniq_val, counts = np.unique(single_img_vote, return_counts = True)\n",
    "        vote_preds.append(uniq_val[np.argmax(counts)])\n",
    "    vote_preds = np.asarray(vote_preds)\n",
    "    return vote_preds\n",
    "    \n",
    "def multi_models_vote(models, eval_df = None, cand_class_id_emb_attr = None, img_feature_map = None, \n",
    "                      class_id_dict = None):\n",
    "    print ('cand shape: ', cand_class_id_emb_attr.shape[0])\n",
    "    preds = models_eval(models, eval_df, cand_class_id_emb_attr, img_feature_map, \n",
    "                class_id_dict)\n",
    "    preds = np.asarray(preds).T\n",
    "    vote_preds = multi_preds_vote(preds)\n",
    "    # print (vote_preds)\n",
    "    print ('Multi model votes results:')\n",
    "    if 'class_id' in eval_df.columns: \n",
    "        calc_detailed_accuracy(eval_df, vote_preds, class_id_dict)\n",
    "    return vote_preds\n",
    "\n",
    "def model_eval(model, model_type, eval_df, cand_class_id_emb_attr = None, img_feature_map = None, \n",
    "                class_id_dict = None, class_to_id = None, TTA = None, img_model = None, only_emb = None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if model_type == 'DenseNet':\n",
    "        flat_model = Model(inputs = model.inputs, outputs = model.get_layer(name = 'avg_pool').output)\n",
    "        pred = flat_model.predict(preprocess_img(eval_df['img']), verbose = 1)\n",
    "    elif model_type == 'DEM' or model_type == 'AE':\n",
    "        zs_model = Model(inputs = model.inputs[:2], outputs = model.outputs[0])\n",
    "        cand_feature_map = zs_model.predict(create_dem_data(cand_class_id_emb_attr, only_emb = only_emb, \n",
    "                                                            model_type = model_type), verbose = 2)\n",
    "        if TTA is not None:\n",
    "            batch_size = 32\n",
    "            datagen = MixedImageDataGenerator(\n",
    "                    rotation_range=20,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True)\n",
    "            img_feature_map = img_model.predict_generator(\n",
    "                datagen.flow((preprocess_img(eval_df['img'])), None, shuffle = False, batch_size = batch_size), \n",
    "                steps = np.ceil(eval_df.shape[0] / batch_size) * TTA, verbose = 1)\n",
    "            pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map, img_feature_map)\n",
    "            pred = np.reshape(pred, (TTA, eval_df.shape[0])).T\n",
    "            pred = multi_preds_vote(pred)\n",
    "        else:\n",
    "            pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map, img_feature_map)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'RCNN_DEM_BC':\n",
    "        zs_model = Model(inputs = model.inputs[1:-1], outputs = model.outputs[0])\n",
    "        cand_feature_map = zs_model.predict(create_dem_data(cand_class_id_emb_attr, only_emb = only_emb, \n",
    "                                                            model_type = model_type), verbose = 1)\n",
    "        zs_model = Model(inputs = model.get_layer('attr_x_img_model').inputs, \n",
    "                         outputs = model.get_layer('attr_x_img_model').outputs)\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map = cand_feature_map, img_feature_map = img_feature_map,\n",
    "                                zs_model = zs_model, model_type = model_type)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'DEM_BC' or model_type == 'RES_DEM_BC':\n",
    "        zs_model = Model(inputs = model.inputs[1:-1], outputs = model.outputs[0])\n",
    "        cand_feature_map = zs_model.predict(create_dem_data(cand_class_id_emb_attr, only_emb = only_emb, \n",
    "                                                            model_type = model_type), verbose = 1)\n",
    "        zs_model = Model(inputs = model.get_layer('attr_x_img_model').inputs, \n",
    "                         outputs = model.get_layer('attr_x_img_model').outputs)\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map = cand_feature_map, img_feature_map = img_feature_map,\n",
    "                                zs_model = zs_model, model_type = model_type)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'GCN':\n",
    "        zs_model = Model(inputs = model.inputs[2:], outputs = model.outputs[0])\n",
    "        cand_class_to_id = [class_to_id[c] for c in cand_class_id_emb_attr.class_id.values]\n",
    "        cand_feature_map = zs_model.predict(None, steps = 1)[cand_class_to_id]\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map, img_feature_map)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'QFSL':\n",
    "        cand_class_to_id = [class_to_id[c] for c in cand_class_id_emb_attr.class_id.values]\n",
    "        zs_model = Model(inputs = [model.inputs[0]] + model.inputs[3:], outputs = model.outputs[0])\n",
    "        dis_arr = zs_model.predict(img_feature_map, verbose = 2)[:, cand_class_to_id]\n",
    "#         print (dis_arr[:2])\n",
    "#         print (dis_arr[:2, cand_class_to_id])\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, model_type = model_type, dis_arr = dis_arr,\n",
    "                                 img_feature_map = img_feature_map)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'I2A':\n",
    "        zs_model = Model(inputs = model.inputs[-1], outputs = model.outputs[0])\n",
    "        attr_preds = zs_model.predict(extract_array_from_series(img_feature_map), verbose = 2)\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, None, img_feature_map, \n",
    "                                  model_type, attr_preds)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'DEM_AUG':\n",
    "        img_model = Model(inputs = model.inputs[0], outputs = model.outputs[-1])\n",
    "        img_feature_map = img_model.predict(preprocess_img(eval_df['img']), verbose = 2)\n",
    "        zs_model = Model(inputs = model.inputs[1:], outputs = model.outputs[0])\n",
    "        cand_feature_map = zs_model.predict(create_dem_data(cand_class_id_emb_attr), verbose = 2)\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map, img_feature_map)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'DEM_BC_AUG':\n",
    "        img_model = Model(inputs = model.inputs[0], outputs = model.outputs[-1])\n",
    "        img_feature_map = img_model.predict(preprocess_img(eval_df['img']), verbose = 2)\n",
    "        zs_model = Model(inputs = model.inputs[1:3], outputs = model.outputs[0])\n",
    "        cand_feature_map = zs_model.predict(create_dem_data(cand_class_id_emb_attr, only_emb = only_emb), verbose = 2)\n",
    "        zs_model = Model(inputs = model.get_layer('attr_x_img_model').inputs, \n",
    "                         outputs = model.get_layer('attr_x_img_model').outputs)\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map = cand_feature_map, img_feature_map = img_feature_map,\n",
    "                                zs_model = zs_model, model_type = model_type)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'WV2ATTR':\n",
    "        pred = model.predict(create_dem_data(eval_df, only_emb = False))\n",
    "    return pred\n",
    "\n",
    "def models_eval(models, eval_df, cand_class_id_emb_attr = None, img_feature_map = None, \n",
    "                class_id_dict = None, class_to_id = None, img_model = None):\n",
    "    preds = []\n",
    "    for (model, model_type) in models:\n",
    "        pred = model_eval(model, model_type, eval_df = eval_df, cand_class_id_emb_attr = cand_class_id_emb_attr, \n",
    "            img_feature_map = img_feature_map, class_id_dict = class_id_dict, class_to_id = class_to_id,\n",
    "            img_model = img_model)\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "class MixedImageDataGenerator(ImageDataGenerator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MixedImageDataGenerator, self).__init__(**kwargs)\n",
    "        \n",
    "    def flow(self,\n",
    "           x,\n",
    "           y=None,\n",
    "           batch_size=32,\n",
    "           shuffle=True,\n",
    "           seed=None,\n",
    "           save_to_dir=None,\n",
    "           save_prefix='',\n",
    "           save_format='png'):\n",
    "        return MixedNumpyArrayIterator(\n",
    "        x,\n",
    "        y,\n",
    "        self,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=seed,\n",
    "        data_format=self.data_format,\n",
    "        save_to_dir=save_to_dir,\n",
    "        save_prefix=save_prefix,\n",
    "        save_format=save_format)\n",
    "\n",
    "\n",
    "class MixedNumpyArrayIterator(NumpyArrayIterator):\n",
    "    \"\"\"Iterator yielding data from a Numpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               x,\n",
    "               y,\n",
    "               image_data_generator,\n",
    "               **kwargs):\n",
    "        self.x_misc = None\n",
    "        if (type(x) is list) or (type(x) is tuple):\n",
    "            super(MixedNumpyArrayIterator, self).__init__(x[0],\n",
    "                   y,\n",
    "                   image_data_generator,\n",
    "                   **kwargs)\n",
    "            self.x_misc = [np.asarray(xx) for xx in x[1]]\n",
    "        else:\n",
    "            super(MixedNumpyArrayIterator, self).__init__(x,\n",
    "               y,\n",
    "               image_data_generator,\n",
    "               **kwargs)\n",
    "            \n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x.\n",
    "\n",
    "        Returns:\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            index_array, current_index, current_batch_size = next(\n",
    "              self.index_generator)\n",
    "#         print (index_array)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        batch_x = np.zeros(\n",
    "            tuple([current_batch_size] + list(self.x.shape)[1:]), dtype=K.floatx())\n",
    "        for i, j in enumerate(index_array):\n",
    "            x = self.x[j]\n",
    "#             x_bak = x\n",
    "            x = self.image_data_generator.random_transform(x.astype(K.floatx()))\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "#             print (np.all(x_bak == x))\n",
    "            batch_x[i] = x\n",
    "        if self.x_misc is None:\n",
    "            batch_x = [batch_x]\n",
    "        else:\n",
    "            batch_x_miscs = [xx[index_array] for xx in self.x_misc]\n",
    "#             perm_batch_x = np.random.permutation(batch_x)\n",
    "            batch_x = [batch_x] + batch_x_miscs\n",
    "#             pos_len = pos_data[0].shape[0]\n",
    "            \n",
    "#             ind_array = np.array(range(pos_len))\n",
    "#     #     rs = np.random.RandomState(seed=420)\n",
    "#     #     perm_ind_array = rs.permutation(rs.permutation(rs.permutation(rs.permutation(ind_array))))\n",
    "#             perm_ind_array = np.random.permutation(ind_array)\n",
    "#             perm_label = np.zeros(pos_len, dtype = 'float')\n",
    "#             perm_label[train_class_id == train_class_id[perm_ind_array]] = 1\n",
    "#             perm_img = pos_data[0][perm_ind_array] \n",
    "#             perm_label = perm_label.reshape((pos_len, 1))\n",
    "# #         print (perm_label.shape)\n",
    "#             neg_data = [perm_img_feature_map] + pos_data[1:3] + [perm_label]\n",
    "#             for y in batch_x:\n",
    "#                 print (y.shape)\n",
    "        return (batch_x, None, None)\n",
    "    \n",
    "#     def next(self):\n",
    "#         \"\"\"For python 2.x.\n",
    "\n",
    "#         Returns:\n",
    "#             The next batch.\n",
    "#         \"\"\"\n",
    "#         with self.lock:\n",
    "#             index_array, current_index, current_batch_size = next(\n",
    "#               self.index_generator)\n",
    "# #         print (index_array)\n",
    "#         # The transformation of images is not under thread lock\n",
    "#         # so it can be done in parallel\n",
    "#         batch_x = np.zeros(\n",
    "#             tuple([current_batch_size] + list(self.x.shape)[1:]), dtype=K.floatx())\n",
    "#         for i, j in enumerate(index_array):\n",
    "#             x = self.x[j]\n",
    "# #             x_bak = x\n",
    "#             x = self.image_data_generator.random_transform(x.astype(K.floatx()))\n",
    "#             x = self.image_data_generator.standardize(x)\n",
    "# #             print (np.all(x_bak == x))\n",
    "#             batch_x[i] = x\n",
    "#         if self.x_misc is None:\n",
    "#             batch_x = [batch_x]\n",
    "#         else:\n",
    "#             batch_x_miscs = [xx[index_array] for xx in self.x_misc]\n",
    "#             batch_x = [batch_x] + batch_x_miscs\n",
    "#         return (batch_x, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over all training size:\n",
      "(63875, 9)\n",
      "Fold:  0\n",
      "Seen unseen Classes:  144 16\n",
      "Seen round1, round2:  0 144\n",
      "Unseen round1, round2:  0 16\n",
      "WARNING:tensorflow:Output \"activation_13\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"activation_13\" during training.\n",
      "WARNING:tensorflow:Output \"attr_x_img_model\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"attr_x_img_model\" during training.\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "desc (InputLayer)                (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "desc_emb (Embedding)             (None, 200, 300)      18000000    desc[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "spatial_dropout1d_7 (SpatialDrop (None, 200, 300)      0           desc_emb[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)               (None, 200, 128)      38528       spatial_dropout1d_7[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)               (None, 198, 128)      115328      spatial_dropout1d_7[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)               (None, 196, 128)      192128      spatial_dropout1d_7[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_31 (Glo (None, 128)           0           conv1d_31[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_31 (GlobalM (None, 128)           0           conv1d_31[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Glo (None, 128)           0           conv1d_32[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (GlobalM (None, 128)           0           conv1d_32[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_33 (Glo (None, 128)           0           conv1d_33[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_33 (GlobalM (None, 128)           0           conv1d_33[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)     (None, 256)           0           global_average_pooling1d_31[0][0]\n",
      "                                                                   global_max_pooling1d_31[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)     (None, 256)           0           global_average_pooling1d_32[0][0]\n",
      "                                                                   global_max_pooling1d_32[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)     (None, 256)           0           global_average_pooling1d_33[0][0]\n",
      "                                                                   global_max_pooling1d_33[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)     (None, 768)           0           concatenate_47[0][0]             \n",
      "                                                                   concatenate_48[0][0]             \n",
      "                                                                   concatenate_49[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "RCNN_CONC (Lambda)               (None, 768)           0           concatenate_50[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "wv (InputLayer)                  (None, 2800)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)     (None, 3568)          0           RCNN_CONC[0][0]                  \n",
      "                                                                   wv[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 3568)          14272       concatenate_51[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 1548)          5524812     batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 1548)          0           dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 1548)          6192        activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 1290)          1998210     batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 1290)          0           dense_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 1290)          5160        activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 1032)          1332312     batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 1032)          0           dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "img (InputLayer)                 (None, 1032)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "attr_x_img (Lambda)              (None, 1032)          0           activation_13[0][0]              \n",
      "                                                                   img[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "attr_x_img_model (Model)         (None, 1)             5161        attr_x_img[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 27,232,103\n",
      "Trainable params: 9,217,227\n",
      "Non-trainable params: 18,014,876\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "-----DNN training-----\n",
      "DEM BC Data Train Len, Pos, Neg: 57534 58354.0 114248\n",
      "Train on 172602 samples, validate on 6341 samples\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s>.] - ETA: 0s - loss: 1.4966\n",
      "\n",
      "\n",
      "Unseen_class: \t0.293329\t1860\t6341\n",
      "Unseen_round1_id: \tnan\t0\t0\n",
      "Unseen_round2_id: \t0.293329\t1860\t6341\n",
      "172602/172602 [==============================] - 94s - loss: 1.4966 - val_loss: 2.1255\n",
      "Epoch 2/15\n",
      "   320/172602 [..............................] - ETA: 93s - loss: 1.2650"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:225: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159424/172602 [==========================>...] - ETA: 6s - loss: 1.0822"
     ]
    }
   ],
   "source": [
    "class AccuracyEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1, batch_interval = 1000000, verbose = 2, \\\n",
    "            scores = [], cand_class_id_emb_attr = None, eval_df = None, threshold = None, \\\n",
    "                 seen_class = None, unseen_class = None, gamma = None, model_type = None, \n",
    "                 class_id_dict = None, class_to_id = None, TTA = None, img_model = None,\n",
    "                only_emb = None):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        # print (validation_data)\n",
    "#         self.X_val, _,\n",
    "        if model_type == 'DEM_BC_AUG':\n",
    "            self.y_val = None\n",
    "        else:\n",
    "            self.y_val = validation_data[0]\n",
    "        self.verbose = verbose\n",
    "        self.scores = scores\n",
    "        self.cand_class_id_emb_attr = cand_class_id_emb_attr\n",
    "        self.eval_df = eval_df\n",
    "        self.seen_class = seen_class\n",
    "        self.unseen_class = unseen_class\n",
    "        self.model_type = model_type\n",
    "        self.class_id_dict = class_id_dict\n",
    "        self.class_to_id = class_to_id\n",
    "        self.TTA = TTA\n",
    "        self.img_model = img_model\n",
    "        self.only_emb = only_emb\n",
    "#         self.class_id_dict['All'] = self.eval_df.class_id.unique()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            model_eval(self.model, self.model_type, self.eval_df, self.cand_class_id_emb_attr, \n",
    "#                 seen_class = self.seen_class, \n",
    "#                 unseen_class = self.unseen_class, \n",
    "                img_feature_map = self.y_val,\n",
    "                class_id_dict = self.class_id_dict,\n",
    "                class_to_id = self.class_to_id,\n",
    "                TTA = self.TTA,\n",
    "                img_model = self.img_model,\n",
    "                      only_emb = self.only_emb)\n",
    "\n",
    "class DEM:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, scores = None, flags = None, model_type = None, seen_class = None, \n",
    "            unseen_class = None, class_id_emb_attr = None, img_flat_len = None, \n",
    "                    unseen_round1_id = None,\n",
    "                    unseen_round2_id = None,\n",
    "                    img_model = None,\n",
    "                    only_emb = False,\n",
    "                    c2c_neg_cnt = None,\n",
    "                    emb_weight = None):\n",
    "        self.batch_size = 64 #flags.dem_batch_size\n",
    "        self.epochs = 15 #flags.dem_epochs\n",
    "        self.patience = 100 #flags.dem_patience\n",
    "        self.scores = scores\n",
    "        self.model_type = model_type\n",
    "        self.verbose = 1 #flags.train_verbose\n",
    "        self.seen_class = seen_class\n",
    "        self.unseen_class = unseen_class\n",
    "        self.cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)]\n",
    "        self.class_id_emb_attr = class_id_emb_attr\n",
    "        class_ids = class_id_emb_attr.class_id.values\n",
    "        self.class_to_id = dict([(c, i) for i, c in enumerate(class_ids)])\n",
    "        self.img_flat_len = img_flat_len\n",
    "        self.TTA = None #flags.TTA\n",
    "        self.img_model = img_model\n",
    "        self.only_emb = only_emb\n",
    "        self.c2c_neg_cnt = c2c_neg_cnt\n",
    "        self.emb_weight = emb_weight\n",
    "        if model_type == 'DEM':\n",
    "            self.model = self.create_dem(img_flat_len = img_flat_len)\n",
    "        elif model_type == 'GCN':\n",
    "            self.model = self.create_gcn(img_flat_len = img_flat_len)\n",
    "        elif model_type == 'I2A':\n",
    "            self.model = self.create_img2attr(img_flat_len = img_flat_len)\n",
    "        elif model_type == 'AE':\n",
    "            self.model = self.create_ae(img_flat_len = img_flat_len)\n",
    "        elif model_type == 'DEM_AUG':\n",
    "            self.model = self.create_dem_aug(img_flat_len = img_flat_len)\n",
    "        elif model_type == 'DEM_BC':\n",
    "            self.model = self.create_dem_bc(img_flat_len = img_flat_len, only_emb = self.only_emb)\n",
    "        elif model_type == 'DEM_BC_AUG':\n",
    "            self.model = self.create_dem_bc_aug(img_flat_len = img_flat_len, only_emb = self.only_emb)\n",
    "        elif model_type == 'RES_DEM_BC':\n",
    "            self.model = self.create_res_dem_bc(img_flat_len = img_flat_len, only_emb = self.only_emb)\n",
    "        elif model_type == 'RCNN_DEM_BC':\n",
    "            self.model = self.create_rcnn_dem_bc(self, img_flat_len = img_flat_len, only_emb = self.only_emb)\n",
    "        elif model_type == 'QFSL':\n",
    "            self.model = self.create_qfsl(img_flat_len = img_flat_len, only_emb = self.only_emb)\n",
    "        elif model_type == 'WV2ATTR':\n",
    "            self.model = self.create_wv2attr()\n",
    "        self.class_id_dict = {\n",
    "#                              'seen_class': seen_class,\n",
    "                             'Unseen_class': unseen_class,\n",
    "                             'Unseen_round1_id': unseen_round1_id,\n",
    "                             'Unseen_round2_id': unseen_round2_id,}\n",
    "\n",
    "    def create_dem(self, kernel_initializer = 'he_normal', img_flat_len = 1024):\n",
    "        attr_input = layers.Input(shape = (50,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (2800,), name = 'wv')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "        \n",
    "        attr_dense = layers.Dense(1300, use_bias = True, kernel_initializer=kernel_initializer, \n",
    "                        kernel_regularizer = l2(1e-4), name = 'attr_dense')(attr_input)\n",
    "        attr_word_emb = word_emb #layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "                                                                            int(img_flat_len * 2),\n",
    "                                                                            int(img_flat_len * 1.5), \n",
    "                                                                            int(img_flat_len * 1.25), \n",
    "                                                                            int(img_flat_len),\n",
    "#                                                                             int(img_flat_len * 1.0625)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "#         attr_word_emb_dense = self.full_connect_layer(attr_word_emb_dense, hidden_dim = [img_flat_len], \n",
    "#                                                 activation = 'relu')\n",
    "\n",
    "        mse_loss = K.mean(mean_squared_error(imag_classifier, attr_word_emb_dense))\n",
    "        \n",
    "        model = Model([attr_input, word_emb, imag_classifier], outputs = attr_word_emb_dense) #, vgg_output])\n",
    "        model.add_loss(mse_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "        return model\n",
    "\n",
    "    def create_dem_bc(self, kernel_initializer = 'he_normal', img_flat_len = 1024, only_emb = False):\n",
    "        attr_input = layers.Input(shape = (22,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (2800,), name = 'wv')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "        label = layers.Input(shape = (1,), name = 'label')\n",
    "        inter_len = 512\n",
    "# #         label = layers.Input(tensor=\n",
    "# #                             tf.constant(np.r_[np.ones((self.batch_size, 1)), \n",
    "# #                                               np.zeros((self.batch_size, 1))], \n",
    "# #                                                  dtype = 'float32'))\n",
    "#         neg_label = layers.Input(tensor=\n",
    "#                             tf.constant(np.zeros((self.batch_size, 1)), \n",
    "#                                                  dtype = 'float32'))\n",
    "#         merge_label = layers.Lambda(lambda x: tf.concat(x, axis = 0))([label, neg_label])\n",
    "        \n",
    "#         attr_emb = layers.Embedding(294, 30)(attr_input)\n",
    "#         attr_dense = layers.Flatten()(attr_emb) #layers.GlobalAveragePooling1D()(attr_emb)\n",
    "        attr_dense = layers.Dense(1600, use_bias = True, kernel_initializer=kernel_initializer, \n",
    "                        kernel_regularizer = l2(1e-4), name = 'attr_dense')(attr_input)\n",
    "        if only_emb:\n",
    "            attr_word_emb = word_emb\n",
    "        else:\n",
    "            attr_word_emb = layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "                                                                            int(img_flat_len * 1.5),\n",
    "#                                                                             int(1032*1.5),\n",
    "#                                                                             int(1032 * 1.5), \n",
    "#                                                                             int(img_flat_len), \n",
    "#                                                                             int(img_flat_len),\n",
    "                                                                            int(img_flat_len),\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "#         attr_word_emb_dense = self.full_connect_layer(attr_word_emb_dense, hidden_dim = [img_flat_len], \n",
    "#                                                 activation = 'relu')\n",
    "        \n",
    "#         merge_attr_word_emb_dense = layers.Lambda(lambda x: tf.tile(x, [2, 1]))(attr_word_emb_dense)\n",
    "#         perm_imag_classifier = layers.Lambda(lambda x: tf.random_shuffle(x))(imag_classifier)\n",
    "#         merge_imag_classifier = layers.Lambda(lambda x: tf.concat(x, axis = 0))([imag_classifier, perm_imag_classifier])\n",
    "        \n",
    "#         imag_classifier_dense = layers.Lambda(lambda x: K.l2_normalize(x, axis=-1))(imag_classifier)\n",
    "        attr_x_img = layers.Lambda(lambda x: x[0] * x[1], name = 'attr_x_img')([attr_word_emb_dense, imag_classifier])\n",
    "#         attr_x_img = layers.Concatenate(name = 'attr_x_img')([attr_word_emb_dense, imag_classifier])\n",
    "    \n",
    "        attr_img_input = layers.Input(shape = (img_flat_len,), name = 'attr_img_input')\n",
    "#         attr_img_input = layers.Input(shape = (img_flat_len * 2,), name = 'attr_img_input')\n",
    "        proba = self.full_connect_layer(attr_img_input, hidden_dim = [1], activation = 'sigmoid')\n",
    "        attr_img_model = Model(inputs = attr_img_input, outputs = proba, name = 'attr_x_img_model')\n",
    "        \n",
    "        out = attr_img_model([attr_x_img])\n",
    "        \n",
    "        bc_loss = K.mean(K.mean(binary_crossentropy(label, out)))\n",
    "        model = Model([imag_classifier, attr_input, word_emb, label], outputs = [attr_word_emb_dense, out])\n",
    "        model.add_loss(bc_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "        return model\n",
    "    \n",
    "    def pooling_blend(self, input):\n",
    "        avg_pool = layers.GlobalAveragePooling1D()(input)\n",
    "        max_pool = layers.GlobalMaxPooling1D()(input)\n",
    "        conc = layers.Concatenate()([avg_pool, max_pool])\n",
    "        return conc\n",
    "    \n",
    "    def ConvBlock(self, x, filter_size, kernel_size_list = [1, 3, 5]):\n",
    "        conc_list =[]\n",
    "        for kernel_size in kernel_size_list:\n",
    "            kernel_maps = layers.Conv1D(filters = filter_size, kernel_size = kernel_size, activation = 'relu', \n",
    "                                       kernel_initializer = 'he_normal', kernel_regularizer = l2(1e-4))(x)\n",
    "            kernel_conc = self.pooling_blend(kernel_maps)\n",
    "            conc_list.append(kernel_conc)\n",
    "        return layers.Concatenate()(conc_list)\n",
    "    \n",
    "    def create_rcnn_dem_bc(self, kernel_initializer = 'he_normal', img_flat_len = 1024, only_emb = False):\n",
    "        attr_input = layers.Input(shape = (22,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (2800,), name = 'wv')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "        label = layers.Input(shape = (1,), name = 'label')\n",
    "        \n",
    "        desc = layers.Input(shape = (200,), name = 'desc')\n",
    "        desc_emb = layers.Embedding(60000, \n",
    "                              300, \n",
    "                              weights=[self.emb_weight], \n",
    "                              trainable=False, name = 'desc_emb')(desc)\n",
    "        desc_emb = layers.SpatialDropout1D(0.2)(desc_emb)\n",
    "        \n",
    "        cnn_list = []\n",
    "        rnn_list = []\n",
    "        for filter_size in [128]: #self.filter_size:\n",
    "            if filter_size > 0:\n",
    "                conc = self.ConvBlock(desc_emb, filter_size)\n",
    "                cnn_list.append(conc)\n",
    "#         for rnn_unit in self.context_vector_dim:\n",
    "#             if rnn_unit > 0:\n",
    "#                 rnn_maps = Bidirectional(GRU(rnn_unit, return_sequences=True, \\\n",
    "#                             dropout=self.rnn_input_dropout, recurrent_dropout=self.rnn_state_dropout))(x)\n",
    "#                 conc = self.pooling_blend(rnn_maps)\n",
    "#                 rnn_list.append(conc)\n",
    "\n",
    "        conc_list = cnn_list + rnn_list\n",
    "        if len(conc_list) == 1:\n",
    "            class_emb = layers.Lambda(lambda x: x, name = 'RCNN_CONC')(conc_list)\n",
    "        else:\n",
    "            class_emb = layers.Concatenate(name = 'RCNN_CONC')(conc_list)\n",
    "        \n",
    "#         attr_emb = layers.Embedding(294, 30)(attr_input)\n",
    "#         attr_dense = layers.Flatten()(attr_emb) #layers.GlobalAveragePooling1D()(attr_emb)\n",
    "        \n",
    "        if only_emb:\n",
    "            attr_word_emb = layers.Concatenate()([class_emb, word_emb])\n",
    "        else:\n",
    "            attr_dense = layers.Dense(1600, use_bias = True, kernel_initializer=kernel_initializer, \n",
    "                        kernel_regularizer = l2(1e-4), name = 'attr_dense')(attr_input)\n",
    "            attr_word_emb = layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "#                                                                             int(img_flat_len * 4),\n",
    "#                                                                             int(img_flat_len),\n",
    "#                                                                             int(img_flat_len), \n",
    "                                                                            int(img_flat_len * 1.5), \n",
    "                                                                            int(img_flat_len * 1.25),\n",
    "                                                                            int(img_flat_len)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = None)\n",
    "        \n",
    "        \n",
    "        attr_x_img = layers.Lambda(lambda x: x[0] * x[1], name = 'attr_x_img')([attr_word_emb_dense, imag_classifier])\n",
    "# #         attr_x_img = layers.Concatenate(name = 'attr_x_img')([attr_word_emb_dense, imag_classifier])\n",
    "    \n",
    "        attr_img_input = layers.Input(shape = (img_flat_len,), name = 'attr_img_input')\n",
    "#         attr_img_input = layers.Input(shape = (img_flat_len * 2,), name = 'attr_img_input')\n",
    "        proba = self.full_connect_layer(attr_img_input, hidden_dim = [1], activation = 'sigmoid')\n",
    "        attr_img_model = Model(inputs = attr_img_input, outputs = proba, name = 'attr_x_img_model')\n",
    "        \n",
    "        out = attr_img_model([attr_x_img])\n",
    "        \n",
    "#         mse_loss = K.mean(mean_squared_error(imag_classifier, attr_word_emb_dense))\n",
    "        \n",
    "        bc_loss = K.mean(K.mean(binary_crossentropy(label, out)))\n",
    "        model = Model([imag_classifier, attr_input, word_emb, desc, label], outputs = [attr_word_emb_dense, out])\n",
    "        model.add_loss(bc_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "        return model\n",
    "    \n",
    "    def create_dem_bc_aug(self, kernel_initializer = 'he_normal', img_flat_len = 1024, only_emb = False):\n",
    "        attr_input = layers.Input(shape = (53,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (2800,), name = 'wv')\n",
    "        img_input = layers.Input(shape = (72, 72, 3))\n",
    "        label = layers.Input(shape = (1,), name = 'label')\n",
    "        \n",
    "        imag_classifier = self.img_model(img_input)\n",
    "        if only_emb:\n",
    "            attr_word_emb = word_emb\n",
    "        else:\n",
    "            attr_word_emb = layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "#                                                                             int(img_flat_len * 4),\n",
    "                                                                            int(img_flat_len * 2),\n",
    "                                                                            int(img_flat_len * 1.5), \n",
    "                                                                            int(img_flat_len * 1.25), \n",
    "#                                                                             int(img_flat_len * 1.125),\n",
    "                                                                            int(img_flat_len)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "#         attr_word_emb_dense = self.full_connect_layer(attr_word_emb_dense, hidden_dim = [img_flat_len], \n",
    "#                                                 activation = 'relu')\n",
    "        \n",
    "        attr_x_img = layers.Lambda(lambda x: x[0] * x[1], name = 'attr_x_img')([attr_word_emb_dense, imag_classifier])\n",
    "#         attr_x_img = layers.Concatenate(name = 'attr_x_img')([attr_word_emb_dense, imag_classifier])\n",
    "    \n",
    "        attr_img_input = layers.Input(shape = (img_flat_len,), name = 'attr_img_input')\n",
    "#         attr_img_input = layers.Input(shape = (img_flat_len * 2,), name = 'attr_img_input')\n",
    "        proba = self.full_connect_layer(attr_img_input, hidden_dim = [1], activation = 'sigmoid')\n",
    "        attr_img_model = Model(inputs = attr_img_input, outputs = proba, name = 'attr_x_img_model')\n",
    "        \n",
    "        out = attr_img_model([attr_x_img])\n",
    "        \n",
    "#         dem_bc_model = self.create_dem_bc(kernel_initializer = 'he_normal', \n",
    "#                                            img_flat_len = img_flat_len, \n",
    "#                                            only_emb = only_emb)\n",
    "#         attr_word_emb_dense, out = dem_bc_model([imag_classifier, attr_input, word_emb, label])\n",
    "        \n",
    "        bc_loss = K.mean(binary_crossentropy(label, out))\n",
    "        model = Model([img_input, attr_input, word_emb, label], outputs = [attr_word_emb_dense, out])\n",
    "        model.add_loss(bc_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "        return model\n",
    "    \n",
    "    def create_res_dem_bc(self, kernel_initializer = 'he_normal', img_flat_len = 1024, only_emb = False):\n",
    "        attr_input = layers.Input(shape = (50,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (1600,), name = 'wv')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "        label = layers.Input(shape = (1,), name = 'label')\n",
    "        \n",
    "        attr_dense = layers.Dense(1600, use_bias = True, kernel_initializer=kernel_initializer, \n",
    "                        kernel_regularizer = l2(1e-4), name = 'attr_dense')(attr_input)\n",
    "        \n",
    "        ini_dem_model = self.create_dem_bc(kernel_initializer = 'he_normal', \n",
    "                                           img_flat_len = img_flat_len, \n",
    "                                           only_emb = True)\n",
    "#         print (ini_dem_model.summary())\n",
    "        ini_dem_model.load_weights('./only_emb.h5')\n",
    "        ini_dem_model_part = Model(inputs = ini_dem_model.inputs[2], \n",
    "                                   outputs = ini_dem_model.outputs[0])\n",
    "        ini_dem_model_part.trainable = False\n",
    "        ini_attr_word_emb_dense = ini_dem_model_part([word_emb])\n",
    "        \n",
    "        if only_emb:\n",
    "            attr_word_emb = word_emb\n",
    "        else:\n",
    "            attr_word_emb = attr_dense #layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "                                                                            int(img_flat_len * 2),\n",
    "                                                                            int(img_flat_len * 1.5), \n",
    "                                                                            int(img_flat_len * 1.25),\n",
    "                                                                            int(img_flat_len)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "        attr_word_emb_dense = layers.Lambda(lambda x: x[0] + x[1])([attr_word_emb_dense, ini_attr_word_emb_dense])\n",
    "        \n",
    "        attr_x_img = layers.Lambda(lambda x: x[0] * x[1], name = 'attr_x_img')([attr_word_emb_dense, imag_classifier])\n",
    "#         attr_x_img = layers.Concatenate(name = 'attr_x_img')([attr_word_emb_dense, imag_classifier])\n",
    "    \n",
    "        attr_img_input = layers.Input(shape = (img_flat_len,), name = 'attr_img_input')\n",
    "#         attr_img_input = layers.Input(shape = (img_flat_len * 2,), name = 'attr_img_input')\n",
    "        proba = self.full_connect_layer(attr_img_input, hidden_dim = [1], activation = 'sigmoid')\n",
    "        attr_img_model = Model(inputs = attr_img_input, outputs = proba, name = 'attr_x_img_model')\n",
    "        \n",
    "        out = attr_img_model([attr_x_img])\n",
    "        \n",
    "        bc_loss = K.mean(binary_crossentropy(label, out))\n",
    "        model = Model([imag_classifier, attr_input, word_emb, label], outputs = [attr_word_emb_dense, out])\n",
    "        model.add_loss(bc_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "        return model\n",
    "    \n",
    "    def create_dem_aug(self, kernel_initializer = 'he_normal', img_flat_len = 1024):\n",
    "        attr_input = layers.Input(shape = (50,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (300,), name = 'wv')\n",
    "        img_input = layers.Input(shape = (64, 64, 3))\n",
    "#         imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "\n",
    "        img_model = DenseNet(scores = None, \n",
    "                     cat_max = 365, #flags.cat_max, \n",
    "                     flags = None, \n",
    "                     model_type = 'DenseNet').model\n",
    "#         print ('Load DenseNet Weights---')\n",
    "        img_model.load_weights(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05108_TF14_DensetNetProcess/imgmodel_20181013_092715.h5')\n",
    "        img_flat_model = Model(inputs = img_model.inputs, outputs = img_model.get_layer(name = 'avg_pool').output)\n",
    "        img_flat_model.trainable = False\n",
    "        imag_classifier = img_flat_model(img_input)\n",
    "        \n",
    "        attr_dense = layers.Dense(1200, use_bias = True, kernel_initializer=kernel_initializer, \n",
    "                        kernel_regularizer = l2(1e-4), name = 'attr_dense')(attr_input)\n",
    "        attr_word_emb = layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "                                                                            int(img_flat_len * 2),\n",
    "#                                                                             int(img_flat_len * 1.5), \n",
    "#                                                                             int(img_flat_len * 1.25), \n",
    "#                                                                             int(img_flat_len * 1.125),\n",
    "#                                                                             int(img_flat_len * 1.0625)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb_dense, hidden_dim = [img_flat_len], \n",
    "                                                activation = 'relu')\n",
    "\n",
    "        mse_loss = K.mean(mean_squared_error(imag_classifier, attr_word_emb_dense))\n",
    "        \n",
    "        model = Model([img_input, attr_input, word_emb], outputs = [attr_word_emb_dense, imag_classifier]) #, vgg_output])\n",
    "        model.add_loss(mse_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "        return model\n",
    "    \n",
    "    def create_img2attr(self, kernel_initializer = 'he_normal', img_flat_len = 1024):\n",
    "        attr_input = layers.Input(shape = (50,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (600,), name = 'wv')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "\n",
    "        attr_dense = layers.Dense(600, use_bias = True, kernel_initializer=kernel_initializer, \n",
    "                        kernel_regularizer = l2(1e-4), name = 'attr_dense')(attr_input)\n",
    "        attr_word_emb = layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        out_size = 50\n",
    "        \n",
    "        attr_preds = self.full_connect_layer(imag_classifier, hidden_dim = [\n",
    "                                                                            int(out_size * 20),\n",
    "                                                                            int(out_size * 15), \n",
    "#                                                                             int(out_size * 7), \n",
    "#                                                                             int(img_flat_len * 1.125),\n",
    "#                                                                             int(img_flat_len * 1.0625)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "        attr_preds = self.full_connect_layer(attr_preds, hidden_dim = [out_size], activation = 'sigmoid')\n",
    "        log_loss = K.mean(binary_crossentropy(attr_input, attr_preds))\n",
    "        \n",
    "        model = Model([attr_input, word_emb, imag_classifier], outputs = [attr_preds]) #, vgg_output])\n",
    "        model.add_loss(log_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-5), loss=None)\n",
    "        return model\n",
    "    \n",
    "    def create_wv2attr(self, kernel_initializer = 'he_normal', attr_len = 27):\n",
    "        attr_input = layers.Input(shape = (attr_len,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (2800,), name = 'wv')\n",
    "        \n",
    "        attr_from_wv = word_emb #self.full_connect_layer(word_emb, hidden_dim = [\n",
    "#                                                                             int(attr_len * 40),\n",
    "#                                                                             int(attr_len * 20), \n",
    "#                                                                             int(attr_len * 10), \n",
    "# #                                                                             int(out_size * 2),\n",
    "# #                                                                             int(img_flat_len * 1.0625)\n",
    "#                                                                             ], \\\n",
    "#                                                 activation = 'sigmoid', resnet = False, drop_out_ratio = 0.2)\n",
    "        attr_preds = self.full_connect_layer(attr_from_wv, hidden_dim = [attr_len], activation = 'sigmoid',\n",
    "                                            drop_out_ratio = 0.2)\n",
    "        log_loss = K.mean(K.mean(binary_crossentropy(attr_input, attr_preds)))\n",
    "        \n",
    "        model = Model([attr_input, word_emb], outputs = [attr_preds]) #, vgg_output])\n",
    "        model.add_loss(log_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-5), loss=None)\n",
    "        return model\n",
    "\n",
    "    def create_qfsl(self, img_flat_len = 1024, only_emb = False):\n",
    "        class_num = self.class_id_emb_attr.shape[0]\n",
    "        print ('Class name in model: ', class_num)\n",
    "        unseen_mask = np.ones(class_num)\n",
    "#         unseen_mask[[self.class_to_id[c] for c in self.unseen_class]] = 0\n",
    "        all_word_emb = layers.Input(tensor=\n",
    "                        tf.constant(extract_array_from_series(self.class_id_emb_attr['emb']), \n",
    "                                    dtype = 'float32'), name = 'wv') #Input(shape = (230, 300,), name = 'wv')\n",
    "        classes = layers.Input(shape = (class_num, ), name = 'classes')\n",
    "        classe_weight = layers.Input(shape = (class_num, ), name = 'classe_weight')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "\n",
    "        if only_emb:\n",
    "            attr_word_emb = all_word_emb\n",
    "            attr_input = layers.Input(tensor=\n",
    "                            tf.constant(0), name = 'attr')\n",
    "        else:\n",
    "            attr_input = layers.Input(tensor=\n",
    "                            tf.constant(np.array(list(self.class_id_emb_attr['attr']), \n",
    "                                                 dtype = 'float32')), name = 'attr')\n",
    "            attr_dense = layers.Dense(1200, use_bias = False, kernel_initializer='he_normal', \n",
    "                        kernel_regularizer = l2(1e-4))(attr_input)\n",
    "            attr_word_emb  = layers.Concatenate()([all_word_emb, attr_dense])\n",
    "#         attr_word_emb_size = 2400\n",
    "        \n",
    "        img_from_attr_emb = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "                                                                            int(1032 * 2),\n",
    "                                                                            int(1032 * 1.5), \n",
    "                                                                            int(1032 * 1.25), \n",
    "                                                                            int(img_flat_len),\n",
    "#                                                                             int(img_flat_len * 1.0625)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "#         img_from_attr_emb = self.full_connect_layer(attr_word_emb_dense, hidden_dim = [img_flat_len], \n",
    "#                                                 activation = 'relu')\n",
    "        \n",
    "#         imag_classifier = layers.Reahpe((img_flat_len, 1))(imag_classifier)\n",
    "#         scoring_sub = layers.Conv1D(filters = class_num, \n",
    "#                                     kernel_size = 1, \n",
    "#                                     use_bias = False,\n",
    "#                                     weights = )(imag_classifier)\n",
    "        scoring_sub = layers.Lambda(lambda x: K.reshape(tf.tile(x[0], [1, class_num]), [-1, class_num, img_flat_len]) *  x[1],\n",
    "                    name = 'scoring_sub')([imag_classifier, img_from_attr_emb])\n",
    "        out = layers.Dense(1, activation=\"sigmoid\")(scoring_sub)\n",
    "        out = layers.Flatten()(out)\n",
    "#         out_mask = layers.Lambda(lambda x: x * unseen_mask, name = 'unseen_class_mask')(out)\n",
    "#         scoring_sub = layers.GlobalAveragePooling1D()(scoring_sub)\n",
    "#         scoring_sub = layers.Flatten()(scoring_sub)\n",
    "#         out = self.full_connect_layer(scoring_sub, hidden_dim = [class_num], \n",
    "#                                     activation = 'softmax')\n",
    "\n",
    "        log_loss = K.sum(K.sum(K.binary_crossentropy(classes, out)))\n",
    "\n",
    "        model = Model([imag_classifier, classes, classe_weight, attr_input, all_word_emb], outputs = [out])\n",
    "        model.add_loss(log_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "    #     model.summary()\n",
    "        return model\n",
    "        \n",
    "    def full_connect_layer(self, input, hidden_dim, activation, resnet = False, adj_graphs = None, \n",
    "                        drop_out_ratio = None, kernel_initializer = 'he_normal'):\n",
    "        full_connect = input\n",
    "        for i, hn in enumerate(hidden_dim):\n",
    "            fc_in = full_connect\n",
    "            if drop_out_ratio is not None:\n",
    "                full_connect = layers.Dropout(drop_out_ratio)(full_connect)\n",
    "            full_connect = layers.BatchNormalization(epsilon=1.001e-5)(full_connect)\n",
    "            full_connect = layers.Dense(hn, kernel_initializer=kernel_initializer, kernel_regularizer = l2(1e-4), \n",
    "                    activation = None)(full_connect)\n",
    "            if adj_graphs is not None:\n",
    "                full_connect = layers.Lambda(lambda x: K.dot(x[1], x[0]))([full_connect, adj_graphs])\n",
    "            full_connect = layers.Activation(activation)(full_connect)\n",
    "            if resnet:\n",
    "                full_connect = layers.Concatenate()([fc_in, full_connect])\n",
    "        return full_connect\n",
    "\n",
    "    def DNN_DataSet(self, df, neg_aug = 0):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.model_type == 'DEM' or self.model_type == 'I2A' or self.model_type == 'AE':\n",
    "#             return create_dem_data(df) + [extract_array_from_series(df['target'])]\n",
    "            return create_dem_data(df) + [sklearn.preprocessing.normalize(extract_array_from_series(df['target']))]\n",
    "        elif self.model_type == 'DEM_AUG':\n",
    "            return [preprocess_img(df['img'])] + create_dem_data(df)\n",
    "        elif self.model_type == 'GCN':\n",
    "            return [create_gcn_data(df, self.class_to_id), \n",
    "                    sklearn.preprocessing.normalize(extract_array_from_series(df['target']))]\n",
    "        elif self.model_type == 'DEM_BC' or self.model_type == 'RES_DEM_BC' or \\\n",
    "            self.model_type == 'RCNN_DEM_BC':\n",
    "#             train_data = create_dem_bc_data(df, neg_aug, self.only_emb, \n",
    "#                 class_id_emb_attr = self.class_id_emb_attr[self.class_id_emb_attr.class_id.isin(self.seen_class)],\n",
    "#                 c2c_neg_cnt = self.c2c_neg_cnt)\n",
    "#             train_len = train_data[0].shape[0]\n",
    "#             trunct_train_len = (train_len // self.batch_size) * self.batch_size\n",
    "#             return [x[:trunct_train_len] for x in train_data]\n",
    "            return create_dem_bc_data(df, neg_aug, self.only_emb, \n",
    "                class_id_emb_attr = self.class_id_emb_attr[self.class_id_emb_attr.class_id.isin(self.seen_class)],\n",
    "                c2c_neg_cnt = self.c2c_neg_cnt, model_type = self.model_type)\n",
    "        elif self.model_type == 'QFSL':\n",
    "            return create_qfsl_data(df, self.class_to_id, categories = self.class_id_emb_attr.shape[0], \n",
    "                                    ns = 0.5)\n",
    "        elif self.model_type == 'DEM_BC_AUG':\n",
    "            return create_dem_bc_aug_data(df, neg_aug, self.only_emb, \n",
    "                class_id_emb_attr = self.class_id_emb_attr[self.class_id_emb_attr.class_id.isin(self.seen_class)],\n",
    "                c2c_neg_cnt = self.c2c_neg_cnt)\n",
    "        elif self.model_type == 'RCNN_DEM_BC':\n",
    "            return create_rcnn_dem_bc_data(df, neg_aug, self.only_emb, \n",
    "                class_id_emb_attr = self.class_id_emb_attr[self.class_id_emb_attr.class_id.isin(self.seen_class)],\n",
    "                c2c_neg_cnt = self.c2c_neg_cnt)\n",
    "        elif self.model_type == 'WV2ATTR':\n",
    "            return create_dem_data(df, only_emb = False)\n",
    "\n",
    "    def lgbm_train(self, train_part, train_part_label, valide_part, valide_part_label, fold_seed = None,\n",
    "        fold = 5, train_weight = None, valide_weight = None, flags = None):\n",
    "        \"\"\"\n",
    "        LGBM Training\n",
    "        \"\"\"\n",
    "        print(\"-----LGBM training-----\")\n",
    "\n",
    "        d_train = lgb.Dataset(train_part, train_part_label)\n",
    "        d_valide = lgb.Dataset(valide_part, valide_part_label)\n",
    "        params = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt', #'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': ['auc', 'binary_logloss'],\n",
    "                'num_leaves': 7, #60, #40, # 60,\n",
    "#                 'min_sum_hessian_in_leaf': 10,\n",
    "                'max_depth': 3,#12, #6, # 10,\n",
    "                'learning_rate': 1, # 0.025,\n",
    "               # 'feature_fraction': 0.5,#0.35, # 0.6\n",
    "                'verbose': 0,\n",
    "                'num_boost_round': 800, #361,\n",
    "#                 'feature_fraction_seed': fold_seed,\n",
    "                #'drop_rate': 0.05,\n",
    "                # 'bagging_fraction': 0.8,\n",
    "                # 'bagging_freq': 20,\n",
    "                # 'bagging_seed': fold_seed,\n",
    "                 'early_stopping_round': 1500,\n",
    "                # 'random_state': 10\n",
    "                # 'verbose_eval': 20\n",
    "                #'min_data_in_leaf': 665\n",
    "            }\n",
    "#         params.update(config.all_params)\n",
    "        print (\"lightgbm params: {0}\\n\".format(params))\n",
    "\n",
    "        bst = lgb.train(\n",
    "                        params ,\n",
    "                        d_train,\n",
    "                        verbose_eval = 200,\n",
    "                        valid_sets = [d_train, d_valide],\n",
    "                        # feature_name= keras_train.DENSE_FEATURE_LIST,\n",
    "                        #feval = gini_lgbm\n",
    "                        #num_boost_round = 1\n",
    "                        )\n",
    "        return bst\n",
    "\n",
    "    def train(self, train_part_df, validate_part_df):\n",
    "        \"\"\"\n",
    "        Keras Training\n",
    "        \"\"\"\n",
    "        print(\"-----DNN training-----\")\n",
    "\n",
    "        DNN_Train_Data = self.DNN_DataSet(train_part_df, neg_aug = 2)\n",
    "        DNN_validate_Data = self.DNN_DataSet(validate_part_df)\n",
    "        \n",
    "        callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=self.patience, verbose=0),\n",
    "        ]\n",
    "        if self.model_type != 'WV2ATTR':\n",
    "            callbacks += [AccuracyEvaluation(validation_data=DNN_validate_Data, interval=1,\n",
    "                            cand_class_id_emb_attr = self.cand_class_id_emb_attr,\n",
    "                            eval_df = validate_part_df,\n",
    "                            model_type = self.model_type,\n",
    "                            class_id_dict = self.class_id_dict,\n",
    "                            class_to_id = self.class_to_id,\n",
    "                            TTA = self.TTA,\n",
    "                            img_model = self.img_model,\n",
    "                          only_emb = self.only_emb)\n",
    "                        ]\n",
    "        if self.model_type == 'DEM_BC_AUG':\n",
    "            datagen = MixedImageDataGenerator(\n",
    "                    rotation_range=20,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True)\n",
    "            datagen.fit(DNN_Train_Data[0])\n",
    "#             print (DNN_Train_Data[0])\n",
    "            h = self.model.fit_generator(\n",
    "                    datagen.flow((DNN_Train_Data[0], DNN_Train_Data[1:]), None, batch_size=self.batch_size), \n",
    "                    validation_data=(DNN_validate_Data, None), steps_per_epoch = DNN_Train_Data[0].shape[0]//self.batch_size,\n",
    "                    epochs=self.epochs, shuffle=True, verbose = self.verbose, workers=1, use_multiprocessing=False, \n",
    "                    callbacks=callbacks)\n",
    "#         elif self.model_type == 'DEM_BC':\n",
    "#             h = self.model.fit(DNN_Train_Data[0],  DNN_Train_Data[1], validation_data = DNN_validate_Data,\n",
    "#                         epochs=self.epochs, batch_size = self.batch_size, shuffle=True, verbose = self.verbose, callbacks=callbacks)\n",
    "        else:\n",
    "            h = self.model.fit(DNN_Train_Data,  validation_data = (DNN_validate_Data, None),\n",
    "                        epochs=self.epochs, batch_size = self.batch_size, shuffle=True, verbose = self.verbose, callbacks=callbacks)\n",
    "        self.scores.append(pd.DataFrame(h.history))\n",
    "        \n",
    "#         pair_list = []\n",
    "#         for i in range(10):\n",
    "#             pair_list.append(np.array(list(zip(DNN_validate_Data[0][i], self.model.predict(DNN_validate_Data)[i]))))\n",
    "#         print (np.moveaxis(np.array(pair_list), 0, 1))\n",
    "#         print (pd.DataFrame(np.array(pair_list)).T)\n",
    "#         print (self.model.predict(DNN_validate_Data)[:10])\n",
    "#         zs_model = Model(inputs = self.model.inputs[1:3], outputs = self.model.outputs[0])\n",
    "#         train_attr_x_img = zs_model.predict(DNN_Train_Data[1:3], verbose = 1)\n",
    "#         train_label = DNN_Train_Data[-1].flatten()\n",
    "#         validate_attr_x_img = zs_model.predict(DNN_validate_Data[1:3], verbose = 1)\n",
    "#         validate_label = DNN_validate_Data[-1].flatten()\n",
    "# #         print (train_label)\n",
    "# #         print (validate_label)\n",
    "#         bst = self.lgbm_train(train_attr_x_img, train_label, validate_attr_x_img, validate_label)\n",
    "        \n",
    "#         cand_feature_map = zs_model.predict(create_dem_data(self.cand_class_id_emb_attr, only_emb = self.only_emb), \n",
    "#                                             verbose = 1)\n",
    "        \n",
    "#         attr_x_img_model = Model(inputs = self.model.get_layer('attr_x_img_model').inputs, \n",
    "#                          outputs = self.model.get_layer('attr_x_img_model').outputs)\n",
    "#         pred = find_nearest_class(self.cand_class_id_emb_attr, \n",
    "#                                   validate_part_df, \n",
    "#                                   cand_feature_map = cand_feature_map, \n",
    "#                                 img_feature_map = DNN_validate_Data[0],\n",
    "#                                 zs_model = bst, model_type = 'lgb')\n",
    "#         calc_detailed_accuracy(validate_part_df, pred, self.class_id_dict)\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    def predict(self, test_part, batch_size = 1024, verbose=2):\n",
    "        \"\"\"\n",
    "        Keras Training\n",
    "        \"\"\"\n",
    "        print(\"-----DNN Test-----\")\n",
    "        pred = self.model.predict(self.DNN_DataSet(test_part), verbose=verbose)\n",
    "        if self.model_type == 'r':\n",
    "            pred = pred[:, -1]\n",
    "        return pred\n",
    "\n",
    "def train_zs_model(train_data, class_id_emb_attr, flags, img_flat_len,\n",
    "                   round1_class_id = None,\n",
    "                   round2_class_id = None,\n",
    "                   img_model = None):\n",
    "    print(\"Over all training size:\")\n",
    "    print(train_data.shape)\n",
    "\n",
    "    fold = 10 #flags.dem_nfold\n",
    "    ensemble_nfold = 10 #flags.dem_ensemble_nfold\n",
    "    kf = KFold(n_splits=fold, shuffle=True, random_state = 100)\n",
    "    num_fold = 0\n",
    "    models = []\n",
    "    model_type = 'RCNN_DEM_BC'\n",
    "    if model_type == 'WV2ATTR':\n",
    "        train_data = class_id_emb_attr\n",
    "    scores = []\n",
    "    classes = train_data.class_id.unique()\n",
    "    model_file_names_0 = glob.glob(path + '../submit/image_05108_10fold_zs_DEMBC_02161/zsmodel_*.h5')\n",
    "    model_file_names_1 = glob.glob(path + '../submit/image_05108_10fold_zs_DEMBC_negaug5_/zsmodel_*.h5')\n",
    "    model_file_names_2 = glob.glob(path + '../submit/image_05108_10fold_zs_DEMBC_negaug2_/zsmodel_*.h5')\n",
    "    model_file_names = [model_file_names_0, model_file_names_1, model_file_names_2]\n",
    "    \n",
    "    for train_index, test_index in kf.split(classes):\n",
    "        print ('Fold: ', num_fold)\n",
    "        seen_class = classes[train_index]\n",
    "        unseen_class = classes[test_index]\n",
    "        \n",
    "        train_part_df = train_data[train_data.class_id.isin(seen_class)]\n",
    "        validate_part_df = train_data[train_data.class_id.isin(unseen_class)]\n",
    "\n",
    "        seen_round1_id = np.intersect1d(seen_class, round1_class_id)\n",
    "        seen_round2_id = np.intersect1d(seen_class, round2_class_id)\n",
    "        unseen_round1_id = np.intersect1d(unseen_class, round1_class_id)\n",
    "        unseen_round2_id = np.intersect1d(unseen_class, round2_class_id)\n",
    "        print ('Seen unseen Classes: ', seen_class.shape[0], unseen_class.shape[0])\n",
    "        print ('Seen round1, round2: ', seen_round1_id.shape[0], seen_round2_id.shape[0])\n",
    "        print ('Unseen round1, round2: ', unseen_round1_id.shape[0], unseen_round2_id.shape[0])\n",
    "\n",
    "        zs_model = DEM(scores = scores, flags = flags, model_type = model_type, \n",
    "                    seen_class = seen_class, img_flat_len = img_flat_len, \n",
    "                    unseen_class = unseen_class,\n",
    "                    class_id_emb_attr = class_id_emb_attr,\n",
    "                    unseen_round1_id = unseen_round1_id,\n",
    "                    unseen_round2_id = unseen_round2_id,\n",
    "                    img_model = img_model,\n",
    "                    only_emb = True,\n",
    "                      c2c_neg_cnt = None,\n",
    "                      emb_weight = emb_weight)\n",
    "        \n",
    "        if num_fold == 0:\n",
    "            print (zs_model.model.summary())\n",
    "        zs_model.train(train_part_df, validate_part_df)\n",
    "        models.append((zs_model.model, model_type))\n",
    "#         zs_model.model.save('./only_emb.h5')\n",
    "#         for model_files in model_file_names:\n",
    "#             zs_model = DEM(scores = scores, flags = flags, model_type = model_type, \n",
    "#                         seen_class = seen_class, img_flat_len = img_flat_len, \n",
    "#                         unseen_class = unseen_class,\n",
    "#                         class_id_emb_attr = class_id_emb_attr,\n",
    "#                         unseen_round1_id = unseen_round1_id,\n",
    "#                         unseen_round2_id = unseen_round2_id,\n",
    "#                         img_model = img_model)\n",
    "#             print ('model file name: ', model_files[num_fold])\n",
    "#             zs_model.model.load_weights(model_files[num_fold])\n",
    "#             zs_model.model.trainable = False\n",
    "# #             model_eval(zs_model.model, model_type, validate_part_df, \n",
    "# #                        cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \n",
    "# #                     img_feature_map = extract_array_from_series(validate_part_df['target']),\n",
    "# #                     class_id_dict = {\n",
    "# #     #                              'seen_class': seen_class,\n",
    "# #                                  'Unseen_class': unseen_class,\n",
    "# #     #                              'Unseen_round1_id': unseen_round1_id,\n",
    "# #                                  'Unseen_round2_id': unseen_round2_id,},\n",
    "# #                     class_to_id = dict([(c, i) for i, c in enumerate(class_id_emb_attr.class_id.values)]))\n",
    "#             models.append((zs_model.model, model_type))\n",
    "#         print ('Multi models votes-------')\n",
    "#         multi_models_vote(models = models[-3:], \n",
    "#                       eval_df = validate_part_df,\n",
    "#                     cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)],\n",
    "#                     img_feature_map = extract_array_from_series(validate_part_df['target']), \n",
    "#                     class_id_dict = {\n",
    "#                          'Unseen_class': unseen_class,\n",
    "#                          'Unseen_round2_id': unseen_round2_id,})\n",
    "        \n",
    "        num_fold += 1\n",
    "        if num_fold == ensemble_nfold:\n",
    "            break\n",
    "    return models\n",
    "\n",
    "# img_model = DenseNet(scores = None, \n",
    "#              cat_max = 365, #flags.cat_max, \n",
    "#              flags = None, \n",
    "#              model_type = 'DenseNet').model\n",
    "# img_model.load_weights(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05108_TF14_DensetNetProcess/imgmodel_20181013_092715.h5')\n",
    "# img_flat_model = Model(inputs = img_model.inputs, outputs = img_model.get_layer(name = 'avg_pool').output)\n",
    "img_flat_model = None\n",
    "\n",
    "round1_class_id = list(set(train_data.class_id.unique()) - set(round2_class_id))\n",
    "zs_models = train_zs_model(train_data[train_data.class_id.isin(round2_class_id)], \n",
    "       class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(round2_class_id)], \n",
    "       flags = None, \n",
    "       img_flat_len = 1032,                     \n",
    "       round1_class_id = round1_class_id,\n",
    "       round2_class_id = round2_class_id,\n",
    "       img_model = img_flat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(round2_class_id)]\n",
    "cand_class_id_emb_attr = cand_class_id_emb_attr[~cand_class_id_emb_attr.class_id.isin(train_data.class_id.unique())]\n",
    "vote_preds = multi_models_vote(models = zs_models[-3:], \n",
    "                      eval_df = test_data,\n",
    "                    cand_class_id_emb_attr = cand_class_id_emb_attr,\n",
    "                    img_feature_map = extract_array_from_series(test_data['target']))\n",
    "sub = pd.DataFrame(vote_preds, index = test_data['img_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_label = time.strftime('%Y%m%d_%H%M%S')\n",
    "sub.to_csv(path + \"../submit/submit_\"+ time_label + \".txt\", header = False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_wv_to_attr():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21665"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((2, 2)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.422474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.432568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.428888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.418894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.422408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.421952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.425294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.415140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.419589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.416849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.417839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.410240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.408449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.400016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.393048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Re\n",
       "Epoch          \n",
       "1      0.422474\n",
       "2      0.432568\n",
       "3      0.428888\n",
       "4      0.418894\n",
       "5      0.422408\n",
       "6      0.421952\n",
       "7      0.425294\n",
       "8      0.415140\n",
       "9      0.419589\n",
       "10     0.416849\n",
       "11     0.417839\n",
       "12     0.410240\n",
       "13     0.408449\n",
       "14     0.400016\n",
       "15     0.393048"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEWCAYAAACKSkfIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3XdcFMf7wPHP3HEUaRYEC9IURVTELlbUiBpjN7EkRtNMMb2aakzPN6bHJD8T04yJMZZYYqLGgL1iwS5FVOygIiCd+f2xQFApB9xxB8z79fKFu7c78wD63N7s7DNCSomiKIpSO+gsHYCiKIpSdVTSVxRFqUVU0lcURalFVNJXFEWpRVTSVxRFqUVU0lcURalFVNJXFEWpRVTSVxRFqUVU0lcURalFVNJXahUhRLwQ4jkhRJQQIk0IMVcI4SGE+EsIkSKE+EcIUS//2O5CiC1CiCtCiH1CiNAi7dwjhDicf06cEOLBIq+FCiEShBDPCCEuCCHOCiHuscC3qyg3UUlfqY3GAAOBlsAw4C/gJaAh2v+Jx4UQTYE/gbeA+sCzwGIhRMP8Ni4AtwEuwD3Ax0KIjkX6aAS4Ak2B+4DZBW8mimJJKukrtdHnUsrzUsrTwEZgu5Ryj5QyA1gKdADuAlZJKVdJKfOklGuBXcCtAFLKP6WUsVKzHlgD9C7SRzbwhpQyW0q5CkgFWlXdt6goxVNJX6mNzhf5e3ox206AN3B7/tDOFSHEFaAX0BhACDFECLFNCHEp/7VbAbci7SRJKXOKbF/Lb1dRLMrG0gEoipU6BcyTUj5w4wtCCDtgMXA3sExKmS2E+AMQVRyjopSbutJXlOL9DAwTQgwSQuiFEPb5N2g9AVvADrgI5AghhgBhlgxWUYylkr6iFENKeQoYgXaD9yLalf9zgE5KmQI8DiwELgMTgeUWClVRykWoRVQURVFqD3WlryiKUouopK8oilKLqKSvKIpSi6ikryiKUotY3Tx9Nzc36ePjY+kwrpOWloajo6OlwzBadYq3OsUK1Sve6hQrVK94rTHWyMjIRCllw7KOs7qk7+Pjw65duywdxnUiIiIIDQ21dBhGq07xVqdYoXrFW51iheoVrzXGKoQ4YcxxanhHURSlFlFJX1EUpRZRSV9RFKUWsboxfUVRrE92djYJCQlkZGSYrQ9XV1cOHz5stvZNyZKx2tvb4+npicFgqND5KukrilKmhIQEnJ2d8fHxQQjzFBNNSUnB2dnZLG2bmqVilVKSlJREQkICvr6+FWpDDe8oilKmjIwMGjRoYLaErxhHCEGDBg0q9YlLJX1FUYyiEr51qOzvQSX9sqScp+GFjZaOQlEUxSRU0i/Lpo9oc2gWJCdYOhJFqZGEEDzzzDOF27NmzeL11183a58+Pj6MGTOmcHvRokVMmTLFrH1aC5X0yxIbrn09sdWycShKDWVnZ8eSJUtISkqq0n4jIyM5dOhQlfZpDVTSL83VM5B4VPv7ic2WjUVRaigbGxumTp3K7Nmzb3otPj6e/v37ExQUxIABAzh58iQAU6ZM4fHHH6dHjx74+fmxaNGiwnM++OADunTpQlBQEDNmzCix32eeeYa33377pv2XLl1i5MiRBAUF0b17d6KiogB4/fXXuffeewkNDSUoKIjPPvus8Jyff/6Zrl27EhwczIMPPkhubm6Ffx7mppJ+aeIiAMiwc4eT6kpfUcxl2rRpLFy4kOTk5Ov2P/bYY0yePJmoqCjuvPNOHn/88cLXzp49y6ZNm1i5ciXTp08HYM2aNURHR7Njxw727t1LZGQkGzZsKLbPO+64g927dxMTE3Pd/hkzZtChQweioqJ45513uPvuuwtfO3LkCKtXryY8PJyZM2eSnZ3N4cOH+e2339i8eTN79+5Fr9czf/58U/1oTE4l/dLEhkMdN840CYOLR+DaJUtHpCg1kouLC+PHj7/u6hlg69atTJw4EYBJkyaxadOmwtdGjhyJTqcjMDCQ8+fPA1rSX7NmDR06dKBjx44cOXKE6OjoYvvU6/U899xzvPvuu9ft37RpE5MmTQKgf//+JCUlcfXqVQCGDh2KnZ0dDRo0wN3dnfPnz7Nu3ToiIyPp0qULwcHBrFu3jri4ONP8YMxAPZxVEim1K32/UJJt2mj7Tm6FgKGWjEpRaqxHHnmEvn37cs899xh1vJ2dXeHfC9b6llLy4osv8uCDDxrVxqRJk3j33Xdp27ZtufvU6/Xk5OQgpWTy5Mk3vXlYK3WlX5ILhyDtAhfcezD/ojdSbwcntlg6KkWpserXr88dd9zB3LlzC/f16NGDBQsWADB//nx69+5dahuDBg3iu+++IzU1FYDTp09z4cIFAAYMGMDp06evO95gMPDUU0/x8ccfF+7r3bt34fBMREQEbm5uuLi4lNjngAEDWLRoUWE/ly5d4sQJo6ocW4RK+iXJn7Xz9K66LIqFyFw/LhwIJzPHem/QKEp198wzz5CYmFi4/fnnn/P9998TFBTEvHnz+PTTT0s9PywsjIkTJxISEkK7du0YO3YsKSkp5OXlERMTQ/369W8657777iMnJ6dw+/XXXycyMpKgoCCmT5/Ojz/+WGqfgYGBvPXWW4SFhREUFMTAgQM5e/ZsOb/zqqOGd0oSF85lB282XbBnQoAtZy91IDh5Abd9tJonb+3AoDaN1BOKimICBVflAB4eHly7dq1w29vbm3///femc3744YcS23jiiSd44oknrnv9wIEDjBkzBgcHB0CbFVTAzs6OM2fOFG7Xr1+fP/7446Y+b3x24MCBA4V/HzduHOPGjSvmu7M+6kq/ODmZ5MVvYmVaAEPbNWaQj4Fhw8dgI/JoL6J56OfdjJ+zjQOnk8tuS1EUi2vbti0fffSRpcOwCirpFyPv5HZ0ORns1LVnxvBAbadnVxA63u2Ywlsj2xJzIZVhX2zi2d/3cf6q+crNKoqimJJK+sU4uGkZOVJHv0GjcHe213bau0CjIHSntnFXd2/Cnwtlam8/lu89Q+gHEXz6TzTpWWq8X1EU66aS/g3OJqdDbDhxdgGM7N76+he9e0DCTsjJwsXewIu3tuafp/sS2qohH/9zjP4fRrB0TwJ5edIywSuKopTBqKQvhBgshDgqhIgRQkwv5bgxQggphOicv91VCLE3/88+IcQoUwVuDlJK3l20hTbE4RE85OYbtV4hkJMBZ/f+t6tBHb66qxO/Te2Om5MdT/22j1FfbSHyhHqQS1EU61Nm0hdC6IHZwBAgEJgghAgs5jhn4Alge5HdB4DOUspgYDDwf0IIq50xtDLqLFmxG9AJiWvbsJsP8ArRvhZTh6ebXwOWTevJrNvbcy45nTFfbWXaL7s5denaTccqiqJYijFX+l2BGCllnJQyC1gAjCjmuDeB94HCu5pSymtSyoIJsPaA1Y57XE7L4vXlBxnhchRp6wxNO918kFNDcGtZYsVNnU4wtpMn4c+G8vgAf9YdPs+Aj9bz/t9HSMnINvN3oCiKUjZjrrqbAqeKbCcA3YoeIIToCDSTUv4phHjuhte6Ad8B3sCkIm8CRY+ZCkwFbZ5uREREeb4Hk/gmKpMr13Lo5bybJOcADmz872o+NTW1MKaWBh8aHt/E5vB/QZT8ntnRAO/0tOP3Y1l8FRHL/C2xjPa3pY+nDTozz+8vGq+1q06xQvWK15Sxurq6kpKSYpK2SpKbm1tqH+np6YwePZqVK1eSkJBAly5d8Pf3Jysriw4dOjB79uwKLxZu6ljL8vLLLxMWFkbfvn0rdH5GRkbFf7dSylL/AGOBb4tsTwK+KLKtAyIAn/ztCLQhnRvbaQ3sAOxL669Tp06yqkUcvSC9X1gp5/yxTsoZLlJu+/q618PDw//b2PurdszZ/Ua3v+fkZTn6y83S+4WVctDH6+Wm6Ismirx418Vr5apTrFJWr3hNGeuhQ4dM1lZJrl69WurrX3zxhfzkk0+klFIeP35ctmnTRkopZU5OjuzXr5/8+eefzR5jgbJiLUt8fLwcOHBghc8v7vcB7JJl5HMppVFX+qeBZkW2PfP3FXAG2gIR+Tc+GwHLhRDDpZS7iry5HBZCpOYfuwsrkZaZw0tL9uPX0JEpjWK1nX79Sj6hcFx/CzQyrkhTcLO6LHoohD/3n+XdVUe489vt3NLanVeGBuLj5ljJ70BRqtbMFQc5dOaqSdsMbOLC06FepR4zf/58fvnll5v26/V6unbtWlhXJzc3l+nTpxMREUFmZibTpk0rswCbk5MTDz/8MKtWraJx48a88847PP/885w8eZJPPvmE4cOHX9dueno6jz32GA8++CCpqamMGDGCy5cvk52dzVtvvcWIESOIj49nyJAh9OrViy1bttC0aVOWLVuGg4MD3t7eJCUlce7cORo1alTxH1wFGDOmvxPwF0L4CiFsgfHA8oIXpZTJUko3KaWPlNIH2AYMl1Luyj/HBkAI4Q0EAPGm/iYq48M1xzh9JZ33xwRhOLEenJuAm3/JJ9T1AhdPOFm+4mtCCG4LasK6Z/ry/OBWbIu7xIRvtqmxfkUxQlZWFnFxcfj4+Nz0WkZGBtu3b2fw4MEAzJ07F1dXV3bu3MnOnTv55ptvOH78eKntp6Wl0b9/fw4ePIizszOvvPIKa9euZenSpbz22ms3tRsREVHYrr29PUuXLmX37t2Eh4fzzDPPFFb9jI6OZtq0aRw8eJC6deuyePHiwj47duzI5s1VvzhTmVf6UsocIcSjwGpAD3wnpTwohHgD7ePE8lJO7wVMF0JkA3nAI1LKxFKOr1J7Tl7m+y3HmdTdmy5errBgvVY6ubQxdyHAOwSOb9TKL5dzfN7eoOeR0BaE+DVgzFdbeO+vI7w9ql0lvxNFqTozhrUxS7uljZEnJiZSt27d6/bFxsYSHBzM8ePHGTp0KEFBQYBWUz8qKqpwNa3k5GSio6Px9fUtsX1bW9vCN4127dphZ2eHwWCgXbt2hXV6irabl5dHSkoK0dHReHp68tJLL7FhwwZ0Oh2nT58urO/v6+tLcHAwAJ06dbqu5o+7u/t1NX+qilHTJ6WUq4BVN+x7rYRjQ4v8fR4wrxLxmU1WTh7TF++nkYs9zw9upc29z7hS+tBOAa8Q2P87XD4O9f0q1H8Hr3rc29OXbzcdZ1j7JnT3a1ChdhSlNnBwcCAj4/pyJ82bN2fv3r0kJibSs2dPli9fzvDhw5FS8vnnnzNo0CCj2zcYDIXP5eh0usK6+TqdrrACZ9F2U1JScHZ2BrTibxcvXiQyMhKDwYCPj09hrDfW309PTy/czsjIKCwAV5Vq7RO5X0XEcvS8VkfH2d5QuDQifkbcTffuoX2tZH39Z8Ja4VW/DtMXR6kSDopSinr16pGbm3tT4gdwc3PjvffeK1zEZNCgQXz11VdkZ2tDp8eOHSMtLQ2AgICACsdQUrvJycm4u7tjMBgIDw83upb+sWPHjF68xZRqZdKPPp/CF+HRDGvfhAGtPbSdseHg0Rac3MtuwK0VONQvcb6+sRxs9bw3ph3xSdf45J9jlWpLUWq6sLCw65ZLLGrkyJFcu3aNjRs3cv/99xMYGEjHjh1p27YtDz74IDk5OSQmJhaOtVdE0Xa7detW2O6dd97Jrl27aNeuHT/99JNRbyzZ2dnExMTQuXPnCsdTYcZM8anKP+aespmbmydHf7lZtp+5Wl5MydB2ZqZJ+YablH+/VOw5xU59+2WClJ8GmySm6Yv3Sd/pK+Xek5dN0l5tnVZYFapTvDVtymZkZKS86667Ktz+ihUr5Kefflrh84uq7JTNJUuWyFdeeaXC51dmymatu9Kft+0EkScu89ptgbg55Y+3ndgCuVnQ3Ijx/ALeIXApDlLOVzqmF29tTUNnO15YHEVWTl6l21OUmqhjx47069eP3NyKDYXedtttPP744yaOqmJycnJ45plnLNJ3rUr6p6+k87+/j9Db341RHZr+90JcOOhtwauH8Y0VHFvOqZvFcbE38PbIdhw5l8JXEbGVbk9Raqp7770XvV5v6TAq7fbbb79pNlJVqTVJX0rJy0v3I4F3RrW7voJmXAQ06wa2dYxvsHEQGOqYbLH0WwI9GN6+CV+ER3PsvHkfd1cUpfaqNUl/+b4zRBy9yLNhrWhWv0hyT70A5w+Ub2gHQG+AZl0rfTO3qBnDAnG2N/D8oihyVU1+RVHMoFYk/UtpWcxccYjgZnWZ3MPn+hcLp2qWM+mDNsRz/gCkX6lsiAA0cLJjxrBA9p66wvebS3+CUCnF6d1w8ailo1AUq1Qrkv4bKw6SkpHN/8YGodfd8ARtXAQ41IPG7cvfsHcIIOHUDlOECcDw9k0YEODOrDVHOZGUZrJ2a4WcLFg7A/lNf3J/maA9Ma0oynVqfNIPP3qBP/ae4ZHQFrT0cL7+RSm1+fm+fUBXgZtDTTuDzlDsoioVJYTgrVFtMeh0TF+8v1LzimuVpFiyvrkFNn/C/jxf9JdjyYzfZumoFBNKT0+nb9++5ObmEh8fj4ODA8HBwQQGBnL33XcXPjRlaVOmTCksAXH//fdz6NChCrWzcuXKwro/plSjk35qZg4vL9lPC3cnHunX/OYDEo9BypmKDe2AduO3SQc4abpxfYDGrg68eGtrtsYlsWDnqbJPqM2k5Oq2H8mc3Ytr52J4NOdJfvD/nGvSjsN//5+lo1NM6LvvvmP06NGFs3cKyjDs37+fhIQEFi5caOEIb/btt98SGHjTQoNGGTp0KCtWrODaNdOuvme1SxeawqzVRzl7NYNFD/XAzqaYK/nYcO1reW/iFuUdAlu/hOx0MJiujsaErs1Yse8M7/x5mH6t3Gnkam+ytmuKy5cSOTP/YdokrWF7XgDrAt5i+pCeeNarQ+THffE/t5rDJ8/T2svD0qHWLH9Nh3P7Tdtmo3bQ6+VSD7HW0spSSh577DHWrl1Ls2bNsLW1LWw3NDSUWbNm0blzZx5++GF27txJeno6Y8eOZebMmQD4+PgwefJkVqxYQXZ2Nr///jsBAQEIIQgNDWXlypXccccd5f2JlqjGXulHnrjMj1vjmRziQyfvesUfFBcB9Xyhnk/FO/LqAXnZcDqy4m0UQwjBe2PakZ2Xxyt/qGGeopLTs/l18e+kfdqdVon/8GfD+/F4dC0vTRyIZz1tZlbLQQ/iIq6x4ve5aiZUDWDNpZWXLl3K0aNHOXToED/99BNbthQ/jfvtt99m165dREVFsX79eqKiogpfc3NzY/fu3Tz88MPMmjWrcH/nzp3ZuHFjeX9cpaqRV/qZObm8sDiKJq4OPDeoVfEH5WZD/CZoN7ZynXl1A4Q2X9+nV+XauoF3A0eeDWvFW38eZkXUWYa3b2LS9qub1MwcftwUg9z4IQ/JRVwxuHN22FKGtg+96VjngP6kOzSm65W/+X7zeO7vXbFqqEoxhrxnnnaraWnlDRs2MGHCBPR6PU2aNKF///7F9rFw4ULmzJlDTk4OZ8+e5dChQ4Uxjx49GtDKLy9ZsqTwHHOUX66RSX92eCwxF1L5/p4uONqV8C0m7IKslMoN7YA288ejjcke0rrRPT19WRF1lteXH6Rn8wY0cLIr+6QaJj0rl5+3nWBpxDZm5HxKN90RrviPxG3sZ2DvWvxJOh32ne+kz8aPeG3NNga1aXT98xlKtWLNpZVXrVpVfKNFHD9+nFmzZrFz507q1avHlClTrvt+CvrT6/WF/YF5yi/XuOGdo+dS+CoihpHBTejXqpSKmXHh2sLmvn0q36lXCCTshNyb1nyvNL1O8L8xQaRkZPPGyorNAqiuMnNy+XFLPH0+CCfy7x/5XT5HZ7tTMPJr6t75Q8kJP58InoiOPEaIjby0VA2RVWfWXFq5T58+/Pbbb+Tm5nL27FnCw8NvOvfq1as4Ojri6urK+fPn+euvv4zq0xzll2tU0s/Nk7ywOApnewOvlbW6T1yENvPGoYTx/vLwDoGsVDgXVfaxFdCqkTPT+rVg2d4zrDtc+QJv1i47N49fd5yk3wcRvLc8kndtvuVr209wbNwS/UMbIXiCcSuWNWgOzbpxn/M2NkZf5I+9p8s+R7Fa1lpaedSoUfj7+xdOHQ0JCbnp3Pbt29OhQwcCAgKYOHEiPXv2NKrP8PBwhg4dWuGYi2VMKc6q/FOZ0spzN8ZJ7xdWyj/2JJR+YPoVKV+vJ+U/M41qt8wStclnpJzhIuXmz40LtAIys3Nl2EfrZbe3/5HJ6VmlHltdy//m5ObJRbtOyd7v/yu9X1gpn/z4R5n2YbDMm+Eq5doZUmZnlr+DXd9LOcNFPvfJdzJ45mqZWFBO2wTxWjtVWvl61lRa2Rjnzp2T/fv3L/a1ypRWrjFj+qcuXeOD1Ufp16ph2Tc84zeBzK34/PwbuTTWZgGd3Ao9HjVNmzewtdHxv7FBjPpyM++uOsK7o6tuXd3DZ6+ycNcpDHodrg4G6tYxUNfBlrp1DP9t17HF0VZ/fSE7I+XlSVYdOMvHa48RezGNto2dWBuynxZRHyLqNIC7/wC/0IoF32YU/PUCLzXZw9ILHry58hCfjO9QsbYUiypaWrkilTZvu+02M0RlPidPnuTDDz80ebs1Junn5Ek6+9TjrRsraBYnNlyrkNmsq+kC8O4Bx/6u0GLpxmrfrC739/ZjzoY4hrVvTI/mbmbpp8DpK+l8uOYoS/ecxlavQwjIyC653r+NThR5I7ClroMB1yJvEDe+VreOgd3nc3jvs40cOZdCSw8nvhvrRb/DryP2/AOtboXhX4BjJdYPtneFgNuoG7OMx/pM5aPwE4zo0LT0+z1KsaSUFXpTN6V7773Xov1XpS5duhS7X1by3lSNSfq+bo7Mu6+bcQfHRYB3T7Ax4UwYrxDYO197yrdhCdNETeCpW1qy5uA5Xlyyn7+f6IODrelri19Oy+LLiBh+3Kqt9Tm1jx+P9G2Bax0DGdm5JKdnc+VaNleuZXElPZvka9lcSc/S9hXZPnc1gyPnUkhOzyY1s+Sb3L5uNnw6PpjbHA6iXz4KMlNg6IfQ+T7TvIEGT4QDi3i4yTGWuzfglaUHWPNUn5Jndik3sbe3JykpiQYNGlg88ddmUkqSkpKwt6/4w5q17199cgIkRUOnKaZtt3Cx9M1mTfoOtnreHR3EhG+28dHao7w8tGKPeBcnIzuX7zfH82VEDGmZOYzp6MlTA1vSpO5/U8bsDXrsDXo8XMr3jy47N4+r6dqbwpVr2STnv0nEHjvCU6O6YfPvTNj+FbgHwt3LwcN03xd+oeDcBEPUAt4f8zVjv97KrDVHmVHWzX6lkKenJwkJCVy8eNFsfWRkZFQqmVUlS8Zqb2+Pp6dnhc+vfUnfFKUXilPfDxzdtfr6nc37ETSkeQMmdvNi7qbjDA1qQnCzyq3Ak5Obx+LdCXy8NppzVzO4pbU7zw0KoFUj57JPNpJBr6OBk91NzxnsOBuOzXcD4fx+6DoVBr5h0nIWgFZMr/042PwZnYZnM6m7Nz9siWd4+yZ08DLB7K1awGAwlPpwkylERETQoUP1uN9SnWK9UY2asmmUuHBw8tCuKE1JCO1q38TF10ry4pAAPFzseX7RvgqvqyulZO2h8wz5dCMvLN5P47r2/Da1O99O7mLShF+iI6voFPm0VvRuwm9w6wemT/gF2k/Ubt7vX8hzg1rRyMWeF5fsV2sSK7WOUUlfCDFYCHFUCBEjhJheynFjhBBSCNE5f3ugECJSCLE//2vxzydXlbw8iFuvfdw3x7ikdw9IPgVXTpq+7Rs42xt4e1Rbjp1PZXZ4TLnPjzxxiTv+bysP/LSL3DzJ13d1ZMnDPejmV4mbpuVx8SgseYBrdZrBw1ug1WDz9tewpVYKe+8vONvZ8OaIthw5l8KcDWpNYqV2KTPpCyH0wGxgCBAITBBC3HSZLIRwBp4AthfZnQgMk1K2AyYD80wRdIWdPwDXEis+/a8sXvkPZZhwCcXS9A/wYGRwE76MiOHIuatGnRNzIZWpP+1izFdbiU+6xtuj2rLmqT4Mbtu46m7QZabAb3eBjT0H2r4Ezo2qpt/giXDhEJzdxy2BHgwNasxn62KIuZBaNf0rihUw5kq/KxAjpYyTUmYBC4ARxRz3JvA+UPictJRyj5SyoFrQQcBBCGG54jFx+eP5fqHmad+jDdi5wEnz1OEpzmvD2uBib+CFRVHk5JY8VHH+agYvLoki7OP1bIlN4tmwlqx/LpQ7u3ljo6/CUT4pYdk0SIqB278n0968006v03Y06O1gr1ae9/VhbXCw1fPSkv3kqUqcSi0hyprzKYQYCwyWUt6fvz0J6CalfLTIMR2Bl6WUY4QQEcCzUspdxbTzkJTylmL6mApMBfDw8Oi0YMGCyn1XJQjaNwO7zCR2dv2iXOelpqbi5ORk1LHtot7APuM8O7vOrkiIFbL9bA5f7ctkXCtbhvgarov3WrZk1fFs1sRnkyuhv5cNw5rb4mJrmWl3nqf+oEXs98T6TeaU1+hy/WxNIfDg/6h3OYotPb5H6gxsTMhm7oEsJgfa0s/LUOb5po5XSklqNjgZMPknrar+2VZWdYrXGmPt169fpJSyc5kHlvXILjAW+LbI9iTgiyLbOiAC8MnfjgA639BGGyAWaF5Wf5Upw1CqrHQp33SXctUL5T61XI+zb/hQK8mQerHc/VRUXl6evO+HnbLly6vk8YupMjw8XGZk58hvNsTK9jNXS+8XVsrHf90tTySmVVlMxYrboJW/WHCnlHl5UkoLlDU4ulr7/RxaLqXUfnYT5myVbV/7W569kl7m6aaMNzElQ079aaf0fmGlvPXTDfK3HSdlelaOydqvTiUjpKxe8VpjrBhZhsGYz/WngWZFtj3z9xVwBtoCEUKIeKA7sLzIzVxPYClwt5TScnfNTm2DnAzzDe0UKJivX0WzeEC7Qnx7VFtsbXS8sDiKLWdy6D9rPW/9eZh2TV1Z+VgvPh3fAa8GFiwtfPUMLLpHm9o64kuzPbVcpub9tdlbe38FtJ/du6PbkZWbx4zlB6osjNUHzxH28QbCj1xkSg8fcvMkzy+Oovu763h31WFOXTLtEnmKUsCYefo7AX8hhC9ash8PTCx4UUqZDBQOzBYd3hFC1AX+BKZLKU23enhFxIaDzgZ8jKtuV2FNOmjjxie2QuvjZxzvAAAgAElEQVRh5u2rCA8Xe16+tTXTl+xnO9CmiQvvjWlHb/+GVRZDiXKyYOFkyLoGk1eCvYvlYtHbQNA42PYlpF4Ep4Z4N3Dk6YEtefevI/x94CyD2zY2W/dXM7KZufwQi3cn0KaJC788EEyrRs5IKdlx/BI/bT3Bt5uOM2djHAMC3JkU4kPvFm7odOopWMU0ykz6UsocIcSjwGpAD3wnpTwohHgD7ePE8lJOfxRoAbwmhChY1j1MSnmhsoGXW1w4eHYFOzPPP7exA88uVXozt8C4Ls24kJLJtfMneH58L+tJFGtehoQdcPsP4F7xeuYmEzwRtnwG+3+HkEcAuK+XL8v3neHVZQcJae6Gq0PZ4/vltTkmked+38f5lEwe79+CR/v7Y2ujfdgWQtDNrwHd/BpwLjmDX7af4JcdJ/nn8A583RyZ1N2bsZ09cbE3fVxK7WLUtA0p5SopZUspZXMp5dv5+14rLuFLKUNl/k1cKeVbUkpHKWVwkT9Vn/DTkuBslOmfwi2Jd4jWX2bJy7+ZgxCCxwf4072JjfUk/H2/wY45EPKoVvHSGri31j6R7ftvkW0bvY73xwRxKS2L9/46bNLu0rNyeX35Qe78djv2tnoWP9yDp8NaFSb8GzVytefpsFZsnt6fT8cHU6+OgTdWHqL7O+t4eel+jp6r2n9XSs1SO57IPb4ekOYfzy/gFaI9/XlqR9X0Z63O7YcVT4B3L7hlpqWjuV77iVp85/YX7mrb1JX7e/ny645TbI1NMkk3e05eZuhnG/lhSzz39PThz8d6G102w85Gz4jgpix5pCcrHu3F0HaN+T0ygUGfbGD8nK38tf9sqdN0TSkvT3L6SjqbYxJJTM2skj4V86gdtXfiwsHOFZp0rJr+mnXVlmI8uRVaDKiaPq1N+mXtASyHunD799pYujVpNxZWv6Td0B3839oET97Skr8OnOOlpfv564ne2BsqVsU0KyePz9ZF82VEDI1dHfjl/m70aFHxZxLaebrywe3teenW1vy26xTztp7g4fm7aexqz53dvBjf1Qs3E6yfnJqZQ9zFVOIuphF3MZXYxDTiLqZxPDG1sKy2h4sdy6b1opFr9SiOplzPyv4nmoGUEBsBvr2rLvHYOUPj9lX2ZK7VycuDpQ9pFU2nrAInK6xdX6c+tBoCUb/BwJmg18bKtSqm7bjz2+18ti6a5weX/x7EkXNXefq3fRw6e5XbO3ny6rBAk43F13O05aG+zXmgtx//HrnAT1vjmbXmGJ+ti+HWdo24u4cPHZrVLXXOf26eJOHyNeIuphF7MZW4xLTCRH8h5b+reJ0Az3p18GvoSIhfA/waOlK3joHpi/dz3487WfhgiCpPXQ3V/N/YpThIPgk9H6/afr16wK65kJNp2rr91cHGD7UFZYZ8AF5GrnFgCcET4fByiPlHewPI17OFG7d38mTOhjhuC2pCYBPjZhvl5knmbIjj47XHcHGw4Zu7OzMw0MMsoet1goGBHgwM9CD2Yirztp5gUWQCf+w9Q7umrtwd4o0hM4/IE5e1hF4ksZ9IukZWkWGhunUM+Lk50qdlQ/waOuLn5kTzho54NaiDnc3Nn3Qc7Wy474edPLFgD/83qTN6a7l/pBil5if9wtILVXQTt4B3CGybDWf2WnfiM7WYfyD8bWh3B3R9wNLRlK7FLeDYUFv8pkjSB3h5aGvCj15g+pIolj7Ss8zEFp+YxrO/72PXicsMaduIt0a2vamMtLk0b+jE68Pb8OygVizdc5qftsTz3KIo7cVwbRaZQS/wql8Hv4ZO9G/tTnM3Jy3BN3SivqNtufrr18qdmcPb8Oqyg7z15yG1LkE1U/OTfmw4uDaDBs2rtt/C4muba0/Sv3wCFt+vla0e9onlHsAylt6gvTntmAPXLmlDPvnq1rFlxrA2PPbrHr7ffJz7e/sV24SUkvnbT/L2n4cx6AWfjAtmRHATi6wu5WRnw6Tu3tzVzYutcUms2LiHW7oF4dfQiWb1HExaY2lSiA/HE6/x3ebj+Lo5cneIj8naVsyrZs/eyc2B4xvNV0q5NI5u4NaySp/MtajsDFg4SRvPHzcPbB0tHZFxgidCXjbsX3TTS7cFNWZAgDsfrjlW7BOy55IzmPz9Tl754wCdfeqx+qk+jOzQ1OLLCQoh6NHcjUE+Bga09sDXzdEsRfVeHtqaW1p78Pryg4QfqfqZ2ErF1Oykf3YvZCZX3fz8G3n3gJPbIS/XMv1XpVXPwtl9MPr/qv5TVWU0aguNgrQhnhsIIXhzZFt0Al5aur9wQWopJX/sOU3Yx+vZefwSb45sy0/3dqWxq5kWgLFSep3g0/HBtG7swqO/7ObwWePKeyuWVbOTfsHSiL59LdO/Vw/tTefCIcv0X1Uif4Q986DPczeNjVcLwRO1C4TzN/+emtR14IUhAWyMTmTpntOkZEmm/bKbJ3/bi7+HM3890ZtJ3b0tfnVvKY52Nsyd3AVnewP3/bCTC1czyj5JsaianfTjwrWrOMcqrNlelHfBuH7Vl2SoMqcjtav85v0h9EVLR1Mx7W7X6jIVeUK3qLu6edPRqy4zVxzi5U3p/HPoAtOHBLDwwRB83KrJMJYZNXK1Z+6UzlxJz+a+H3dxLSvH0iEppai5ST8zVXsi1lJDOwB1vcDFs+Ym/bQkrZCaUyMYM1dbgLw6cnQD/0FayYjcmxOWTid4f0wQ6dm5uNoJlj/Wk4f6Nq+dUxUvHoW0xJt2t2niyucTOnDwTDJPLthLrlqUxmrV3KR/Yot2g66qp2reqGCx9DIWq6l28nJh8b2QegHu+PG6mS/VUvBESLsAsf8W+7K/hzObX+jPjBB7AhpZsEqoJaRegK2z4ateMLsrLCl+Ku6A1h68elsgaw6dN3n9IsV0au6UzbhwrcSxV3fLxuEdAvsXag+JVacbnGUJfxviImD459C0ispbmJN/GNRpoN3QbRlW7CENne2wqS1X9zmZcPQv2PcrRK/Vakk16QA+vbUZcRlXiy2RfU9PX+IT0/hm43F83By5s5u3BYJXSlNzk35suJZwDRaeUeGVv6jKiS3mT/pZ1+CfGbQ8dQJcT2s1gBq0MP101SN/ak/ddrxb+1MT2NhqY/u7vtPqBjnUs3REVU9K7R7N3l/gwGLIuALOjaHHo1qBOvcALeH/eJtWxLCE9SJevS2Qk5eu8dqyg3jWq0PfllawpoNSqGYm/atn4eJhaD/e0pFAw1bgUF8b4uk4yXz9ZKfDr+MhfiPuOntYtlrbb19Xq+/v2QWadYGmncDeteL9JMVqdXUaB2tlFmqS4Imw/Wst4XW539LRVJ3k0xC1QCs+lxQNNvZaQm8/QXvGpei9Gq/uYOusXf2XkPRt9Do+n9iR27/eyrT5u1n8cA9aNTLzOhaK0Wpm0j++XvtqyZu4BYTQxvXNeTM3OwMWTITjG2DU12y65E5omybawiUJO+HUTq08AhIQ0DAAPDtrnwQ8u4BbK9AZcXsnK02rnKnTaw9gGWpYlcVGQeDeRkt+NT3pZ6XB4ZXajKW4/NLjXj20GlWBI0te3Uxv0P5fRa/VPhmU8CnSyc6G76Z0ZsQXm7n3h50sndYDd+ca9u+lmqqZST82XBuf9WhX9rFVwSsEjqzUPoG4mHgpvpxMLRHH/gsjZmufbiIitI/i7gH/Db9kJGsf3RN2aW8ER1Zqc+sB7Fy0TwAFbwJNO918Y1ZKrTb+hcNw12JtZlJNI4R2tb/mZW2WSsNWlo7ItPLytLIg+xbAoT8gKxXqekPfF6D9OG39YmP4h2mF6s4f1B5uK0FjVwfmTu7CHf+3lQd+imTBA91xsK2mM7xqkJqX9KXUbjD69jXu6rUqFMzXP7kF2o4xXbs5WbDwbohZC8M+hQ53lXysvas2l755f21bSm2opuingQ0fgMyvvtjAP/9NoLO2zOTxDdrygv1frdlrBATdAWtf08a1B1rZwi8VlRSrJfqoBXDlpDY802akNk7vFVL+/yctbtG+xqwtNemDtg7Ap+ODefDnSJ5euJfZEztaz6putVTNS/oXDkPqOesY2inQqD0YHLX6+qZK+rnZsOgerYTx0I+g05TynS8EuLXQ/gTnr3OfmQpndv/3JnDs7+vLE7S6FXo9bZr4rZWTO/gP1OrsD3it+j57kH6FxmfWwNx34dQ2QGjj8/1fhYDbwLZOxdt2aQyN2mlDPL2eKvPwsDaNePnW1rz152H+t/oo04dYwTrJtVjNS/pxEdpXS8/PL0pvo101m6r4Wm42LLpXG6IZ8gF0uc807do5gW8f7Q9onwYuH9eGhC7FQfeHrefTkzkFT9Te8OLC/7uqrQ7SL8ORVdrQTWw4rfKytfs1t7wOQePApYnp+vIPg02fQPoVbXW0MtzXy5f4pDS+Xh+LT4M6jO9aA4cHq4kamPTDoX5zqNvM0pFcz7sHhL9T+emAuTmwZKo2pjroHeg21XQx3kgIbZzX2LHemqLlYG3W095frT/pp1/WptAe/EO74MnLBlcv6P4QkRnedBp2v3kqzPqHadN248KNWvBeCMHrw9pw6lI6r/xxgGb169CzEstHKhVXsy7bcrIgfrN1De0U8AoBpFZ1s6LycuGPh+DgEhj4JoRMM1l4ShE2dtqc/SMrtStZa3PtEuyeBz+PgQ9awLJpkHhU+yT2wL/wZBSEvUWKi7/5Soo37azdJ4r+x+hTbPQ6vpjYgeYNnXjo50iiz6eYJzalVDUr6SfsgOw06xraKeDZGXQG7WZuReTlav+59/8OA2ZU/fKPtU3wRMjJgINLLR2Jpmiin+UPyx+FxGPQ/RF4IByeiIKwN7WZV1VR8VNvA80HaDdz8/LKPj6fs72BuVM6Y2ej554fdpKYmln2SYpJ1aykHxcBQqctgm5tDA7aY+wVWSw9Lw9WPK49Et/vZehdw2+mWoMmHbTnGfb9arkYrl2C3T/BvNFFEn209gnvukTf0TKrlPmHQep5OBdVrtM869Vh7uTOJKZm8sBPu8jIrgXrTVgRo5K+EGKwEOKoECJGCDG9lOPGCCGkEKJz/nYDIUS4ECJVCPGFqYIuUWx45Z84NSfvHnBmj1YuwVh5ebDySdjzszafuu/z5otP+U/BnP1T2yExpur6vXZJW59g3iht6Gb5Y3ApFkIehakR8MQ+GPiG5RJ9UQX3O6LXlvvU9s3q8sm4YPaeusIzv+8jT1XlrDJlJn0hhB6YDQwBAoEJQojAYo5zBp4Aig5aZwCvAs+aJNrSpF/Wphta49BOAe8e2o2207uMO15KrVb97h+h9zPVt159dRU0TvvkaO6r/bQkiPwBfhqpJfoVj2uzpXo8BlPXw+N7tWcGmnSwfKIvyqmhFlNM+ZM+wOC2jZk+OIA/o87y0dpjJg5OKYkxs3e6AjFSyjgAIcQCYARw4zJDbwLvA88V7JBSpgGbhBAtTBNuKRKjwdbJOm/iFmjWDRDaEE/BtMiSSAl/T4ddc6HnE9r8amv6D18bODfSxq33LdCG1Uw5XfXKKW3WzZGVWokOmQv1fP8rg9C4ffX4ffuHaQ/13bCwvLGm9vEjPimNL8JjCHLTc9xwnN7+DWne0LHWrkZmbsYk/abAqSLbCUC3ogcIIToCzaSUfwohnsMSmnWF549b938Uh7rg0absm7lSwuqXteJf3afBLTOt+/uqyYInaM9ExG/QHm6qKCm1ZTMLEv3Zfdr+hgHam3qbkVrtn+r2e/YPg/Xva2VA2o0t9+lCCN4Y0Za6dWxZuiOOmSu0a8mmdR3o7e9Gn5YN6dncDdc6BlNHXmtVep6+EEIHfARMqUQbU4GpAB4eHkRERFQ2LJNKTU01WUwtbLxpHL+OTf/+g9QV8+OXEr+4n/A6tYSEpkOJsRsI69dbLF5zs/ZYdbnO9NA7krj6Y460Lme8MhfX5KO4JW7HLXEbDhnnkAiuurQk0W8yiW7dSK/TVDv26GU4Wr7fc1mq5Gcrc+lhcOHSpnkcSar4vPtu9tCmYx7pujocTMplf2IWy/acYsHOUwjAz1VHWzc9bd30+LnqLL5qmbX/uy2NMUn/NFD0SSfP/H0FnIG2QET+x7FGwHIhxHAppVGD11LKOcAcgM6dO8vQ0FBjTqsyERERmCwmt0uw6E/6tqwHnp2uf01K+PdNOLUEOt+H59AP8azAlZ9J4zWzahFr+jga7f2VRt07ErFtd+nxZmdoVV4Pr9AWIbmWCHpbrRZUwHREqyG4OjfCFTD3kjpV9rO9NJhGseto1KdPpYbAIiIiuC00lNvzt3Ny89iXcIX1xxLZGH2RFXFXWBabjbO9DT2bu9G7pRt9/BvSrH4lSkpUIlar/3dbAmOS/k7AXwjhi5bsxwMTC16UUiYDhW/xQogI4FljE36t452/qMrJLTcn/Yj3tKccO02BW2dVv4/6NVX7idriKoeWcf31T770KxC9Rhu2if5He1bE1llbgSvgNm2WS0mlimsC/zBtdbgze27+N10JNnodnbzr08m7Pk8PbMmVa1lsjkliY/RFNhy7yN8HzwHg6+ZIH383evs3pHvzBjjZ1bxCA6ZU5k9HSpkjhHgUWA3oge+klAeFEG8Au6SUy0s7XwgRD7gAtkKIkUCYlPLGm8C1h3Mj7Ybdia3a7IwC6/8H69/TKmUO/bh21LipLjw7a1VH9/4Cfi9o+66eyR+f/xPiN0JejrZAfPtxEDBUW1bQxs6ycVeVFgMAob3xmTDp36huHVuGBjVmaFBjpJTEXkxjw7GLbIy+yMJdCfy49QQGvaCjVz36tGxIH/+GtGnioqp63sCot0Qp5Spg1Q37Xivh2NAbtn0qGFvN5d0Tjq7S5uDrdLDxI23N2fYTYNhnKuFbGyG0G7rr3sCXeXBspjY9GLTlKEMe1VaRatKxdv7u6tTX1mGIXgP9qmZasRCCFu5OtHB34t5evmTm5BIZf5n10RfZcCyRD1Yf5YPVR2nkYs+8+7ri76FW7iqgPgdZgncI7P1Zq5cSvRbWzdRqvYyYXX1L+dZ0QeMh/B28Ty7S6s4MmKEN3TRsaenIrIP/QK2gYOpFbf5+FbOz0dOjhRs9Wrjx4hC4kJLBpuhE3ll1mIfn72bZtJ44qmEfoKaVYaguvPIXVVn5NKx9FdqMhpFfq4RvzVybwoMb2BLyHTywTiuFoRL+f/wHAhJi11k6EgDcne0Z3dGTz8Z3IO5iKtOX7EdK9dQvqKRvGfX9wMlDu5nbejiM/kYrYKVYN482ZNk1sHQU1qlRe3B014Z4rEiPFm48E9aKFfvO8OOWeEuHYxVUprEEIbTqiJfj4dYPVMJXqj+dTrvaP/KntuaDFf2bfrhvc3afuMzbqw4T1KwuHb0qsZ5FDaCu9C2l15Mw7BPQqycNlRqixS2QcQVOR1o6kuvodIKP7gimkas90+bvJqmWl3NWSV9RFNNo3g+E3uqGeABc6xj46s5OJKVl8eRve8mtxVU9VdJXFMU0HOppRQWtMOkDtG3qyhvD27AxOpFP/6m9VT1V0lcUxXT8B2qLqqScs3QkxRrXpRljO3ny2b8xhB+9YOlwLEIlfUVRTMd/oPY1xvi1c6uSEII3R7SldWMXnvptL6culWNBoxpCJX1FUUzHoy04N7baIR4AB1s9X93ZkdxcybRfdpOZU7uWa1RJX1EU0xFCu9qPDYfcbEtHUyIfN0dm3dGeqIRk3lhRu0qBqaSvKIpptRgImVfh1A5LR1KqQW0a8WBfP+ZvP8nSPQmWDqfKqKSvKIpp+YWCzsaqh3gKPBfWim6+9XlxyX6OnLtq6XCqhEr6iqKYlr2LVl8qumILplclG72Ozyd2wNnewMM/7yYlw3qHpExFJX1FUUzPPwwuHIRk6x82cXe254sJHTh56RrPL4qq8YXZVNJXFMX0rHzq5o26+TXghcGt+OvAOeZuOm7pcMxKJX1FUUyvYQC4NqsWQzwFHujtx6A2Hrz31xF2xl+ydDhmo5K+oiimVzB1My4CcqpHgTMhBB/c3h7Peg5Mm7+biynVI+7yUklfURTz8A+DrFQ4udXSkRjNxd7AV3d14mpGNo//uoec3DxLh2RyKukrimIevn1Ab1uthngAWjd24a2R7dgal8RHa2teYTaV9BVFMQ9bR/DuWe2SPsDYTp5M6OrFlxGxrD103tLhmJRK+oqimI9/GCQe1VaJq2ZmDAukbVMXnl64l5NJNacwm0r6iqKYj3+Y9rUaXu3bG/R8dWcndELw8PxIMrJrRmE2lfQVRTGfBs2hnk+1ma9/o2b16/DxuPYcPHOV15cftHQ4JqGSvqIo5iOEdrUftx6yMywdTYX0D/Dg0X4tWLDzFAt3nbJ0OJVmVNIXQgwWQhwVQsQIIaaXctwYIYQUQnQusu/F/POOCiEGmSJoRVGqEf8wyEmHE5ssHUmFPTWwJT1bNODVPw5w8EyypcOplDKTvhBCD8wGhgCBwAQhRGAxxzkDTwDbi+wLBMYDbYDBwJf57SmKUlv49AIb+2o5rl9ArxN8Or4D9erY8sj83aRlV9/6PMZc6XcFYqSUcVLKLGABMKKY494E3geKfoYbASyQUmZKKY8DMfntKYpSWxgcwKd3tU76AG5Odsy+syOnL6fzTVQmuXnVM/HbGHFMU6DoQFYC0K3oAUKIjkAzKeWfQojnbjh32w3nNr2xAyHEVGAqgIeHBxEREUYFX1VSU1OtLqbSVKd4q1OsUL3itaZYm+KL/6W1bF/1C+l1mhR7jDXFW5oJAQbmHcrini/XcE8bW4QQlg6pXIxJ+qUSQuiAj4ApFW1DSjkHmAPQuXNnGRoaWtmwTCoiIgJri6k01Sne6hQrVK94rSrWS97w2Ry61b8K3ScWe4hVxVuKUCD5mzUsj82mlW8zXrq1dbVK/MYM75wGmhXZ9szfV8AZaAtECCHige7A8vybuWWdqyhKbVDfFxr4Q0z1HuIpMKqFgckh3nyz8ThfRsRaOpxyMSbp7wT8hRC+QghbtBuzywtelFImSyndpJQ+UkoftOGc4VLKXfnHjRdC2AkhfAF/wLoXzlQUxTz8B8LxjZBV/Z9uFUIwY1gbRnVoygerjzJv2wlLh2S0MpO+lDIHeBRYDRwGFkopDwoh3hBCDC/j3IPAQuAQ8DcwTUpZMx5rUxSlfPwHQm4mxG+0dCQmodMJ/jc2iFtau/PasgMs21s9BjGMGtOXUq4CVt2w77USjg29Yftt4O0KxqcoSk3h3RMMdbQF01vWjEd2DHodX0zsyJTvd/D0wn042dkwoLWHpcMqlXoiV1GUqmFjB36hWtKvQevQ2hv0fDu5C22auPDI/N1si0uydEilUklfUZSq0+IWuHISEqPN39fpSFg2DVLOmb0rJzsbfrinK83q1+H+H3exP8F6n9pVSV9RlKpTsGB69Brz9hP1O3w3BPb8DD+NgLRE8/YH1He0Zd59XXF1MDD5+x3EXEg1e58VoZK+oihVp64XNGxtvqSflwv/vA5L7gfPznDHT1ot/3kjIf2yefosorGrA/Pv74ZOCCbN3U7CZeubqaSSvqIoVct/IJzYApkmvhLOuAoLJsKmj6HTPTDpDwgcAePnw8Wj8PMY7Rgz83FzZN59XUnLzOGub7db3QLrKukrilK1/AdCXjYcX2+6Ni/FwdyBWn2fW2fBsE/AxlZ7rcUtcPsPcHYf/DKuSp4TaN3Yhe/v6cL5q5nc/d0OktOzzd6nsVTSVxSlajXrDrbOphviiVsP3/SH1PMwaSl0feDmYwKGwug5cGqb9mmgCmr7d/Kuz9eTOhFzIYV7f9jJtawcs/dpDJX0FUWpWja20DxUuyqv7NTNHd/AvFHg5AEP/At+fUs+tu0YGDEb4sLh98mQk1W5vo3Qt2VDPh3fgT0nL/Pwz7vJyskze59lUUlfUZSq5x8GV0/DhcMVOz8nC1Y+Baue1YaL7lsL9f3KPi94Igz9CI79DUsegFzzX33f2q4x745ux/pjF3nqt70WL8lc6SqbiqIo5dbiFu1r9BrwuGlNptKlJcHCu7WVuHo+CQNeA1051mbqch/kZMDql7TFXUZ+BTrzXv+O6+LF1fQc3l51GGd7G94d3c5ilTlV0lcUpeq5NAGPdtoQT68njT/v/EH4dTyknIfR30DQHRXrP2QaZF+Df98Cgz3c9om2nq8ZPdDHj+T0bL4Ij8HVwcD0IQEWSfwq6SuKYhn+A2Hzp5CRDPauZR9/5E9YMhVsneCev8CzU+X67/McZKfDxg+1K/7B75k98T8T1pLk9Gz+b0McrnUMPBLawqz9FUeN6SuKYhn+YSBzIS6i9OOkhA0faLNu3FrC1IjKJ/wC/V+F7o/A9q9h3Uyz1wQSQjBzeBtGBDfhf38f5WcLlGRWV/qKoliGZxftCj96jfYQVXGyrsHyR+HAYmh3Owz/XFtz11SEgEHvaFf8mz4GgyP0fa7s8ypBpxPMur09qRk5vLrsAM72NowIvmkVWbNRSV9RFMvQ20Dz/hD9T/FX2Mmntav7s/vglte1m7bmGH4RQpvRk5MB4flj/D0eM30/RRj0Ombf2ZG7v9vBMwv34WxvQ/+AqinJrIZ3FEWxHP8wSD0H5/Zfv//UTpgTCkkxMOFX6PWUecfbdToY/gW0GQVrXtHm/5uZvUHP3MmdCWjszMM/72bH8Utm7xNU0lcUxZKKTt0ssPdX+OFWsK0D9/8DrYZUTSx6G21GUKtbtfn/e+abvUtnewM/3tMVz3oO3PfDTg6cNn9JZpX0FUWxHCd3aByc/3RurnaV/cdD0KwbPBAO7q2rNh69AcZ+rw07LX8U9i8ye5cNnOyYd183XBwMfLz2mNn7U2P6iqJYln8YbJxFUOobcHkvdHkABr+rJWBLMNjDuPkwf6w2RdTGHlrfZtYum9R1YMHU7tRztDVrP6Cu9BVFsTT/MJB51L2yH277GIbOslzCL2BbByb+Bk06wKJ7tJvNZtasfh2c7Mx/Ha6SvqIoltW0I+2gmz4AAA1LSURBVPR9gX3t34TO91o6mv/YOcNdi6FhAPx2JxzfYOmITEIlfUVRLEunh34vkVy3jaUjuZlDXW0xlnq+8Mt4OLnd0hFVmkr6iqIopXFsAHcvA+dG2jj/mT2WjqhSVNJXFEUpi7MHTF6uXfnPG4VjarylI6owo5K+EGKwEOKoECJGCDG9mNcfEkLsF0LsFUJsEkIE5u+3FUJ8n//aPiFEqInjVxRFqRqunnD3crBxoN3+tyDXepZALI8yk74QQg/MBoYAgcCEgqRexC9SynZSymDgf8BH+fsfAJBStgMGAh8KIdSnC0VRqqf6vjDsE+wzL8Lh5ZaOpkKMScBdgRgpZZyUMgtYAFxXHUlKWXSJeUegoJBGIPBv/jEXgCtA58oGrSiKYjEtBpJu36hKSjWYgzFJvylwqsh2Qv6+6wghpgkhYtGu9B/P370PGC6EsBFC+AKdgGaVC1lRFMWCdDpON70VTm6Fs1GWjqbchCyjfrQQYiwwWEp5f/72JKCblPLREo6fCAySUk4WQtgAHwD9gBOAAZgjpfzjhnOmAlMBPDw8Oi1YsKBy35WJpaam4uTkZOkwjFad4q1OsUL1irc6xQrVK96MK+cZGPUYF9x7czTAvBU5jdWvX79IKWXZIylSylL/ACHA6iLbLwIvlnK8Dkgu4bUtQGBp/XXq1Elam/DwcEuHUC7VKd7qFKuU1Sve6hSrlNUr3vDwcCmXPyHlm+5SpiVZOhwppZTALllGPpdSGjW8sxPwF0L4CiFsgfHAdXcwhBD+RTaHAtH5++sIIRzz/z4QyJFSHjKiT0VRFOvWdapWg3/3T5aOpFzKLPQgpcwRQjwKrAb0wHdSyoNCiDfQ3lmWA48KIW4BsoHLwOT8092B1UKIPOA0MMkc34SiKEqV8wgEn96wc6626IpOb+mIjGJUdR8p5Spg1Q37Xivy9ydKOC8eaFWJ+BRFUaxX16mwcBIc+xsChlo6GqOoOfOKoigV1epWcPGEHXMsHYnRVNJXFEWpKL0NdLkX4iLg4lFLR2MUlfQVRVEqo+Nk0NtVm4e1VNJXFEWpDEc3aPv/7d17jFTlHcbx7yMLlluVFkUFFS+oRanI4oolaSvUBi8BUzXeahUv2EYstaattol/tE1jbGOt1bRRqhCLEGM1WmNRgqQXtRVELiIKBJWiIKjxst5A+PWPc9YOsDs7wM68c5znk2z2zLtnOc+QmWfOnjnznjNh8Uz46N3O10/MpW9mtrtaLodNrbDontRJOuXSNzPbXQNHwKDjYf4dsHVr6jRlufTNzLpCyxXw5ipY/XjqJGW59M3MusLQCdB737p/Q9elb2bWFZp6wMiJsOJReOul1Gk65NI3M+sqzROz6RjmT02dpEMufTOzrvL5/eFL4+HZu2HT+6nTtMulb2bWlVomwUfvwJJ7Uydpl0vfzKwrHTQK9huWvaHbyUWqUnDpm5l1JSnb29+wDF55InWaHbj0zcy62rCzoWe/upx906VvZtbVuveEEd+B5Q/DO2tTp9mGS9/MrBpGXgoELLgzdZJtuPTNzKqh38FwxCnwzDTY/FHqNJ9y6ZuZVUvL5fDBm7DsgdRJPuXSNzOrlkO/Dv2PqKs3dF36ZmbV0nb65msLYe2C1GkAl76ZWXUdey706Fs3e/sufTOzatqzLww/H567H1o3pE7j0jczq7qWSbB1c3YmT2IufTOzaut/OBw2Njtnf8vmpFEqKn1J4yS9KGmVpGvb+fl3JS2VtEjSvyQNzce7S5qe/2y5pOu6+g6YmRVCyyR4bx0s/2vSGJ2WvqRuwG3AKcBQ4Ly2Ui9xT0QMi4jhwI3ATfn42cCeETEMaAaukDS4i7KbmRXHkJOh3+Dkl1OsZE+/BVgVEasjYhMwC5hQukJEvFtyszfQNp9oAL0lNQE9gU1A6bpmZo1hj25w/GWw5klYvzRZDEUn8z1LOgsYFxGX5bcvBE6IiMnbrXcl8EOgBzAmIlZK6g7cDYwFegFXR8QO5y1JmgRMAhgwYEDzrFmzdvuOdaXW1lb69OmTOkbFipS3SFmhWHmLlBWKlXdXszZtbuXEpyby+oCvseLIyZ3/wk446aSTnomIkZ2uGBFlv4CzgKklty8Ebi2z/vnA9Hx5NDAD6A7sC7wIHFpue83NzVFv5s2blzrCTilS3iJljShW3iJljShW3t3K+uBVEb8YEPH+m12WJyICWBCd9HlEVHR451XgwJLbg/KxjswCzsiXzwdmR8TmiNgAPAF0/kpkZvZZ1TIJPvkwu45uApWU/nxgiKRDJPUAzgUeKl1B0pCSm6cBK/PlNcCYfJ3ewCjghd0NbWZWWPsdAwePhvlTYeuWmm++09KPiE+AycCjwHLg3ohYJunnksbnq02WtEzSIrLj+hfl47cBfSQtI3vxuCsilnT5vTAzK5KWSfD2GljxaM033VTJShHxCPDIdmPXlyxP6eD3WslO2zQzszZHnQZ9D8jm4znq1Jpu2p/INTOrtW7d4fhLYPU82Liippt26ZuZpTDiYujWo+azb7r0zcxS6LMPHP0tWDwTPqrdZ1Zd+mZmqZwwCTa1ZsVfIy59M7NUBjZnX0/fDlu31mSTLn0zs5RaroA3V2Vv6taAS9/MLKWjz4De+9Rs9k2XvplZSk17QvPFsGI2vPVS1Tfn0jczS615ImiPbGqGKnPpm5mlttfAbGqGvQ+q+qYqmobBzMyq7JQbarIZ7+mbmTUQl76ZWQNx6ZuZNRCXvplZA3Hpm5k1EJe+mVkDcembmTUQl76ZWQNRRKTOsA1JG4FXUufYTn/gjdQhdkKR8hYpKxQrb5GyQrHy1mPWgyNin85WqrvSr0eSFkTEyNQ5KlWkvEXKCsXKW6SsUKy8Rcq6PR/eMTNrIC59M7MG4tKvTG0vV7/7ipS3SFmhWHmLlBWKlbdIWbfhY/pmZg3Ee/pmZg3EpW9m1kBc+mVIOlDSPEnPS1omaUrqTJ2R1E3Ss5IeTp2lM5L2lnSfpBckLZd0YupMHZF0df4YeE7STEmfS52plKQ7JW2Q9FzJ2BckzZG0Mv/eL2XGNh1k/XX+OFgi6QFJe6fMWKq9vCU/u0ZSSOqfItuucOmX9wlwTUQMBUYBV0oamjhTZ6YAy1OHqNDvgNkRcRRwLHWaW9JA4PvAyIg4BugGnJs21Q6mAeO2G7sWmBsRQ4C5+e16MI0ds84BjomILwMrgOtqHaqMaeyYF0kHAt8E1tQ60O5w6ZcREesiYmG+/B5ZKQ1Mm6pjkgYBpwHVv7rybpK0F/BV4E8AEbEpIt5Om6qsJqCnpCagF/Ba4jzbiIh/AG9tNzwBmJ4vTwfOqGmoDrSXNSIei4hP8pv/BgbVPFgHOvi/Bfgt8GOgUGfDuPQrJGkwcBzwn7RJyrqZ7EG4NXWQChwCbATuyg9HTZXUO3Wo9kTEq8BvyPbo1gHvRMRjaVNVZEBErMuX1wMDUobZCZcAf0sdohxJE4BXI2Jx6iw7y6VfAUl9gL8AP4iId1PnaY+k04ENEfFM6iwVagJGAH+IiOOA96mfww/byI+FTyB7oToA6C3p22lT7ZzIzs2u+z1SST8jO6w6I3WWjkjqBfwUuD51ll3h0u+EpO5khT8jIu5PnaeM0cB4SS8Ds4Axkv6cNlJZa4G1EdH2l9N9ZC8C9egbwEsRsTEiNgP3A19JnKkSr0vaHyD/viFxnrIkXQycDlwQ9f0BosPIdgAW58+3QcBCSfslTVUhl34ZkkR2zHl5RNyUOk85EXFdRAyKiMFkbzI+HhF1uzcaEeuB/0o6Mh8aCzyfMFI5a4BRknrlj4mx1Ombztt5CLgoX74IeDBhlrIkjSM7NDk+Ij5InaeciFgaEftGxOD8+bYWGJE/puueS7+80cCFZHvNi/KvU1OH+gy5CpghaQkwHPhV4jztyv8auQ9YCCwle97U1cfwJc0EngKOlLRW0qXADcDJklaS/bVyQ8qMbTrIeivQF5iTP8/+mDRkiQ7yFpanYTAzayDe0zczayAufTOzBuLSNzNrIC59M7MG4tI3M2sgLn1rSJK2lJyGu0hSl30aWNLg9mZkNKsHTakDmCXyYUQMTx3CrNa8p29WQtLLkm6UtFTS05IOz8cHS3o8n+99rqSD8vEB+fzvi/OvtukZukm6I5+D/zFJPZPdKbMSLn1rVD23O7xzTsnP3omIYWSfEr05H/s9MD2f730GcEs+fgvw94g4lmzuoGX5+BDgtog4GngbOLPK98esIv5ErjUkSa0R0aed8ZeBMRGxOp9sb31EfFHSG8D+EbE5H18XEf0lbQQGRcTHJf/GYGBOfvESJP0E6B4Rv6z+PTMrz3v6ZjuKDpZ3xscly1vw+2dWJ1z6Zjs6p+T7U/nyk/z/EokXAP/Ml+cC34NPr0+8V61Cmu0K731Yo+opaVHJ7dkR0XbaZr985s+PgfPysavIrvL1I7Irfk3Mx6cAt+czL24hewFYh1md8jF9sxL5Mf2REfFG6ixm1eDDO2ZmDcR7+mZmDcR7+mZmDcSlb2bWQFz6ZmYNxKVvZtZAXPpmZg3kf05QB5zs1k+HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log = pd.read_csv(path + 'log_ft1300_addattr', header = None, names = ['Epoch', 'Re'], sep = ' ')\n",
    "log.groupby('Epoch').agg({'Re': ['mean', 'median']}).plot(grid = True, title = 'mean')\n",
    "log.groupby('Epoch').agg({'Re': 'std'}).plot(grid = True, title = 'std')\n",
    "# log.groupby('Epoch').agg({'Re': 'median'}).plot(grid = True, title = 'median')\n",
    "# log.groupby('Epoch').agg({'Re': 'count'}).plot(grid = True)\n",
    "log.groupby('Epoch').agg({'Re': 'mean'})\n",
    "# log[log.Fold == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cand shape:  16\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.377448\t1195\t3166\n",
      "\n",
      "\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.263108\t833\t3166\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unseen_class: \t0.385028\t1219\t3166\n",
      "\n",
      "\n",
      "cand shape:  16\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.612995\t1934\t3155\n",
      "\n",
      "\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.556894\t1757\t3155\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unseen_class: \t0.584152\t1843\t3155\n",
      "\n",
      "\n",
      "cand shape:  16\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.590740\t1901\t3218\n",
      "\n",
      "\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.631137\t2031\t3218\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unseen_class: \t0.628962\t2024\t3218\n",
      "\n",
      "\n",
      "cand shape:  16\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.572295\t1793\t3133\n",
      "\n",
      "\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.575806\t1804\t3133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unseen_class: \t0.547399\t1715\t3133\n",
      "\n",
      "\n",
      "cand shape:  16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b714bdda1934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     multi_models_vote(models = zs_model_list, eval_df = validate_part_df,             cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr['class_id'].isin(unseen_class)],             img_feature_map = extract_array_from_series(validate_part_df['target']),\n\u001b[1;32m     20\u001b[0m             class_id_dict = {\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;34m'Unseen_class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munseen_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             })\n",
      "\u001b[0;32m<ipython-input-26-6ca63de5e674>\u001b[0m in \u001b[0;36mmulti_models_vote\u001b[0;34m(models, eval_df, cand_class_id_emb_attr, img_feature_map, class_id_dict)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'cand shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_class_id_emb_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     preds = models_eval(models, eval_df, cand_class_id_emb_attr, img_feature_map, \n\u001b[0;32m---> 41\u001b[0;31m                 class_id_dict)\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# print (preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-6ca63de5e674>\u001b[0m in \u001b[0;36mmodels_eval\u001b[0;34m(models, eval_df, cand_class_id_emb_attr, img_feature_map, class_id_dict, class_to_id)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         pred = model_eval(model, model_type, eval_df = eval_df, cand_class_id_emb_attr = cand_class_id_emb_attr, \n\u001b[0;32m---> 81\u001b[0;31m             img_feature_map = img_feature_map, class_id_dict = class_id_dict, class_to_id = class_to_id)\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-6ca63de5e674>\u001b[0m in \u001b[0;36mmodel_eval\u001b[0;34m(model, model_type, eval_df, cand_class_id_emb_attr, img_feature_map, class_id_dict, class_to_id)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DEM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mzs_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mcand_feature_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzs_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_dem_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand_class_id_emb_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_nearest_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand_class_id_emb_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_feature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_feature_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'class_id'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     return self._predict_loop(\n\u001b[0;32m-> 1739\u001b[0;31m         f, ins, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m           \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2477\u001b[0;31m         **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2478\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1291\u001b[0m                 run_metadata):\n\u001b[1;32m   1292\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1354\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def lgbm_train(train_part, train_part_label, valide_part, valide_part_label, fold_seed,\n",
    "        fold = 5, train_weight = None, valide_weight = None, flags = None):\n",
    "    \"\"\"\n",
    "    LGBM Training\n",
    "    \"\"\"\n",
    "    if flags.stacking:\n",
    "        FEATURE_LIST += ['emb_' + str(i) for i in range(len(CATEGORY_FEATURES) * 5)] + ['k_pred']\n",
    "    print(\"-----LGBM training-----\")\n",
    "\n",
    "    d_train = lgb.Dataset(train_part[FEATURE_LIST].values, train_part_label, weight = train_weight, \n",
    "            feature_name = FEATURE_LIST, categorical_feature = CATEGORY_FEATURES,)#, init_score = train_part[:, -1])\n",
    "    d_valide = lgb.Dataset(valide_part[FEATURE_LIST].values, valide_part_label, weight = valide_weight,\n",
    "            feature_name = FEATURE_LIST, categorical_feature = CATEGORY_FEATURES,)#, init_score = valide_part[:, -1])\n",
    "    params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt', #'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'num_leaves': 240, #60, #40, # 60,\n",
    "            'min_sum_hessian_in_leaf': 10,\n",
    "            'max_depth': 50,#12, #6, # 10,\n",
    "            'learning_rate': 0.025, # 0.025,\n",
    "           # 'feature_fraction': 0.5,#0.35, # 0.6\n",
    "            'verbose': 0,\n",
    "            'num_boost_round': 500, #361,\n",
    "            'feature_fraction_seed': fold_seed,\n",
    "            #'drop_rate': 0.05,\n",
    "            # 'bagging_fraction': 0.8,\n",
    "            # 'bagging_freq': 20,\n",
    "            # 'bagging_seed': fold_seed,\n",
    "             'early_stopping_round': 1500,\n",
    "            # 'random_state': 10\n",
    "            # 'verbose_eval': 20\n",
    "            #'min_data_in_leaf': 665\n",
    "        }\n",
    "    params.update(config.all_params)\n",
    "    print (\"lightgbm params: {0}\\n\".format(params))\n",
    "\n",
    "    bst = lgb.train(\n",
    "                    params ,\n",
    "                    d_train,\n",
    "                    verbose_eval = 50,\n",
    "                    valid_sets = [d_train, d_valide],\n",
    "                    # feature_name= keras_train.DENSE_FEATURE_LIST,\n",
    "                    #feval = gini_lgbm\n",
    "                    #num_boost_round = 1\n",
    "                    )\n",
    "    #pred = model_eval(bst, 'l', valide_part)\n",
    "    #print(pred[:10])\n",
    "    #print(valide_part_label[:10])\n",
    "    #print(valide_part[:10, -1])\n",
    "    # exit(0)\n",
    "    feature_imp = bst.feature_importance(importance_type = 'gain')\n",
    "    sort_ind = np.argsort(feature_imp)[::-1]\n",
    "    print (np.c_[np.array(FEATURE_LIST)[sort_ind], feature_imp[sort_ind]])\n",
    "    # print (np.array(keras_train.FEATURE_LIST)[np.argsort(feature_imp)])\n",
    "    # exit(0)\n",
    "    # cv_result = lgb.cv(params, d_train, nfold=fold) #, feval = gini_lgbm)\n",
    "    # pd.DataFrame(cv_result).to_csv('cv_result', index = False)\n",
    "    # exit(0)\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cand shape:  45\n",
      "32/45 [====================>.........] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "def sub(models, train_data, test_data, class_id_emb_attr, img_model, output_model_path):\n",
    "    train_id = train_data['class_id'].unique()\n",
    "    test_img_feature_map = extract_array_from_series(test_data['target'])\n",
    "    preds = multi_models_vote(models = models, eval_df = test_data, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[~class_id_emb_attr['class_id'].isin(train_id)], \\\n",
    "            img_feature_map = test_img_feature_map)\n",
    "    sub = pd.DataFrame(preds, index = test_data['img_id'])\n",
    "    time_label = time.strftime('%Y%m%d_%H%M%S')\n",
    "    tmp_model_dir = \"./model_sub/\"\n",
    "    if not os.path.isdir(tmp_model_dir):\n",
    "        os.makedirs(tmp_model_dir, exist_ok=True)\n",
    "    sub_name = tmp_model_dir + \"/submit_\"+ time_label + \".txt\"\n",
    "    sub.to_csv(sub_name, header = False, sep = '\\t')\n",
    "\n",
    "#     model_name = tmp_model_dir + \"imgmodel_\" + time_label + \".h5\"\n",
    "#     img_model[0].save(model_name)\n",
    "#     for i, model in enumerate(models):\n",
    "#         model_name = tmp_model_dir + \"zsmodel_\" + str(i) + time_label + \".h5\"\n",
    "#         model[0].save(model_name)\n",
    "\n",
    "    if not os.path.isdir(output_model_path):\n",
    "        os.makedirs(output_model_path, exist_ok=True)\n",
    "    for fileName in os.listdir(tmp_model_dir):\n",
    "        dst_file = os.path.join(output_model_path, fileName)\n",
    "        if os.path.exists(dst_file):\n",
    "            os.remove(dst_file)\n",
    "        shutil.move(os.path.join(tmp_model_dir, fileName), output_model_path)\n",
    "        \n",
    "cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(round2_class_id)]\n",
    "sub(models = zs_models, train_data = train_data, test_data = test_data, \n",
    "        class_id_emb_attr = cand_class_id_emb_attr, img_model = img_model, \n",
    "        output_model_path = '../submit/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model_dir = path + \"./model_dir/6_12_24_16_ini_64_grow_32_03535/\"\n",
    "# time_label = time.strftime('_%Y_%m_%d_%H_%M_%S', time.gmtime())\n",
    "# if not os.path.isdir(tmp_model_dir):\n",
    "#     os.makedirs(tmp_model_dir, exist_ok=True)\n",
    "# model_name = tmp_model_dir + \"model\" + time_label + \".h5\"\n",
    "# img_model.model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38221/38221 [==============================] - 2s 45us/step\n"
     ]
    }
   ],
   "source": [
    "pred_class_probas = img_classifi_model.predict(train_image_feature_map, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['preds'] = list(pred_class_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "891 * 80 / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n03598930', 'jigsaw_puzzle', 0.023861792), ('n02095314', 'wire-haired_fox_terrier', 0.022931756), ('n04589890', 'window_screen', 0.021361042)]\n"
     ]
    }
   ],
   "source": [
    "# np.asarray(list(train_data['img'])).shape\n",
    "# train_data[train_data['class_id'] == 'ZJL1']\n",
    "# train_data['class_id'].value_counts().hist()\n",
    "# train_data.head()\n",
    "# class_id_emb_attr.iloc[0].name\n",
    "# resnet50_model = ResNet50(weights='imagenet', input_shape=(224, 224, 3))\n",
    "img = image.load_img(path + '/DatasetA_train_20180813/train/000c0d617f5b67d116dee15c40d1d47d.jpeg', target_size=(224, 224))\n",
    "img = image.img_to_array(img)\n",
    "img = vgg16.preprocess_input(img)\n",
    "preds = vgg_model.predict(np.expand_dims(img, axis=0))\n",
    "print('Predicted:', vgg16.decode_predictions(preds, top=3)[0])\n",
    "# imread(path + '/DatasetA_train_20180813/train/000c0d617f5b67d116dee15c40d1d47d.jpeg')\n",
    "# image.img_to_array(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = setA_train_data\n",
    "\n",
    "img_model = DenseNet(blocks = [6, 12, 24, 16], \n",
    "                                cat_max = 205,\n",
    "                                weight_decay = 1e-4, \n",
    "                                kernel_initializer = 'glorot_normal',\n",
    "                                reduction = 0.5, \n",
    "                                init_filters = 128, \n",
    "                                growth_rate = 32).model\n",
    "img_model.load_weights(path + '/model_sub/6_12_24_16_ini64_growth32_inistride2_03621/model_0_2018_09_21_21_10_59.h5')\n",
    "img_model_flat = Model(input = img_model.input, output = img_model.get_layer(name = 'avg_pool').output)\n",
    "train_data['target'] = list(img_model_flat.predict(preprocess_img(train_data['img']), verbose = 1))\n",
    "# train_data['preds'] = list(img_model.predict(train_img, verbose = 1))\n",
    "# train_data['target'] = list(train_y) #\n",
    "# with open('../..//Data/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/model_0_2018_09_24_03_07_15.h5', 'rb') as handle:\n",
    "#     flat_train_re = pickle.load(handle)\n",
    "# train_data['target'] = list(flat_train_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0262023 , 1.0496653 , 0.42650044, ..., 0.09345146, 0.14242867,\n",
       "       0.        ], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_train_re[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2266190e+00, 1.6327184e-01, 2.8384233e-01, ..., 4.1941926e-04,\n",
       "       3.3683911e-02, 7.6473856e-01], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['target'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_part_img_id = pd.read_csv(path + '/model_sub/6_12_24_16_ini64_growth16_02962/train_part_img_id_0.csv', header = None)\n",
    "validate_part_img_id = pd.read_csv(path + '/model_sub/6_12_24_16_ini64_growth16_02962/validate_part_img_id_0.csv', header = None)\n",
    "train_part_img_id = train_part_img_id[0].values\n",
    "validate_part_img_id = validate_part_img_id[0].values\n",
    "\n",
    "train_part_df = train_data[train_data['img_id'].isin(train_part_img_id)]\n",
    "validate_part_df = train_data[train_data['img_id'].isin(validate_part_img_id)]\n",
    "\n",
    "seen_class = train_part_df.append(validate_part_df).class_id.unique()\n",
    "\n",
    "train_part_df = train_data[train_data['class_id'].isin(seen_class)]\n",
    "validate_part_df = train_data[~train_data['class_id'].isin(seen_class)]\n",
    "# unseen_class_df = train_data[~train_data['class_id'].isin(seen_class)]\n",
    "unseen_class = validate_part_df.class_id.unique()\n",
    "\n",
    "# validate_part_df = validate_part_df.append(unseen_class_df)\n",
    "# train_part_df = train_part_df.append(validate_part_df)\n",
    "# validate_part_df = unseen_class_df\n",
    "\n",
    "train_part_data = create_dnn_data(train_part_df)\n",
    "train_part_target = extract_array_from_series(train_part_df['target'])\n",
    "\n",
    "validate_part_data = create_dnn_data(validate_part_df)\n",
    "validate_part_target = extract_array_from_series(validate_part_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39184, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setB_train_data[setB_train_data.class_id.isin(seen_class)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3713906765013109"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 / 10.77032961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 6 4]\n",
      " [8 7 1]\n",
      " [2 2 5]\n",
      " [2 1 5]]\n",
      "[10.77032961 10.67707825  5.74456265  5.47722558]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.74278135, 0.55708601, 0.37139068],\n",
       "       [0.74926865, 0.65561007, 0.09365858],\n",
       "       [0.34815531, 0.34815531, 0.87038828],\n",
       "       [0.36514837, 0.18257419, 0.91287093]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(1, 10, (4,3))\n",
    "print (x)\n",
    "print (np.linalg.norm(x, axis = 1))\n",
    "sklearn.preprocessing.normalize(x, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "        \n",
    "def find_nearest_class(class_id_emb_attr, model, eval_df, cand_feature_map, img_feature_map, \n",
    "                       threshold = None, gamma = None, seen_class = None):\n",
    "#     cand_feature_map = model.predict(create_dnn_data(class_id_emb_attr))\n",
    "    # img_feature_map = np.repeat(img_feature_map, [cand_feature_map.shape[0]] * img_feature_map.shape[0], axis = 0)\n",
    "    nearest_class_id = ['ZJL'] * eval_df.shape[0]\n",
    "#     seen_class_bias = np.zeros(class_id_emb_attr.shape[0])\n",
    "#     seen_class_bias[class_id_emb_attr.class_id.isin(seen_class)] = gamma\n",
    "    \n",
    "#     seen_indice = [category_dict[c] for c in seen_class]\n",
    "#     preds = np.asarray(list(eval_df['preds']))[:, seen_indice]\n",
    "    for i in range(img_feature_map.shape[0]):\n",
    "        if False: #np.amax(preds[i]) > threshold:\n",
    "            nearest_class_id[i] = seen_class[np.argmax(preds[i])]\n",
    "        else:\n",
    "            img = img_feature_map[i]\n",
    "#             dis = 1 - sklearn.metrics.pairwise.cosine_similarity([img], \n",
    "#                                     cand_feature_map).reshape((cand_feature_map.shape[0]))\n",
    "            dis = np.linalg.norm(img - cand_feature_map, axis = 1)\n",
    "#             print (dis.shape)\n",
    "#             if np.amin(dis[class_id_emb_attr.class_id.isin(seen_class)]) > \\\n",
    "#                     gamma * np.amin(dis[~class_id_emb_attr.class_id.isin(seen_class)]):\n",
    "#                 dis[class_id_emb_attr.class_id.isin(seen_class)] = 200\n",
    "#             dis += seen_class_bias\n",
    "#             print(dis)\n",
    "#             plt.hist(dis)\n",
    "#             return\n",
    "            min_ind = np.where(dis == np.amin(dis))[0]\n",
    "            if len(min_ind) > 1:\n",
    "                print ('eval img id: ', eval_df.iloc[i]['class_id'], 'has multiple best candidates: ', len(min_ind), 'min val: ', np.amin(dis))\n",
    "    #         print (i, img, class_id_emb_attr.iloc[min_ind[0]].name)\n",
    "            nearest_class_id[i] = class_id_emb_attr.iloc[min_ind[0]]['class_id']\n",
    "#     print (nearest_class_id)\n",
    "    return np.asarray(nearest_class_id)\n",
    "        \n",
    "def calc_accuracy(eval_df, eval_class, preds):\n",
    "    eval_mask = eval_df.class_id.isin(eval_class)\n",
    "    eval_num = np.sum(eval_mask)\n",
    "    right_num = np.sum(preds[eval_mask] == eval_df.class_id[eval_mask])\n",
    "    return right_num / np.sum(eval_mask), right_num, eval_num\n",
    "    \n",
    "def calc_detailed_accuracy(eval_df, preds, seen_class, unseen_class):\n",
    "    all_re = calc_accuracy(eval_df, eval_df['class_id'].values, preds)\n",
    "    seen_re = calc_accuracy(eval_df, seen_class, preds)\n",
    "    unseen_re = calc_accuracy(eval_df, unseen_class, preds)\n",
    "    print(\"\\nAll_re: \\t%.6f\\t%.0f\\t%.0f\" % all_re)\n",
    "    print(\"Seen_re: \\t%.6f\\t%.0f\\t%.0f\" % seen_re)\n",
    "    print(\"Unseen_re: \\t%.6f\\t%.0f\\t%.0f\" % unseen_re)\n",
    "\n",
    "def model_eval(model, model_type, eval_df, cand_class_id_emb_attr, img_feature_map, seen_class = None, unseen_class = None):\n",
    "    if model_type == 'DEM':\n",
    "        zs_model = Model(inputs = model.inputs[:2], outputs = model.outputs[0])\n",
    "        cand_feature_map = zs_model.predict(create_dnn_data(cand_class_id_emb_attr))\n",
    "    elif model_type == 'GCN':\n",
    "        zs_model = Model(inputs = model.inputs[2:], outputs = model.outputs[0])\n",
    "        cand_class_to_id = [class_to_id[c] for c in cand_class_id_emb_attr.class_id.values]\n",
    "        cand_feature_map = zs_model.predict(None, steps = 1)[cand_class_to_id]\n",
    "    preds = find_nearest_class(cand_class_id_emb_attr, zs_model, eval_df, cand_feature_map, \n",
    "                               img_feature_map)\n",
    "    if 'class_id' in eval_df.columns:\n",
    "        calc_detailed_accuracy(eval_df, preds, seen_class, unseen_class)\n",
    "    return preds\n",
    "\n",
    "def models_eval(models, eval_df, cand_class_id_emb_attr, img_feature_map, \n",
    "                seen_class = None, unseen_class = None):\n",
    "    preds = []\n",
    "    for model, model_type in models:\n",
    "        pred = model_eval(model, model_type, eval_df, cand_class_id_emb_attr, img_feature_map, \n",
    "                          seen_class, unseen_class)\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "        \n",
    "def multi_models_vote(models, eval_df = None, cand_class_id_emb_attr = None, img_feature_map = None, \n",
    "                      seen_class = None, unseen_class = None):\n",
    "    preds = models_eval(models, eval_df, cand_class_id_emb_attr, img_feature_map, \n",
    "                seen_class, unseen_class)\n",
    "    preds = np.asarray(preds).T\n",
    "    print (preds)\n",
    "    vote_preds = []\n",
    "    for single_img_vote in preds:\n",
    "        uniq_val, counts = np.unique(single_img_vote, return_counts = True)\n",
    "        vote_preds.append(uniq_val[np.argmax(counts)])\n",
    "    vote_preds = np.asarray(vote_preds)\n",
    "    print (vote_preds)\n",
    "    if 'class_id' in eval_df.columns: \n",
    "        calc_detailed_accuracy(eval_df, vote_preds, seen_class, unseen_class)\n",
    "    return vote_preds\n",
    "    \n",
    "class AccuracyEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1, batch_interval = 1000000, verbose = 2, \\\n",
    "            scores = [], class_id_emb_attr = None, eval_df = None, threshold = None, \\\n",
    "                 seen_class = None, unseen_class = None, gamma = None, model_type = None):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        # print(\"y_val shape:{0}\".format(self.y_val.shape))\n",
    "        self.batch_interval = batch_interval\n",
    "        self.verbose = verbose\n",
    "        self.scores = scores\n",
    "        self.class_id_emb_attr = class_id_emb_attr\n",
    "        self.eval_df = eval_df\n",
    "        self.threshold = threshold\n",
    "        self.seen_class = seen_class\n",
    "        self.unseen_class = unseen_class\n",
    "        self.gamma = gamma\n",
    "        self.model_type = model_type\n",
    "#         seen_indice = [category_dict[c] for c in seen_class]\n",
    "#         self.preds = np.asarray(list(eval_df['preds']))[:, seen_indice]\n",
    "#         self.img_feature_map = self.y_val\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            model_eval(self.model, self.model_type, self.eval_df, self.class_id_emb_attr, \n",
    "                seen_class = self.seen_class, unseen_class = self.unseen_class, img_feature_map = self.y_val)\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "        if(self.verbose >= 2) and (batch % self.batch_interval == 0):\n",
    "            # y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            # loss = metrics.log_loss(self.y_val, y_pred)\n",
    "            print(\"Hi! on_batch_end() , batch=\",batch,\",logs:\",logs)\n",
    "            # print(\"Valide size=\",y_pred.shape[0], \"  Valide loss=\",loss)\n",
    "\n",
    "def full_connect_layer(input, hidden_dim, activation, resnet = False, adj_graphs = None, \n",
    "                       drop_out_ratio = None, kernel_initializer = 'he_normal'):\n",
    "    full_connect = input\n",
    "    for i, hn in enumerate(hidden_dim):\n",
    "        fc_in = full_connect\n",
    "        if drop_out_ratio is not None:\n",
    "            full_connect = Dropout(drop_out_ratio)(full_connect)\n",
    "        full_connect = BatchNormalization(epsilon=1.001e-5)(full_connect)\n",
    "        # full_connect = Dense(hn, kernel_regularizer = l2(0.001), activity_regularizer = l1(0.001))(full_connect)\n",
    "#         full_connect = Concatenate()([Dense(hn, kernel_initializer='lecun_uniform', activation = 'relu')(full_connect), \n",
    "#             Dense(hn, kernel_initializer='lecun_uniform', activation = 'sigmoid')(full_connect)])\n",
    "        full_connect = Dense(hn, kernel_initializer=kernel_initializer, kernel_regularizer = l2(1e-4), activation = activation)(full_connect)\n",
    "#         full_connect = LeakyReLU(alpha=0.02)(full_connect)\n",
    "        # full_connect = self.act_blend(full_connect)\n",
    "        # if self.full_connect_dropout > 0:\n",
    "#             full_connect = Dropout(self.full_connect_dropout)(full_connect) #Dropout(self.full_connect_dropout)(full_connect)\n",
    "        if adj_graphs is not None:\n",
    "            full_connect = Lambda(lambda x: K.dot(x[1], x[0]), \\\n",
    "                                  name = 'rela_' + str(i))([full_connect, adj_graphs])\n",
    "        if resnet:\n",
    "            full_connect = Concatenate()([fc_in, full_connect])\n",
    "    return full_connect\n",
    "\n",
    "def extract_array_from_series(s):\n",
    "    return np.asarray(list(s))\n",
    "\n",
    "def create_dnn_data(df):\n",
    "    # return [extract_array_from_series(df['attr']), extract_array_from_series(df['emb'])[:, :]]\n",
    "    return [extract_array_from_series(df['attr']), extract_array_from_series(df['emb'])[:, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD...\n"
     ]
    }
   ],
   "source": [
    "print ('SVD...')\n",
    "svd = decomposition.TruncatedSVD(n_components=10, n_iter=50, random_state=12)\n",
    "svd_features = svd.fit_transform(extract_array_from_series(class_id_emb_attr['attr']))\n",
    "class_id_emb_attr['svd_attr'] = list(svd_features)\n",
    "train_data = train_data.merge(class_id_emb_attr[['class_id', 'svd_attr']], how = 'left', on = 'class_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87249, 10)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen unseen Classes:  164 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_140\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_140\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attr (InputLayer)               (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "wv (InputLayer)                 (None, 600)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_137 (Dense)               (None, 300)          9000        attr[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 900)          0           wv[0][0]                         \n",
      "                                                                 dense_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 900)          0           concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 900)          3600        dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_139 (Dense)               (None, 1536)         1383936     batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 1536)         6144        dense_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_140 (Dense)               (None, 1024)         1573888     batch_normalization_92[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,976,568\n",
      "Trainable params: 2,971,696\n",
      "Non-trainable params: 4,872\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 70408 samples, validate on 16841 samples\n",
      "Epoch 1/50\n",
      "70408/70408 [==============================] - 104s 1ms/step - loss: 0.5655 - val_loss: 0.4114\n",
      "\n",
      "All_re: \t0.199216\t3355\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.199216\t3355\t16841\n",
      "Epoch 2/50\n",
      "  128/70408 [..............................] - ETA: 1:08 - loss: 0.3791"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70408/70408 [==============================] - 31s 436us/step - loss: 0.3235 - val_loss: 0.3294\n",
      "\n",
      "All_re: \t0.219999\t3705\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.219999\t3705\t16841\n",
      "Epoch 3/50\n",
      "70408/70408 [==============================] - 28s 394us/step - loss: 0.2883 - val_loss: 0.3257\n",
      "\n",
      "All_re: \t0.207945\t3502\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.207945\t3502\t16841\n",
      "Epoch 4/50\n",
      "70408/70408 [==============================] - 29s 418us/step - loss: 0.2808 - val_loss: 0.3216\n",
      "\n",
      "All_re: \t0.222315\t3744\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.222315\t3744\t16841\n",
      "Epoch 5/50\n",
      "70408/70408 [==============================] - 28s 402us/step - loss: 0.2787 - val_loss: 0.3213\n",
      "\n",
      "All_re: \t0.235972\t3974\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.235972\t3974\t16841\n",
      "Epoch 6/50\n",
      "70408/70408 [==============================] - 32s 449us/step - loss: 0.2776 - val_loss: 0.3168\n",
      "\n",
      "All_re: \t0.238525\t4017\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.238525\t4017\t16841\n",
      "Epoch 7/50\n",
      "70408/70408 [==============================] - 28s 399us/step - loss: 0.2766 - val_loss: 0.3047\n",
      "\n",
      "All_re: \t0.234547\t3950\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.234547\t3950\t16841\n",
      "Epoch 8/50\n",
      "70408/70408 [==============================] - 30s 432us/step - loss: 0.2744 - val_loss: 0.3047\n",
      "\n",
      "All_re: \t0.231993\t3907\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.231993\t3907\t16841\n",
      "Epoch 9/50\n",
      "70408/70408 [==============================] - 28s 396us/step - loss: 0.2723 - val_loss: 0.3039\n",
      "\n",
      "All_re: \t0.211626\t3564\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.211626\t3564\t16841\n",
      "Epoch 10/50\n",
      "70408/70408 [==============================] - 32s 451us/step - loss: 0.2705 - val_loss: 0.3030\n",
      "\n",
      "All_re: \t0.229321\t3862\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.229321\t3862\t16841\n",
      "Epoch 11/50\n",
      "70408/70408 [==============================] - 28s 402us/step - loss: 0.2700 - val_loss: 0.3027\n",
      "\n",
      "All_re: \t0.223621\t3766\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.223621\t3766\t16841\n",
      "Epoch 12/50\n",
      "70408/70408 [==============================] - 30s 420us/step - loss: 0.2694 - val_loss: 0.3019\n",
      "\n",
      "All_re: \t0.212695\t3582\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.212695\t3582\t16841\n",
      "Epoch 13/50\n",
      "70408/70408 [==============================] - 28s 394us/step - loss: 0.2692 - val_loss: 0.3031\n",
      "\n",
      "All_re: \t0.211033\t3554\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.211033\t3554\t16841\n",
      "Seen unseen Classes:  164 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_144\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_144\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71411 samples, validate on 15838 samples\n",
      "Epoch 1/50\n",
      "71411/71411 [==============================] - 106s 1ms/step - loss: 0.5631 - val_loss: 0.4024\n",
      "\n",
      "All_re: \t0.194090\t3074\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.194090\t3074\t15838\n",
      "Epoch 2/50\n",
      "71411/71411 [==============================] - 28s 398us/step - loss: 0.3217 - val_loss: 0.3223\n",
      "\n",
      "All_re: \t0.224081\t3549\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.224081\t3549\t15838\n",
      "Epoch 3/50\n",
      "71411/71411 [==============================] - 29s 402us/step - loss: 0.2867 - val_loss: 0.3117\n",
      "\n",
      "All_re: \t0.208675\t3305\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.208675\t3305\t15838\n",
      "Epoch 4/50\n",
      "71411/71411 [==============================] - 29s 401us/step - loss: 0.2788 - val_loss: 0.3085\n",
      "\n",
      "All_re: \t0.203687\t3226\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.203687\t3226\t15838\n",
      "Epoch 5/50\n",
      "71411/71411 [==============================] - 28s 393us/step - loss: 0.2757 - val_loss: 0.3052\n",
      "\n",
      "All_re: \t0.207791\t3291\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.207791\t3291\t15838\n",
      "Epoch 6/50\n",
      "71411/71411 [==============================] - 28s 395us/step - loss: 0.2735 - val_loss: 0.3035\n",
      "\n",
      "All_re: \t0.218146\t3455\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.218146\t3455\t15838\n",
      "Epoch 7/50\n",
      "71411/71411 [==============================] - 28s 396us/step - loss: 0.2720 - val_loss: 0.3026\n",
      "\n",
      "All_re: \t0.229385\t3633\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.229385\t3633\t15838\n",
      "Epoch 8/50\n",
      "71411/71411 [==============================] - 29s 400us/step - loss: 0.2711 - val_loss: 0.3006\n",
      "\n",
      "All_re: \t0.231216\t3662\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.231216\t3662\t15838\n",
      "Epoch 9/50\n",
      "71411/71411 [==============================] - 28s 396us/step - loss: 0.2706 - val_loss: 0.3014\n",
      "\n",
      "All_re: \t0.245107\t3882\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.245107\t3882\t15838\n",
      "Seen unseen Classes:  164 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_148\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_148\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69141 samples, validate on 18108 samples\n",
      "Epoch 1/50\n",
      "69141/69141 [==============================] - 107s 2ms/step - loss: 0.5679 - val_loss: 0.4168\n",
      "\n",
      "All_re: \t0.197095\t3569\t18108\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.197095\t3569\t18108\n",
      "Epoch 2/50\n",
      "69141/69141 [==============================] - 30s 439us/step - loss: 0.3233 - val_loss: 0.3322\n",
      "\n",
      "All_re: \t0.204385\t3701\t18108\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.204385\t3701\t18108\n",
      "Epoch 3/50\n",
      "69141/69141 [==============================] - 30s 438us/step - loss: 0.2866 - val_loss: 0.3208\n",
      "\n",
      "All_re: \t0.193837\t3510\t18108\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.193837\t3510\t18108\n",
      "Epoch 4/50\n",
      "69141/69141 [==============================] - 30s 436us/step - loss: 0.2784 - val_loss: 0.3116\n",
      "\n",
      "All_re: \t0.206980\t3748\t18108\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.206980\t3748\t18108\n",
      "Epoch 5/50\n",
      "69141/69141 [==============================] - 30s 433us/step - loss: 0.2749 - val_loss: 0.3123\n",
      "\n",
      "All_re: \t0.200961\t3639\t18108\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.200961\t3639\t18108\n",
      "Seen unseen Classes:  164 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_152\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_152\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69739 samples, validate on 17510 samples\n",
      "Epoch 1/50\n",
      "69739/69739 [==============================] - 108s 2ms/step - loss: 0.5657 - val_loss: 0.4218\n",
      "\n",
      "All_re: \t0.141005\t2469\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.141005\t2469\t17510\n",
      "Epoch 2/50\n",
      "69739/69739 [==============================] - 30s 434us/step - loss: 0.3225 - val_loss: 0.3367\n",
      "\n",
      "All_re: \t0.165677\t2901\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.165677\t2901\t17510\n",
      "Epoch 3/50\n",
      "69739/69739 [==============================] - 30s 430us/step - loss: 0.2867 - val_loss: 0.3231\n",
      "\n",
      "All_re: \t0.139406\t2441\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.139406\t2441\t17510\n",
      "Epoch 4/50\n",
      "69739/69739 [==============================] - 30s 425us/step - loss: 0.2796 - val_loss: 0.3168\n",
      "\n",
      "All_re: \t0.153113\t2681\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.153113\t2681\t17510\n",
      "Epoch 5/50\n",
      "69739/69739 [==============================] - 29s 419us/step - loss: 0.2770 - val_loss: 0.3163\n",
      "\n",
      "All_re: \t0.141576\t2479\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.141576\t2479\t17510\n",
      "Epoch 6/50\n",
      "69739/69739 [==============================] - 30s 430us/step - loss: 0.2748 - val_loss: 0.3158\n",
      "\n",
      "All_re: \t0.171845\t3009\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.171845\t3009\t17510\n",
      "Epoch 7/50\n",
      "69739/69739 [==============================] - 30s 431us/step - loss: 0.2740 - val_loss: 0.3157\n",
      "\n",
      "All_re: \t0.150885\t2642\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.150885\t2642\t17510\n",
      "Epoch 8/50\n",
      "69739/69739 [==============================] - 30s 426us/step - loss: 0.2720 - val_loss: 0.3145\n",
      "\n",
      "All_re: \t0.170017\t2977\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.170017\t2977\t17510\n",
      "Epoch 9/50\n",
      "69739/69739 [==============================] - 30s 425us/step - loss: 0.2700 - val_loss: 0.3129\n",
      "\n",
      "All_re: \t0.168247\t2946\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.168247\t2946\t17510\n",
      "Epoch 10/50\n",
      "69739/69739 [==============================] - 30s 433us/step - loss: 0.2690 - val_loss: 0.3129\n",
      "\n",
      "All_re: \t0.159794\t2798\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.159794\t2798\t17510\n",
      "Seen unseen Classes:  164 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_156\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_156\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68297 samples, validate on 18952 samples\n",
      "Epoch 1/50\n",
      "68297/68297 [==============================] - 97s 1ms/step - loss: 0.5718 - val_loss: 0.4250\n",
      "\n",
      "All_re: \t0.190903\t3618\t18952\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.190903\t3618\t18952\n",
      "Epoch 2/50\n",
      "68297/68297 [==============================] - 30s 441us/step - loss: 0.3240 - val_loss: 0.3366\n",
      "\n",
      "All_re: \t0.209529\t3971\t18952\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.209529\t3971\t18952\n",
      "Epoch 3/50\n",
      "68297/68297 [==============================] - 30s 433us/step - loss: 0.2860 - val_loss: 0.3216\n",
      "\n",
      "All_re: \t0.201192\t3813\t18952\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.201192\t3813\t18952\n",
      "Epoch 4/50\n",
      "68297/68297 [==============================] - 29s 432us/step - loss: 0.2777 - val_loss: 0.3179\n",
      "\n",
      "All_re: \t0.212854\t4034\t18952\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.212854\t4034\t18952\n",
      "Epoch 5/50\n",
      "68297/68297 [==============================] - 29s 430us/step - loss: 0.2744 - val_loss: 0.3180\n",
      "\n",
      "All_re: \t0.206205\t3908\t18952\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.206205\t3908\t18952\n"
     ]
    }
   ],
   "source": [
    "def create_dnn(kernel_initializer = 'he_normal', img_flat_len = 1024):\n",
    "    alpha = 0.03\n",
    "    img_flat_len = img_flat_len\n",
    "    attr_input = Input(shape = (30,), name = 'attr')\n",
    "    word_emb = Input(shape = (600,), name = 'wv')\n",
    "    imag_classifier = Input(shape = (img_flat_len,), name = 'img')\n",
    "\n",
    "    attr_dense = Dense(300, use_bias = False, kernel_initializer=kernel_initializer, \n",
    "                       kernel_regularizer = l2(1e-4))(attr_input)\n",
    "#     attr_dense = Dense(512)(attr_dense)\n",
    "#     attr_dense = LeakyReLU(alpha=alpha)(attr_dense)\n",
    "#     attr_dense = Dense(256)(attr_dense)\n",
    "#     attr_dense = LeakyReLU(alpha=alpha)(attr_dense)\n",
    "#     attr_dense = Dense(300, activation=\"relu\")(attr_dense)\n",
    "    # attr_dense = Dense(512, activation=\"relu\")(attr_dense)\n",
    "    word_emb_dense = Dense(300, use_bias = False, kernel_initializer=kernel_initializer, \n",
    "                           kernel_regularizer = l2(1e-4))(word_emb)\n",
    "\n",
    "    attr_word_emb = Concatenate()([word_emb, attr_dense])\n",
    "#     attr_word_emb = word_emb #Add()([word_emb_dense, attr_dense])\n",
    "    attr_word_emb_dense = full_connect_layer(attr_word_emb, hidden_dim = [int(img_flat_len * 1.5), \n",
    "#                                                                           int(img_flat_len * 1.25), \n",
    "#                                                                           int(img_flat_len * 1.125),\n",
    "#                                                                           int(img_flat_len * 0.5)\n",
    "                                                                         ], \\\n",
    "                                             activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "    attr_word_emb_dense = full_connect_layer(attr_word_emb_dense, hidden_dim = [img_flat_len], \n",
    "                                             activation = 'relu')\n",
    "\n",
    "    mse_loss = K.mean(keras.losses.mean_squared_error(imag_classifier, attr_word_emb_dense))\n",
    "    \n",
    "    model = Model([attr_input, word_emb, imag_classifier], outputs = attr_word_emb_dense) #, vgg_output])\n",
    "    model.add_loss(mse_loss)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=5e-4), loss=None)\n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state = 100)\n",
    "num_fold = 0\n",
    "zs_model_list = []\n",
    "classes = train_data.class_id.unique()\n",
    "\n",
    "for train_index, test_index in kf.split(classes):\n",
    "    seen_class = classes[train_index]\n",
    "    unseen_class = classes[test_index]\n",
    "    \n",
    "    train_part_df = train_data[train_data.class_id.isin(seen_class)]\n",
    "    validate_part_df = train_data[train_data.class_id.isin(unseen_class)]\n",
    "    \n",
    "    train_part_data = create_dnn_data(train_part_df)\n",
    "    validate_part_data = create_dnn_data(validate_part_df)\n",
    "    \n",
    "    train_part_target = extract_array_from_series(train_part_df['target']) #sklearn.preprocessing.normalize(extract_array_from_series(train_part_df['target']), norm='l2')\n",
    "    validate_part_target = extract_array_from_series(validate_part_df['target']) #sklearn.preprocessing.normalize(extract_array_from_series(validate_part_df['target']), norm='l2')\n",
    "\n",
    "#     print ('Train Validation Classes: ', train_part_df.class_id.unique().shape[0], \n",
    "#            validate_part_df.class_id.unique().shape[0])\n",
    "    print ('Seen unseen Classes: ', seen_class.shape[0], unseen_class.shape[0])\n",
    "\n",
    "    callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=1, verbose=0),\n",
    "            AccuracyEvaluation(validation_data=(validate_part_data,  validate_part_target), interval=1, \\\n",
    "    #                         class_id_emb_attr = class_id_emb_attr, \\\n",
    "                               eval_df = validate_part_df, threshold= 0.3, \\\n",
    "                              seen_class = seen_class, unseen_class = unseen_class, \\\n",
    "                class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \\\n",
    "                              gamma = 0.8, model_type = 'DEM')\n",
    "            ]\n",
    "#     for i in range(5):\n",
    "    zs_model = create_dnn(kernel_initializer = 'he_normal', img_flat_len = 1024)\n",
    "    if num_fold == 0:\n",
    "        print (zs_model.summary())\n",
    "    zs_model.fit(train_part_data + [train_part_target],  validation_data = (validate_part_data + [validate_part_target], None),\n",
    "                  epochs=50, batch_size = 128, shuffle=True, verbose = 1, callbacks=callbacks)\n",
    "    zs_model_list.append((zs_model, 'DEM'))\n",
    "    num_fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(zs_model_list):\n",
    "    model_name = path + '/model_sub/zs_model_' + str(i) + \"_05011.txt\"\n",
    "    zs_model_list[i][0].save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All_re: \t0.213811\t3570\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.213811\t3570\t16697\n",
      "\n",
      "All_re: \t0.227226\t3794\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.227226\t3794\t16697\n",
      "\n",
      "All_re: \t0.235911\t3939\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.235911\t3939\t16697\n",
      "\n",
      "All_re: \t0.216566\t3616\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.216566\t3616\t16697\n",
      "\n",
      "All_re: \t0.070492\t1177\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.070492\t1177\t16697\n",
      "[['ZJL200' 'ZJL200' 'ZJL200' 'ZJL19' 'ZJL101']\n",
      " ['ZJL4' 'ZJL100' 'ZJL41' 'ZJL4' 'ZJL101']\n",
      " ['ZJL192' 'ZJL192' 'ZJL192' 'ZJL192' 'ZJL101']\n",
      " ...\n",
      " ['ZJL261' 'ZJL102' 'ZJL254' 'ZJL254' 'ZJL261']\n",
      " ['ZJL254' 'ZJL254' 'ZJL261' 'ZJL254' 'ZJL137']\n",
      " ['ZJL261' 'ZJL261' 'ZJL261' 'ZJL254' 'ZJL261']]\n",
      "['ZJL200' 'ZJL4' 'ZJL192' ... 'ZJL254' 'ZJL254' 'ZJL261']\n",
      "\n",
      "All_re: \t0.234533\t3916\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.234533\t3916\t16697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['ZJL200', 'ZJL4', 'ZJL192', ..., 'ZJL254', 'ZJL254', 'ZJL261'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_models_vote(models = zs_model_list, eval_df = validate_part_df, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \\\n",
    "            img_feature_map = validate_part_target, seen_class = seen_class, unseen_class = unseen_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 5, 9],\n",
       "       [8, 7, 7],\n",
       "       [1, 1, 7],\n",
       "       [7, 1, 9]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(1, 10, (4, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 7 8 9] [3 1 4 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_val, counts = np.unique(x, return_counts = True)\n",
    "print (uniq_val, counts)\n",
    "# uniq_val[np.argmax(counts)]\n",
    "np.argmin(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = np.array(list(class_id_emb_attr['attr']))\n",
    "# adj_graph = scipy.eye(attr.shape[0]) #1 - sklearn.metrics.pairwise.pairwise_distances(attr, metric = 'cosine')\n",
    "adj_graph = 1 - sklearn.metrics.pairwise.pairwise_distances(\n",
    "    np.array(list(class_id_emb_attr['emb'])), metric = 'cosine')\n",
    "# th = 0.99999\n",
    "# adj_graph[adj_graph > th] = 1\n",
    "# adj_graph[adj_graph <= th] = 0\n",
    "# adj_graph = adj_graph / np.linalg.norm(adj_graph)\n",
    "# adj_graph = adj_graph[:, np.argsort(adj_graph)[:]]\n",
    "# adj_graph[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = class_id_emb_attr.class_id.values\n",
    "class_to_id = dict([(c, i) for i, c in enumerate(class_ids)])\n",
    "# class_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen unseen Classes:  164 41\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_32 (InputLayer)            (285, 30)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_33 (InputLayer)            (285, 300)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_84 (Dense)                 (285, 300)            9000        input_32[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)     (285, 600)            0           input_33[0][0]                   \n",
      "                                                                   dense_84[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNor (285, 600)            2400        concatenate_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_85 (Dense)                 (285, 1548)           930348      batch_normalization_58[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "input_34 (InputLayer)            (285, 285)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "rela_0 (Lambda)                  (285, 1548)           0           dense_85[0][0]                   \n",
      "                                                                   input_34[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNor (285, 1548)           6192        rela_0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_86 (Dense)                 (285, 1290)           1998210     batch_normalization_59[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "rela_1 (Lambda)                  (285, 1290)           0           dense_86[0][0]                   \n",
      "                                                                   input_34[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNor (285, 1290)           5160        rela_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_87 (Dense)                 (285, 1032)           1332312     batch_normalization_60[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "rela_2 (Lambda)                  (285, 1032)           0           dense_87[0][0]                   \n",
      "                                                                   input_34[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 4,283,622\n",
      "Trainable params: 4,276,746\n",
      "Non-trainable params: 6,876\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:32: UserWarning: Output \"rela_2\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"rela_2\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69913 samples, validate on 17336 samples\n",
      "Epoch 1/25\n",
      "69792/69913 [============================>.] - ETA: 0s - loss: 1.7330eval img id:  ZJL263 has multiple best candidates:  2 min val:  18.49614\n",
      "\n",
      "All_re: \t0.038417\t666\t17336\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.038417\t666\t17336\n",
      "69913/69913 [==============================] - 33s - loss: 1.7317 - val_loss: 1.0597\n",
      "Epoch 2/25\n",
      "  416/69913 [..............................] - ETA: 29s - loss: 0.9983"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69856/69913 [============================>.] - ETA: 0s - loss: 0.9736\n",
      "All_re: \t0.068066\t1180\t17336\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.068066\t1180\t17336\n",
      "69913/69913 [==============================] - 32s - loss: 0.9736 - val_loss: 1.0059\n",
      "Epoch 3/25\n",
      "69792/69913 [============================>.] - ETA: 0s - loss: 0.8904\n",
      "All_re: \t0.082603\t1432\t17336\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.082603\t1432\t17336\n",
      "69913/69913 [==============================] - 32s - loss: 0.8903 - val_loss: 0.9059\n",
      "Epoch 4/25\n",
      " 9536/69913 [===>..........................] - ETA: 24s - loss: 0.8300"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4eef8651ed11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     zs_model.fit([train_part_data, train_part_target],  \n\u001b[1;32m     74\u001b[0m                  \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidate_part_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_part_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                   epochs=25, batch_size = 32, shuffle=True, verbose = 1, callbacks=callbacks)\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mzs_model_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GCN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mnum_fold\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# adj_graph = 1 - sklearn.metrics.pairwise.pairwise_distances(\n",
    "#     np.array(list(class_id_emb_attr['emb']))[:, :300], metric = 'cosine')\n",
    "\n",
    "def create_gcn():\n",
    "    alpha = 0.03\n",
    "    img_flat_len = 1032\n",
    "    attr_input = Input(tensor=tf.constant(np.array(list(class_id_emb_attr['attr']), dtype = 'float32')))\n",
    "    all_word_emb = Input(tensor=tf.constant(np.array(list(class_id_emb_attr['emb']))[:, :300], dtype = 'float32')) #Input(shape = (230, 300,), name = 'wv')\n",
    "    class_index = Input(shape = (1, ), name = 'class_index', dtype = 'int32')\n",
    "    adj_graphs = Input(tensor=tf.constant(adj_graph, dtype = 'float32')) #Input(shape = (230, 230,), name = 'adj_graph')\n",
    "    imag_classifier = Input(shape = (img_flat_len,), name = 'img')\n",
    "    \n",
    "    attr_dense = Dense(300, use_bias = False, kernel_initializer='he_normal', \n",
    "                    kernel_regularizer = l2(1e-4))(attr_input)\n",
    "    attr_word_emb = Concatenate()([all_word_emb, attr_dense])\n",
    "#     x = Lambda(lambda xx: all_word_emb)(class_index)\n",
    "#     x = Dense(516, kernel_initializer='he_normal', kernel_regularizer = l2(1e-4), \n",
    "#               activation = 'relu', name = 'conv')(all_word_emb)\n",
    "#     all_classifier = Lambda(lambda x: K.dot(x[1], x[0]), name = 'rela')([x, adj_graphs])\n",
    "    all_classifier = full_connect_layer(attr_word_emb, hidden_dim = [int(img_flat_len * 1.5), \n",
    "                                                                    int(img_flat_len * 1.25 ),\n",
    "                                                                    img_flat_len], \n",
    "                                activation = 'relu', adj_graphs = adj_graphs)\n",
    "    x = tf.gather_nd(all_classifier, class_index)\n",
    "\n",
    "    mse_loss = K.mean(keras.losses.mean_squared_error(imag_classifier, x))\n",
    "    \n",
    "    model = Model([class_index, imag_classifier, attr_input, all_word_emb, adj_graphs], outputs = [all_classifier]) #, vgg_output])\n",
    "    model.add_loss(mse_loss)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=None)\n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state = 1)\n",
    "num_fold = 0\n",
    "zs_model_list = []\n",
    "classes = train_data.class_id.unique()\n",
    "class_ids = class_id_emb_attr.class_id.values\n",
    "class_to_id = dict([(c, i) for i, c in enumerate(class_ids)])\n",
    "\n",
    "for train_index, test_index in kf.split(classes):\n",
    "#     print (train_index)\n",
    "    seen_class = classes[train_index]\n",
    "    unseen_class = classes[test_index]\n",
    "    \n",
    "    train_part_df = train_data[train_data.class_id.isin(seen_class)]\n",
    "    validate_part_df = train_data[train_data.class_id.isin(unseen_class)]\n",
    "    \n",
    "    train_part_data = np.array([class_to_id[c] for c in train_part_df['class_id'].values]).astype('int32')\n",
    "    validate_part_data = np.array([class_to_id[c] for c in validate_part_df['class_id'].values]).astype('int32')\n",
    "\n",
    "    train_part_target = extract_array_from_series(train_part_df['target'])\n",
    "    validate_part_target = extract_array_from_series(validate_part_df['target'])\n",
    "\n",
    "#     print ('Train Validation Classes: ', train_part_df.class_id.unique().shape[0], \n",
    "#            validate_part_df.class_id.unique().shape[0])\n",
    "    print ('Seen unseen Classes: ', seen_class.shape[0], unseen_class.shape[0])\n",
    "\n",
    "    callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=50, verbose=0),\n",
    "            AccuracyEvaluation(validation_data=(validate_part_data,  validate_part_target), interval=1, \\\n",
    "    #                         class_id_emb_attr = class_id_emb_attr, \\\n",
    "                               eval_df = validate_part_df, threshold= 0.3, \\\n",
    "                              seen_class = seen_class, unseen_class = unseen_class, \\\n",
    "                class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \\\n",
    "                              gamma = 0.8, model_type = 'GCN')\n",
    "            ]\n",
    "    zs_model = create_gcn()\n",
    "    if num_fold == 0:\n",
    "        print (zs_model.summary())\n",
    "    zs_model.fit([train_part_data, train_part_target],  \n",
    "                 validation_data = ([validate_part_data, validate_part_target], None),\n",
    "                  epochs=25, batch_size = 32, shuffle=True, verbose = 1, callbacks=callbacks)\n",
    "    zs_model_list.append((zs_model, 'GCN'))\n",
    "    num_fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All_re: \t0.156834\t2140\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.156834\t2140\t13645\n",
      "\n",
      "All_re: \t0.155148\t2117\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.155148\t2117\t13645\n",
      "\n",
      "All_re: \t0.151118\t2062\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.151118\t2062\t13645\n",
      "\n",
      "All_re: \t0.153316\t2092\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.153316\t2092\t13645\n",
      "\n",
      "All_re: \t0.157640\t2151\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.157640\t2151\t13645\n",
      "[['ZJL264' 'ZJL264' 'ZJL264' 'ZJL264' 'ZJL264']\n",
      " ['ZJL102' 'ZJL102' 'ZJL102' 'ZJL102' 'ZJL102']\n",
      " ['ZJL254' 'ZJL276' 'ZJL254' 'ZJL254' 'ZJL254']\n",
      " ...\n",
      " ['ZJL254' 'ZJL254' 'ZJL276' 'ZJL254' 'ZJL254']\n",
      " ['ZJL276' 'ZJL276' 'ZJL276' 'ZJL276' 'ZJL254']\n",
      " ['ZJL168' 'ZJL125' 'ZJL50' 'ZJL168' 'ZJL254']]\n",
      "['ZJL264' 'ZJL102' 'ZJL254' ... 'ZJL254' 'ZJL276' 'ZJL168']\n",
      "\n",
      "All_re: \t0.166215\t2268\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.166215\t2268\t13645\n"
     ]
    }
   ],
   "source": [
    "multi_models_vote(models = zs_model_list, eval_df = validate_part_df, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \\\n",
    "            img_feature_map = validate_part_target, seen_class = seen_class, unseen_class = unseen_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'model_sub/train_data_2018_09_23_10_32_21.pickle', 'rb') as handle:\n",
    "    test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>img_id</th>\n",
       "      <th>img</th>\n",
       "      <th>target</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>a6394b0f513290f4651cc46792e5ac86.jpeg</td>\n",
       "      <td>[[[20.0, 33.0, 7.0], [19.0, 32.0, 6.0], [22.0,...</td>\n",
       "      <td>[2.2266192, 0.16327204, 0.2838421, 0.2219766, ...</td>\n",
       "      <td>[0.9520665, 4.647786e-09, 7.417136e-08, 3.9506...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>2fb89ef2ace869d3eb3bdd3afe184e1c.jpeg</td>\n",
       "      <td>[[[140.0, 45.0, 51.0], [144.0, 47.0, 54.0], [1...</td>\n",
       "      <td>[0.17702743, 0.55263615, 0.0, 0.030876435, 1.2...</td>\n",
       "      <td>[1.30949e-05, 5.735788e-10, 6.5571693e-12, 5.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>eda9f3bef2bd8da038f6acbc8355fc25.jpeg</td>\n",
       "      <td>[[[81.0, 69.0, 21.0], [86.0, 74.0, 26.0], [85....</td>\n",
       "      <td>[0.0, 0.42584094, 0.0, 0.034428038, 0.527664, ...</td>\n",
       "      <td>[0.9993531, 4.2289447e-09, 1.2198088e-07, 3.68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>7d93ef45972154aae150b4f9980a79c0.jpeg</td>\n",
       "      <td>[[[16.0, 14.0, 15.0], [18.0, 16.0, 17.0], [19....</td>\n",
       "      <td>[0.6332717, 0.23473893, 0.0, 0.779357, 1.38354...</td>\n",
       "      <td>[0.9999671, 4.578459e-10, 6.6002594e-12, 8.168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>fb901b4f9a8e396c1d0155bccc5e5671.jpeg</td>\n",
       "      <td>[[[120.0, 124.0, 127.0], [89.0, 93.0, 96.0], [...</td>\n",
       "      <td>[0.37087774, 1.1033719, 0.0, 0.23497638, 3.003...</td>\n",
       "      <td>[0.86532485, 3.0740804e-08, 3.727919e-08, 1.21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_id                                 img_id  \\\n",
       "0     ZJL1  a6394b0f513290f4651cc46792e5ac86.jpeg   \n",
       "1     ZJL1  2fb89ef2ace869d3eb3bdd3afe184e1c.jpeg   \n",
       "2     ZJL1  eda9f3bef2bd8da038f6acbc8355fc25.jpeg   \n",
       "3     ZJL1  7d93ef45972154aae150b4f9980a79c0.jpeg   \n",
       "4     ZJL1  fb901b4f9a8e396c1d0155bccc5e5671.jpeg   \n",
       "\n",
       "                                                 img  \\\n",
       "0  [[[20.0, 33.0, 7.0], [19.0, 32.0, 6.0], [22.0,...   \n",
       "1  [[[140.0, 45.0, 51.0], [144.0, 47.0, 54.0], [1...   \n",
       "2  [[[81.0, 69.0, 21.0], [86.0, 74.0, 26.0], [85....   \n",
       "3  [[[16.0, 14.0, 15.0], [18.0, 16.0, 17.0], [19....   \n",
       "4  [[[120.0, 124.0, 127.0], [89.0, 93.0, 96.0], [...   \n",
       "\n",
       "                                              target  \\\n",
       "0  [2.2266192, 0.16327204, 0.2838421, 0.2219766, ...   \n",
       "1  [0.17702743, 0.55263615, 0.0, 0.030876435, 1.2...   \n",
       "2  [0.0, 0.42584094, 0.0, 0.034428038, 0.527664, ...   \n",
       "3  [0.6332717, 0.23473893, 0.0, 0.779357, 1.38354...   \n",
       "4  [0.37087774, 1.1033719, 0.0, 0.23497638, 3.003...   \n",
       "\n",
       "                                               preds  \n",
       "0  [0.9520665, 4.647786e-09, 7.417136e-08, 3.9506...  \n",
       "1  [1.30949e-05, 5.735788e-10, 6.5571693e-12, 5.9...  \n",
       "2  [0.9993531, 4.2289447e-09, 1.2198088e-07, 3.68...  \n",
       "3  [0.9999671, 4.578459e-10, 6.6002594e-12, 8.168...  \n",
       "4  [0.86532485, 3.0740804e-08, 3.727919e-08, 1.21...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(path + '/DatasetA_test_20180813/DatasetA_test/image.txt', header = None, names = ['img_id'])\n",
    "imag_path = path + '/DatasetA_test_20180813/DatasetA_test/test/'\n",
    "test_data['img'] = test_data['img_id'].apply(lambda id: read_image(imag_path, id))\n",
    "# with open(path + 'test_data.pickle', 'rb') as handle:\n",
    "#     test_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.12423625, 0.1190791, 0.7566846]\n",
       "1       [1.7260123e-13, 2.1865514e-08, 1.0]\n",
       "2       [0.2117803, 0.17989996, 0.60831976]\n",
       "3        [0.2420589, 0.17926142, 0.5786797]\n",
       "4    [0.041276723, 0.95284086, 0.005882429]\n",
       "dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(path + 'test_data.pickle', 'wb+') as handle:\n",
    "#     pickle.dump(test_data, handle)\n",
    "# test_data.head()\n",
    "with open(path + '/model_sub/stacking_train_label_2018_09_12_12_10_06.pickle', 'rb') as handle:\n",
    "    stacking_train_data = pickle.load(handle)\n",
    "stacking_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_17 (InputLayer)            (None, 64, 64, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D) (None, 70, 70, 3)     0           input_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)              (None, 68, 68, 64)    1728        zero_padding2d_3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)    (None, 68, 68, 64)    256         conv1/conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1/relu (Activation)          (None, 68, 68, 64)    0           conv1/bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D) (None, 70, 70, 64)    0           conv1/relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (AveragePooling2D)         (None, 34, 34, 64)    0           zero_padding2d_4[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormaliz (None, 34, 34, 64)    256         pool1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation) (None, 34, 34, 64)    0           conv2_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)     (None, 34, 34, 64)    4096        conv2_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenate (None, 34, 34, 80)    0           pool1[0][0]                      \n",
      "                                                                   conv2_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormaliz (None, 34, 34, 80)    320         conv2_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation) (None, 34, 34, 80)    0           conv2_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)     (None, 34, 34, 64)    5120        conv2_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenate (None, 34, 34, 96)    0           conv2_block1_concat[0][0]        \n",
      "                                                                   conv2_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormaliz (None, 34, 34, 96)    384         conv2_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation) (None, 34, 34, 96)    0           conv2_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)     (None, 34, 34, 64)    6144        conv2_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenate (None, 34, 34, 112)   0           conv2_block2_concat[0][0]        \n",
      "                                                                   conv2_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormaliz (None, 34, 34, 112)   448         conv2_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation) (None, 34, 34, 112)   0           conv2_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)     (None, 34, 34, 64)    7168        conv2_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenate (None, 34, 34, 128)   0           conv2_block3_concat[0][0]        \n",
      "                                                                   conv2_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormaliz (None, 34, 34, 128)   512         conv2_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation) (None, 34, 34, 128)   0           conv2_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)     (None, 34, 34, 64)    8192        conv2_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenate (None, 34, 34, 144)   0           conv2_block4_concat[0][0]        \n",
      "                                                                   conv2_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormaliz (None, 34, 34, 144)   576         conv2_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation) (None, 34, 34, 144)   0           conv2_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)     (None, 34, 34, 64)    9216        conv2_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenate (None, 34, 34, 160)   0           conv2_block5_concat[0][0]        \n",
      "                                                                   conv2_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)    (None, 34, 34, 160)   640         conv2_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "pool2_relu (Activation)          (None, 34, 34, 160)   0           pool2_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)              (None, 34, 34, 80)    12800       pool2_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)    (None, 17, 17, 80)    0           pool2_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormaliz (None, 17, 17, 80)    320         pool2_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation) (None, 17, 17, 80)    0           conv3_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)     (None, 17, 17, 64)    5120        conv3_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenate (None, 17, 17, 96)    0           pool2_pool[0][0]                 \n",
      "                                                                   conv3_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormaliz (None, 17, 17, 96)    384         conv3_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation) (None, 17, 17, 96)    0           conv3_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)     (None, 17, 17, 64)    6144        conv3_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenate (None, 17, 17, 112)   0           conv3_block1_concat[0][0]        \n",
      "                                                                   conv3_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormaliz (None, 17, 17, 112)   448         conv3_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation) (None, 17, 17, 112)   0           conv3_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)     (None, 17, 17, 64)    7168        conv3_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenate (None, 17, 17, 128)   0           conv3_block2_concat[0][0]        \n",
      "                                                                   conv3_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormaliz (None, 17, 17, 128)   512         conv3_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation) (None, 17, 17, 128)   0           conv3_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)     (None, 17, 17, 64)    8192        conv3_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenate (None, 17, 17, 144)   0           conv3_block3_concat[0][0]        \n",
      "                                                                   conv3_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormaliz (None, 17, 17, 144)   576         conv3_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation) (None, 17, 17, 144)   0           conv3_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)     (None, 17, 17, 64)    9216        conv3_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenate (None, 17, 17, 160)   0           conv3_block4_concat[0][0]        \n",
      "                                                                   conv3_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormaliz (None, 17, 17, 160)   640         conv3_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation) (None, 17, 17, 160)   0           conv3_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)     (None, 17, 17, 64)    10240       conv3_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenate (None, 17, 17, 176)   0           conv3_block5_concat[0][0]        \n",
      "                                                                   conv3_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormaliz (None, 17, 17, 176)   704         conv3_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation) (None, 17, 17, 176)   0           conv3_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)     (None, 17, 17, 64)    11264       conv3_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenate (None, 17, 17, 192)   0           conv3_block6_concat[0][0]        \n",
      "                                                                   conv3_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormaliz (None, 17, 17, 192)   768         conv3_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation) (None, 17, 17, 192)   0           conv3_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)     (None, 17, 17, 64)    12288       conv3_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenate (None, 17, 17, 208)   0           conv3_block7_concat[0][0]        \n",
      "                                                                   conv3_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormaliz (None, 17, 17, 208)   832         conv3_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation) (None, 17, 17, 208)   0           conv3_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)     (None, 17, 17, 64)    13312       conv3_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenate (None, 17, 17, 224)   0           conv3_block8_concat[0][0]        \n",
      "                                                                   conv3_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormali (None, 17, 17, 224)   896         conv3_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activation (None, 17, 17, 224)   0           conv3_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)    (None, 17, 17, 64)    14336       conv3_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activation (None, 17, 17, 64)    0           conv3_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatenat (None, 17, 17, 240)   0           conv3_block9_concat[0][0]        \n",
      "                                                                   conv3_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormali (None, 17, 17, 240)   960         conv3_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activation (None, 17, 17, 240)   0           conv3_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)    (None, 17, 17, 64)    15360       conv3_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activation (None, 17, 17, 64)    0           conv3_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatenat (None, 17, 17, 256)   0           conv3_block10_concat[0][0]       \n",
      "                                                                   conv3_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormali (None, 17, 17, 256)   1024        conv3_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activation (None, 17, 17, 256)   0           conv3_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)    (None, 17, 17, 64)    16384       conv3_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activation (None, 17, 17, 64)    0           conv3_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatenat (None, 17, 17, 272)   0           conv3_block11_concat[0][0]       \n",
      "                                                                   conv3_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)    (None, 17, 17, 272)   1088        conv3_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool3_relu (Activation)          (None, 17, 17, 272)   0           pool3_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)              (None, 17, 17, 136)   36992       pool3_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)    (None, 8, 8, 136)     0           pool3_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormaliz (None, 8, 8, 136)     544         pool3_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation) (None, 8, 8, 136)     0           conv4_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)     (None, 8, 8, 64)      8704        conv4_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenate (None, 8, 8, 152)     0           pool3_pool[0][0]                 \n",
      "                                                                   conv4_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormaliz (None, 8, 8, 152)     608         conv4_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation) (None, 8, 8, 152)     0           conv4_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)     (None, 8, 8, 64)      9728        conv4_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenate (None, 8, 8, 168)     0           conv4_block1_concat[0][0]        \n",
      "                                                                   conv4_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormaliz (None, 8, 8, 168)     672         conv4_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation) (None, 8, 8, 168)     0           conv4_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)     (None, 8, 8, 64)      10752       conv4_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenate (None, 8, 8, 184)     0           conv4_block2_concat[0][0]        \n",
      "                                                                   conv4_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormaliz (None, 8, 8, 184)     736         conv4_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation) (None, 8, 8, 184)     0           conv4_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)     (None, 8, 8, 64)      11776       conv4_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenate (None, 8, 8, 200)     0           conv4_block3_concat[0][0]        \n",
      "                                                                   conv4_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormaliz (None, 8, 8, 200)     800         conv4_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation) (None, 8, 8, 200)     0           conv4_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)     (None, 8, 8, 64)      12800       conv4_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenate (None, 8, 8, 216)     0           conv4_block4_concat[0][0]        \n",
      "                                                                   conv4_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormaliz (None, 8, 8, 216)     864         conv4_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation) (None, 8, 8, 216)     0           conv4_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)     (None, 8, 8, 64)      13824       conv4_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenate (None, 8, 8, 232)     0           conv4_block5_concat[0][0]        \n",
      "                                                                   conv4_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormaliz (None, 8, 8, 232)     928         conv4_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation) (None, 8, 8, 232)     0           conv4_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)     (None, 8, 8, 64)      14848       conv4_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenate (None, 8, 8, 248)     0           conv4_block6_concat[0][0]        \n",
      "                                                                   conv4_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormaliz (None, 8, 8, 248)     992         conv4_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation) (None, 8, 8, 248)     0           conv4_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)     (None, 8, 8, 64)      15872       conv4_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenate (None, 8, 8, 264)     0           conv4_block7_concat[0][0]        \n",
      "                                                                   conv4_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormaliz (None, 8, 8, 264)     1056        conv4_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation) (None, 8, 8, 264)     0           conv4_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)     (None, 8, 8, 64)      16896       conv4_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenate (None, 8, 8, 280)     0           conv4_block8_concat[0][0]        \n",
      "                                                                   conv4_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormali (None, 8, 8, 280)     1120        conv4_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activation (None, 8, 8, 280)     0           conv4_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)    (None, 8, 8, 64)      17920       conv4_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activation (None, 8, 8, 64)      0           conv4_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatenat (None, 8, 8, 296)     0           conv4_block9_concat[0][0]        \n",
      "                                                                   conv4_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormali (None, 8, 8, 296)     1184        conv4_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activation (None, 8, 8, 296)     0           conv4_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)    (None, 8, 8, 64)      18944       conv4_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activation (None, 8, 8, 64)      0           conv4_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatenat (None, 8, 8, 312)     0           conv4_block10_concat[0][0]       \n",
      "                                                                   conv4_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormali (None, 8, 8, 312)     1248        conv4_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activation (None, 8, 8, 312)     0           conv4_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)    (None, 8, 8, 64)      19968       conv4_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activation (None, 8, 8, 64)      0           conv4_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatenat (None, 8, 8, 328)     0           conv4_block11_concat[0][0]       \n",
      "                                                                   conv4_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormali (None, 8, 8, 328)     1312        conv4_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activation (None, 8, 8, 328)     0           conv4_block13_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)    (None, 8, 8, 64)      20992       conv4_block13_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block13_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activation (None, 8, 8, 64)      0           conv4_block13_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block13_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatenat (None, 8, 8, 344)     0           conv4_block12_concat[0][0]       \n",
      "                                                                   conv4_block13_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormali (None, 8, 8, 344)     1376        conv4_block13_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activation (None, 8, 8, 344)     0           conv4_block14_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)    (None, 8, 8, 64)      22016       conv4_block14_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block14_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activation (None, 8, 8, 64)      0           conv4_block14_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block14_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatenat (None, 8, 8, 360)     0           conv4_block13_concat[0][0]       \n",
      "                                                                   conv4_block14_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormali (None, 8, 8, 360)     1440        conv4_block14_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activation (None, 8, 8, 360)     0           conv4_block15_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)    (None, 8, 8, 64)      23040       conv4_block15_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block15_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activation (None, 8, 8, 64)      0           conv4_block15_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block15_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatenat (None, 8, 8, 376)     0           conv4_block14_concat[0][0]       \n",
      "                                                                   conv4_block15_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormali (None, 8, 8, 376)     1504        conv4_block15_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activation (None, 8, 8, 376)     0           conv4_block16_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)    (None, 8, 8, 64)      24064       conv4_block16_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block16_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activation (None, 8, 8, 64)      0           conv4_block16_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block16_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatenat (None, 8, 8, 392)     0           conv4_block15_concat[0][0]       \n",
      "                                                                   conv4_block16_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormali (None, 8, 8, 392)     1568        conv4_block16_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activation (None, 8, 8, 392)     0           conv4_block17_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)    (None, 8, 8, 64)      25088       conv4_block17_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block17_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activation (None, 8, 8, 64)      0           conv4_block17_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block17_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatenat (None, 8, 8, 408)     0           conv4_block16_concat[0][0]       \n",
      "                                                                   conv4_block17_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormali (None, 8, 8, 408)     1632        conv4_block17_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activation (None, 8, 8, 408)     0           conv4_block18_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)    (None, 8, 8, 64)      26112       conv4_block18_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block18_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activation (None, 8, 8, 64)      0           conv4_block18_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block18_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatenat (None, 8, 8, 424)     0           conv4_block17_concat[0][0]       \n",
      "                                                                   conv4_block18_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormali (None, 8, 8, 424)     1696        conv4_block18_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activation (None, 8, 8, 424)     0           conv4_block19_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)    (None, 8, 8, 64)      27136       conv4_block19_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block19_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activation (None, 8, 8, 64)      0           conv4_block19_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block19_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatenat (None, 8, 8, 440)     0           conv4_block18_concat[0][0]       \n",
      "                                                                   conv4_block19_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormali (None, 8, 8, 440)     1760        conv4_block19_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activation (None, 8, 8, 440)     0           conv4_block20_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)    (None, 8, 8, 64)      28160       conv4_block20_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block20_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activation (None, 8, 8, 64)      0           conv4_block20_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block20_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatenat (None, 8, 8, 456)     0           conv4_block19_concat[0][0]       \n",
      "                                                                   conv4_block20_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormali (None, 8, 8, 456)     1824        conv4_block20_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activation (None, 8, 8, 456)     0           conv4_block21_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)    (None, 8, 8, 64)      29184       conv4_block21_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block21_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activation (None, 8, 8, 64)      0           conv4_block21_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block21_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatenat (None, 8, 8, 472)     0           conv4_block20_concat[0][0]       \n",
      "                                                                   conv4_block21_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormali (None, 8, 8, 472)     1888        conv4_block21_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activation (None, 8, 8, 472)     0           conv4_block22_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)    (None, 8, 8, 64)      30208       conv4_block22_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block22_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activation (None, 8, 8, 64)      0           conv4_block22_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block22_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatenat (None, 8, 8, 488)     0           conv4_block21_concat[0][0]       \n",
      "                                                                   conv4_block22_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormali (None, 8, 8, 488)     1952        conv4_block22_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activation (None, 8, 8, 488)     0           conv4_block23_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)    (None, 8, 8, 64)      31232       conv4_block23_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block23_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activation (None, 8, 8, 64)      0           conv4_block23_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block23_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatenat (None, 8, 8, 504)     0           conv4_block22_concat[0][0]       \n",
      "                                                                   conv4_block23_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormali (None, 8, 8, 504)     2016        conv4_block23_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activation (None, 8, 8, 504)     0           conv4_block24_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)    (None, 8, 8, 64)      32256       conv4_block24_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block24_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activation (None, 8, 8, 64)      0           conv4_block24_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block24_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatenat (None, 8, 8, 520)     0           conv4_block23_concat[0][0]       \n",
      "                                                                   conv4_block24_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)    (None, 8, 8, 520)     2080        conv4_block24_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool4_relu (Activation)          (None, 8, 8, 520)     0           pool4_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)              (None, 8, 8, 260)     135200      pool4_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)    (None, 4, 4, 260)     0           pool4_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormaliz (None, 4, 4, 260)     1040        pool4_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation) (None, 4, 4, 260)     0           conv5_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)     (None, 4, 4, 64)      16640       conv5_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenate (None, 4, 4, 276)     0           pool4_pool[0][0]                 \n",
      "                                                                   conv5_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormaliz (None, 4, 4, 276)     1104        conv5_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation) (None, 4, 4, 276)     0           conv5_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)     (None, 4, 4, 64)      17664       conv5_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenate (None, 4, 4, 292)     0           conv5_block1_concat[0][0]        \n",
      "                                                                   conv5_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormaliz (None, 4, 4, 292)     1168        conv5_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation) (None, 4, 4, 292)     0           conv5_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)     (None, 4, 4, 64)      18688       conv5_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenate (None, 4, 4, 308)     0           conv5_block2_concat[0][0]        \n",
      "                                                                   conv5_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormaliz (None, 4, 4, 308)     1232        conv5_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation) (None, 4, 4, 308)     0           conv5_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)     (None, 4, 4, 64)      19712       conv5_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenate (None, 4, 4, 324)     0           conv5_block3_concat[0][0]        \n",
      "                                                                   conv5_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormaliz (None, 4, 4, 324)     1296        conv5_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation) (None, 4, 4, 324)     0           conv5_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)     (None, 4, 4, 64)      20736       conv5_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenate (None, 4, 4, 340)     0           conv5_block4_concat[0][0]        \n",
      "                                                                   conv5_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormaliz (None, 4, 4, 340)     1360        conv5_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation) (None, 4, 4, 340)     0           conv5_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)     (None, 4, 4, 64)      21760       conv5_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenate (None, 4, 4, 356)     0           conv5_block5_concat[0][0]        \n",
      "                                                                   conv5_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormaliz (None, 4, 4, 356)     1424        conv5_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation) (None, 4, 4, 356)     0           conv5_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)     (None, 4, 4, 64)      22784       conv5_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenate (None, 4, 4, 372)     0           conv5_block6_concat[0][0]        \n",
      "                                                                   conv5_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormaliz (None, 4, 4, 372)     1488        conv5_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation) (None, 4, 4, 372)     0           conv5_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)     (None, 4, 4, 64)      23808       conv5_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenate (None, 4, 4, 388)     0           conv5_block7_concat[0][0]        \n",
      "                                                                   conv5_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormaliz (None, 4, 4, 388)     1552        conv5_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation) (None, 4, 4, 388)     0           conv5_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)     (None, 4, 4, 64)      24832       conv5_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenate (None, 4, 4, 404)     0           conv5_block8_concat[0][0]        \n",
      "                                                                   conv5_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormali (None, 4, 4, 404)     1616        conv5_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activation (None, 4, 4, 404)     0           conv5_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)    (None, 4, 4, 64)      25856       conv5_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activation (None, 4, 4, 64)      0           conv5_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatenat (None, 4, 4, 420)     0           conv5_block9_concat[0][0]        \n",
      "                                                                   conv5_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormali (None, 4, 4, 420)     1680        conv5_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activation (None, 4, 4, 420)     0           conv5_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)    (None, 4, 4, 64)      26880       conv5_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activation (None, 4, 4, 64)      0           conv5_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatenat (None, 4, 4, 436)     0           conv5_block10_concat[0][0]       \n",
      "                                                                   conv5_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormali (None, 4, 4, 436)     1744        conv5_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activation (None, 4, 4, 436)     0           conv5_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)    (None, 4, 4, 64)      27904       conv5_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activation (None, 4, 4, 64)      0           conv5_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatenat (None, 4, 4, 452)     0           conv5_block11_concat[0][0]       \n",
      "                                                                   conv5_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormali (None, 4, 4, 452)     1808        conv5_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activation (None, 4, 4, 452)     0           conv5_block13_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)    (None, 4, 4, 64)      28928       conv5_block13_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block13_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activation (None, 4, 4, 64)      0           conv5_block13_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block13_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatenat (None, 4, 4, 468)     0           conv5_block12_concat[0][0]       \n",
      "                                                                   conv5_block13_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormali (None, 4, 4, 468)     1872        conv5_block13_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activation (None, 4, 4, 468)     0           conv5_block14_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)    (None, 4, 4, 64)      29952       conv5_block14_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block14_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activation (None, 4, 4, 64)      0           conv5_block14_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block14_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatenat (None, 4, 4, 484)     0           conv5_block13_concat[0][0]       \n",
      "                                                                   conv5_block14_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormali (None, 4, 4, 484)     1936        conv5_block14_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activation (None, 4, 4, 484)     0           conv5_block15_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)    (None, 4, 4, 64)      30976       conv5_block15_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block15_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activation (None, 4, 4, 64)      0           conv5_block15_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block15_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatenat (None, 4, 4, 500)     0           conv5_block14_concat[0][0]       \n",
      "                                                                   conv5_block15_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormali (None, 4, 4, 500)     2000        conv5_block15_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activation (None, 4, 4, 500)     0           conv5_block16_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)    (None, 4, 4, 64)      32000       conv5_block16_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block16_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activation (None, 4, 4, 64)      0           conv5_block16_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block16_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatenat (None, 4, 4, 516)     0           conv5_block15_concat[0][0]       \n",
      "                                                                   conv5_block16_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "bn (BatchNormalization)          (None, 4, 4, 516)     2064        conv5_block16_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "relu (Activation)                (None, 4, 4, 516)     0           bn[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2D (None, 516)           0           relu[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "fc (Dense)                       (None, 171)           88407       avg_pool[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 1,945,831\n",
      "Trainable params: 1,902,543\n",
      "Non-trainable params: 43,288\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"av...)`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "img_model = DenseNet(blocks = [6, 12, 24, 16], \n",
    "                                cat_max = 171,\n",
    "                                weight_decay = 1e-4, \n",
    "                                kernel_initializer = 'glorot_normal',\n",
    "                                reduction = 0.5, \n",
    "                                init_filters = 64, \n",
    "                                growth_rate = 16).model\n",
    "img_model.load_weights(path + 'model_sub/6_12_24_16_ini64_growth16_02962/model_0_2018_09_13_08_23_48.h5')\n",
    "img_model_flat = Model(input = img_model.input, output = img_model.get_layer(name = 'avg_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = setB_test_data\n",
    "# # test_img = extract_array_from_series(test_data['img'])\n",
    "# # test_img = vgg16.preprocess_input(test_img)\n",
    "# # test_img_feature_map = img_model_flat.predict(test_img, verbose = 1)\n",
    "\n",
    "# with open(path + '/model_sub/6_12_24_16_ini64_growth32_inistride1_augdata_05010/flat_test_re_2018_09_22_19_24_38.pickle', 'rb') as handle:\n",
    "#     test_img_feature_map = pickle.load(handle)\n",
    "train_id = train_data['class_id'].unique()\n",
    "test_img_feature_map = extract_array_from_series(test_data['target'])\n",
    "# class_id_emb_attr.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_partial_model = Model(inputs = zs_model.inputs[2:], outputs = zs_model.outputs[0])\n",
    "test_class_id_emb_attr = class_id_emb_attr #[~class_id_emb_attr['class_id'].isin(train_id)]\n",
    "# pred_nearest_class_id = find_nearest_class(test_class_id_emb_attr, zs_partial_model, test_data, test_img_feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_05011 = models_eval(models = zs_model_list, eval_df = test_data, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[~class_id_emb_attr['class_id'].isin(train_id)], \\\n",
    "            img_feature_map = test_img_feature_map)\n",
    "preds_05011 = np.asarray(preds_05011).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ed5117811f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds_05077\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "preds_05077.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_102\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_102\" during training.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_108\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_108\" during training.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_114\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_114\" during training.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_120\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_120\" during training.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_126\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_126\" during training.\n"
     ]
    }
   ],
   "source": [
    "with open(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/flat_test_re_2018_09_24_03_07_15.pickle', 'rb') as handle:\n",
    "    flat_test_re = pickle.load(handle)\n",
    "zs_model_05077_list = []\n",
    "for i in range(5):\n",
    "    zs_model = create_dnn(kernel_initializer = 'he_normal', img_flat_len = 1032)\n",
    "    zs_model_name = path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/zs_model_' + str(i) +'_2018_09_25_01_01_31.txt'\n",
    "    zs_model.load_weights(zs_model_name)\n",
    "    zs_model_05077_list.append((zs_model, 'DEM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-086b73470e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreds_05077\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzs_model_05077_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mcand_class_id_emb_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_id_emb_attr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mclass_id_emb_attr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mimg_feature_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_test_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds_05011\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_05077\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "preds_05077 = models_eval(models = zs_model_05077_list, eval_df = test_data, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[~class_id_emb_attr['class_id'].isin(train_id)], \\\n",
    "            img_feature_map = flat_test_re)\n",
    "preds_05011 = np.asarray(preds_05011).T\n",
    "preds = np.c_[preds_05011, preds_05077]\n",
    "print (preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_preds = []\n",
    "for single_img_vote in preds:\n",
    "    uniq_val, counts = np.unique(single_img_vote, return_counts = True)\n",
    "    vote_preds.append(uniq_val[np.argmax(counts)])\n",
    "vote_preds = np.asarray(vote_preds)\n",
    "print (vote_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ZJL272' 'ZJL224' 'ZJL242' 'ZJL243' 'ZJL272']\n",
      " ['ZJL243' 'ZJL288' 'ZJL239' 'ZJL243' 'ZJL239']\n",
      " ['ZJL255' 'ZJL255' 'ZJL255' 'ZJL255' 'ZJL253']\n",
      " ...\n",
      " ['ZJL286' 'ZJL259' 'ZJL249' 'ZJL259' 'ZJL287']\n",
      " ['ZJL270' 'ZJL253' 'ZJL253' 'ZJL253' 'ZJL253']\n",
      " ['ZJL270' 'ZJL288' 'ZJL288' 'ZJL288' 'ZJL253']]\n",
      "['ZJL272' 'ZJL239' 'ZJL255' ... 'ZJL259' 'ZJL253' 'ZJL288']\n"
     ]
    }
   ],
   "source": [
    "pred_nearest_class_id = multi_models_vote(models = zs_model_list, eval_df = test_data, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[~class_id_emb_attr['class_id'].isin(train_id)], \\\n",
    "            img_feature_map = test_img_feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_label = time.strftime('_%Y_%m_%d_%H_%M_%S', time.gmtime())\n",
    "sub_name = path + '/model_sub/sub' + time_label + '.txt'\n",
    "sub = pd.DataFrame(pred_nearest_class_id, index = test_data['img_id'])\n",
    "sub.to_csv(sub_name, header = False, sep = '\\t')\n",
    "# zs_model.save(path + 'zs_model' + time_label + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.444444444444445"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "152 * 200 / 3600"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
