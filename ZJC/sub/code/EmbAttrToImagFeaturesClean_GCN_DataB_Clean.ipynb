{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import glob\n",
    "import gc\n",
    "import sklearn\n",
    "from DenseNet import DenseNet\n",
    "from DEM import DEM\n",
    "from sklearn.model_selection import KFold\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "from sklearn import metrics, preprocessing, pipeline, \\\n",
    "    feature_extraction, decomposition, model_selection\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import image\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator\n",
    "from tensorflow.python.keras import layers, preprocessing\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.python.keras.regularizers import l1, l2\n",
    "from tensorflow.python.keras.optimizers import SGD, RMSprop, Adam, Nadam\n",
    "from tensorflow.python.keras.losses import mean_squared_error, binary_crossentropy, categorical_crossentropy\n",
    "from tensorflow.python.keras.applications import vgg16\n",
    "\n",
    "# from keras.preprocessing import image\n",
    "# from keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator\n",
    "# from keras import layers, preprocessing\n",
    "# from keras import backend as K\n",
    "# from keras.models import Model, load_model\n",
    "# from keras.callbacks import EarlyStopping, Callback\n",
    "# from keras.regularizers import l1, l2\n",
    "# from keras.optimizers import SGD, RMSprop, Adam, Nadam\n",
    "# from keras.losses import mean_squared_error, binary_crossentropy\n",
    "# from keras.applications import vgg16\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import scipy \n",
    "import lightgbm as lgb\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "def read_image(imag_path, image_id):\n",
    "    img = image.load_img(imag_path + image_id)\n",
    "    img = image.img_to_array(img)\n",
    "#     img= vgg16.preprocess_input(image.img_to_array(img))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attr_list = pd.read_csv(path + '/DatasetA_train_20180813/attribute_list.txt', index_col = 0, sep = '\\t', header = None)\n",
    "# attr_list = attr_list.apply(lambda s: s[1].replace(' ', '_'), axis = 1)\n",
    "# attributes_per_class = pd.read_csv(path + '/DatasetA_train_20180813/attributes_per_class.txt', \n",
    "#                                    index_col = 0, sep = '\\t', header = None)\n",
    "# adj_graph\n",
    "# attr = np.array(list(class_id_emb_attr['attr']))\n",
    "# adj_graph = sklearn.metrics.pairwise.pairwise_distances(attr, metric = 'l2')\n",
    "# np.argsort(adj_graph[0])\n",
    "# adj_graph\n",
    "# class_id_emb_attr['attr'].values\n",
    "with open(path + '../../zero-shot-gcn/data/imagenet_graph.pkl', 'rb') as handle:\n",
    "    imagenet_graph = pickle.load(handle)\n",
    "with open(path + '../../zero-shot-gcn/data/list/invdict_wordntext.json', 'r') as fp:\n",
    "    words = json.load(fp)\n",
    "class_names = class_id_emb_attr[class_id_emb_attr.class_id.isin(round2_class_id)]['class_name'].values\n",
    "class_name_to_id = dict([(c, i) for i, c in enumerate(class_names)])\n",
    "class_wns = [[]] * class_names.shape[0]\n",
    "class_neighbor_wns = [[]] * class_names.shape[0]\n",
    "words_array = np.asarray(words)\n",
    "for i, word in enumerate(words):\n",
    "    for w in word.split(', '):\n",
    "        if w in class_name_to_id:\n",
    "#             print (class_name_to_id[w], i)\n",
    "            if len(class_wns[class_name_to_id[w]]) == 0:\n",
    "                class_wns[class_name_to_id[w]] = []\n",
    "            class_wns[class_name_to_id[w]].append(i)\n",
    "#             print (class_wns)\n",
    "            if len(class_neighbor_wns[class_name_to_id[w]]) == 0:\n",
    "                class_neighbor_wns[class_name_to_id[w]] = []\n",
    "            class_neighbor_wns[class_name_to_id[w]].extend(imagenet_graph[i])\n",
    "#             print (word, words_array[imagenet_graph[i]])\n",
    "#             print (i, w)\n",
    "#             if w not in class_dict:\n",
    "#                 class_dict[w] = 0\n",
    "#             class_dict[w] += 1\n",
    "# print (class_wns)\n",
    "wn_to_class = dict()\n",
    "for i, wns in enumerate(class_wns):\n",
    "    for wn in wns:\n",
    "        if wn not in wn_to_class:\n",
    "            wn_to_class[wn] = []\n",
    "        wn_to_class[wn].append(i)\n",
    "        \n",
    "adj_graph = scipy.eye(class_names.shape[0])\n",
    "# for i, neighbor_wns in enumerate(class_neighbor_wns):\n",
    "#     for j, wns in enumerate(class_wns):\n",
    "#         if i == j:\n",
    "#             continue\n",
    "#         for neighbor in neighbor_wns:\n",
    "#             if neighbor in wns:\n",
    "#                 adj_graph[i, j] = 1\n",
    "#                 adj_graph[j, i] = 1\n",
    "# #                 print(class_names[i], class_names[j])\n",
    "#                 break\n",
    "\n",
    "def find_adj(root, current_wn, remain_depth):\n",
    "    if current_wn in wn_to_class:\n",
    "        for adj_class in wn_to_class[current_wn]:\n",
    "            adj_graph[root][adj_class] = 1\n",
    "            adj_graph[adj_class][root] = 1\n",
    "    if remain_depth == 0:\n",
    "        return\n",
    "    for wn in imagenet_graph[current_wn]:\n",
    "        find_adj(root, wn, remain_depth - 1)\n",
    "        \n",
    "for i, wns in enumerate(class_wns):\n",
    "    for wn in wns:\n",
    "        find_adj(i, wn, 2)\n",
    "adj_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  3.,  5.,  1.,  3.,  2.,  2.,  3., 19.,  1.,  2.,  2.,\n",
       "        2., 19.,  5., 20., 19.,  2.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
       "       22.,  3.,  1.,  1.,  2., 19.,  1.,  1.,  2.,  2.,  1.,  1.,  1.,\n",
       "        1.,  1., 22.,  2.,  1.,  1.,  2.,  1.,  1.,  1.,  3.,  1.,  2.,\n",
       "        2.,  1.,  2.,  3.,  1.,  3.,  3.,  2.,  3.,  2.,  1.,  1., 19.,\n",
       "        1.,  1.,  4.,  2.,  1.,  1.,  1.,  4.,  1.,  3.,  2.,  1.,  1.,\n",
       "        3.,  1.,  1.,  1.,  2.,  1., 19.,  1.,  1.,  3.,  1.,  1.,  1.,\n",
       "        1.,  4.,  2.,  2.,  2.,  1.,  7.,  1.,  1.,  4.,  2.,  1.,  1.,\n",
       "        1.,  4.,  2.,  1.,  1., 19.,  2.,  1.,  1.,  1.,  1., 19.,  2.,\n",
       "        2.,  2.,  2.,  1., 19.,  1.,  1.,  1.,  2.,  1.,  2.,  2., 23.,\n",
       "        1.,  2.,  1.,  1.,  2.,  2.,  1.,  2., 22.,  1.,  7.,  1.,  2.,\n",
       "        2.,  2.,  1.,  1.,  1.,  1.,  2.,  4.,  1.,  4.,  1.,  4.,  1.,\n",
       "        2.,  2.,  3., 19.,  4.,  1.,  2.,  3.,  3.,  5.,  4., 19.,  2.,\n",
       "        1.,  2.,  4.,  1., 19.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  3.,  1.,  2.,\n",
       "        2.,  2.,  1.,  1.,  3., 19.,  1.,  3., 19.,  1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_graph.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del setA_train_data, setB_train_data\n",
    "# with open(path + 'setB_class_id_emb_attr.pickle', 'rb') as handle:\n",
    "#     class_id_emb_attr = pickle.load(handle)\n",
    "# with open(path + '/setA_train_data.pickle', 'rb') as handle:\n",
    "#     setA_train_data = pickle.load(handle)\n",
    "# with open(path + '/setB_train_data.pickle', 'rb') as handle:\n",
    "#     setB_train_data = pickle.load(handle)\n",
    "# with open(path + 'setB_test_data.pickle', 'rb') as handle:\n",
    "#     test_data = pickle.load(handle)\n",
    "    \n",
    "# train_data = setA_train_data.append(setB_train_data)\n",
    "# del setA_train_data, setB_train_data\n",
    "# with open(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/flat_train_re_2018_09_24_03_07_15.pickle', 'rb') as handle:\n",
    "#     flat_train_re = pickle.load(handle)\n",
    "# train_data['target'] = list(flat_train_re)\n",
    "# with open(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/flat_test_re_2018_09_24_03_07_15.pickle', 'rb') as handle:\n",
    "#     flat_test_re = pickle.load(handle)\n",
    "# test_data['target'] = list(flat_test_re)\n",
    "# train_data.drop(columns = ['img_id'], inplace = True)\n",
    "\n",
    "# with open(path + '/model_sub/6_12_24_16_ini64_growth32_inistride1_augdata_05010/train_data_2018_09_23_10_47_03.pickle', 'rb') as handle:\n",
    "#     train_data = pickle.load(handle)\n",
    "# with open(path + '/model_sub/6_12_24_16_ini64_growth32_inistride1_augdata_05010/test_data_2018_09_23_10_47_03.pickle', 'rb') as handle:\n",
    "#     test_data = pickle.load(handle)\n",
    "    \n",
    "with open(path + 'round2_class_id_emb_attr_add_golve_crawl.pkl', 'rb') as handle:\n",
    "    class_id_emb_attr = pickle.load(handle)\n",
    "with open(path + '/round1_train_img_part0.pkl', 'rb') as handle:\n",
    "    round1_train_img_part0 = pickle.load(handle)\n",
    "with open(path + '/round1_train_img_part1.pkl', 'rb') as handle:\n",
    "    round1_train_img_part1 = pickle.load(handle)\n",
    "with open(path + '/round2_train_img.pkl', 'rb') as handle:\n",
    "    round2_train_img = pickle.load(handle)\n",
    "with open(path + '/round2_test_img.pkl', 'rb') as handle:\n",
    "    test_data = pickle.load(handle)\n",
    "    \n",
    "with open(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05108_TF14_DensetNetProcess/train_data_img_flat_20181015_113824.pickle', 'rb') as handle:\n",
    "    train_img_flat = pickle.load(handle)\n",
    "with open(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05108_TF14_DensetNetProcess/test_data_img_flat_20181015_113824.pickle', 'rb') as handle:\n",
    "    test_img_flat = pickle.load(handle)\n",
    "round2_class_id = ['ZJL' + str(i) for i in range(296, 501)]\n",
    "round2_train_class_id = round2_train_img.class_id.unique()\n",
    "train_data = pd.concat([round1_train_img_part0, round1_train_img_part1, round2_train_img], axis = 0, sort = False)\n",
    "del round1_train_img_part0, round1_train_img_part1, round2_train_img\n",
    "gc.collect()\n",
    "train_data = train_data.merge(class_id_emb_attr, how = 'left', on = 'class_id')\n",
    "train_data['target'] = list(train_img_flat)\n",
    "test_data['target'] = list(test_img_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>class_name</th>\n",
       "      <th>emb_glove</th>\n",
       "      <th>emb_fasttext</th>\n",
       "      <th>emb</th>\n",
       "      <th>attr</th>\n",
       "      <th>emb_glove_crawl</th>\n",
       "      <th>emb_glove_crawl_42B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>[-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...</td>\n",
       "      <td>[0.10102, 0.1569, -0.56942, 0.2555300000000000...</td>\n",
       "      <td>[-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...</td>\n",
       "      <td>[0.023955, -0.09954099999999999, -0.4325600000...</td>\n",
       "      <td>[-0.31398000000000004, -0.076149, -0.5452, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZJL10</td>\n",
       "      <td>tarantula</td>\n",
       "      <td>[-0.1564, 0.38850999999999997, -0.331580000000...</td>\n",
       "      <td>[-0.12623, 0.14482, -0.064853, 0.2378900000000...</td>\n",
       "      <td>[-0.1564, 0.38850999999999997, -0.331580000000...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.27804, -0.5869800000000001, 0.2388, -0.020...</td>\n",
       "      <td>[-0.018662, -0.5282600000000001, -0.77403, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZJL100</td>\n",
       "      <td>drumstick</td>\n",
       "      <td>[-0.15132, 0.18007, 0.12005, 0.21443, -0.40094...</td>\n",
       "      <td>[-0.054255, -0.25582, -0.20056, 0.41381, -0.23...</td>\n",
       "      <td>[-0.15132, 0.18007, 0.12005, 0.21443, -0.40094...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.038967, -0.036926, 0.17812, 0.049033, 0.08...</td>\n",
       "      <td>[-0.41523999999999994, 0.053773, -0.327, 0.705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZJL101</td>\n",
       "      <td>dumbbell</td>\n",
       "      <td>[-0.3159, 0.15689, -0.037299, -0.2394800000000...</td>\n",
       "      <td>[-0.05218300000000001, 0.35549000000000003, -0...</td>\n",
       "      <td>[-0.3159, 0.15689, -0.037299, -0.2394800000000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.1, 0.3, ...</td>\n",
       "      <td>[-0.46728000000000003, 0.19132000000000002, 0....</td>\n",
       "      <td>[-0.14802, -0.38605, -0.46418999999999994, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZJL102</td>\n",
       "      <td>flagpole</td>\n",
       "      <td>[0.06352200000000001, -0.17278, 0.12012, -0.15...</td>\n",
       "      <td>[0.15872999999999998, -0.15893, 0.57038, 0.272...</td>\n",
       "      <td>[0.06352200000000001, -0.17278, 0.12012, -0.15...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, ...</td>\n",
       "      <td>[0.3301, -0.16408, -0.023536, -0.12745, 0.2338...</td>\n",
       "      <td>[0.14790999999999999, -0.015684, 0.045004, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ZJL103</td>\n",
       "      <td>fountain</td>\n",
       "      <td>[-0.06857, 0.22303, 0.02907, -0.59109, 0.23779...</td>\n",
       "      <td>[-0.11295, 0.05284, -0.036552999999999995, -0....</td>\n",
       "      <td>[-0.06857, 0.22303, 0.02907, -0.59109, 0.23779...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, ...</td>\n",
       "      <td>[0.48888000000000004, 0.17752, 0.20508, -0.632...</td>\n",
       "      <td>[-0.6807300000000001, -0.17543, 0.36665, -0.60...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ZJL104</td>\n",
       "      <td>car</td>\n",
       "      <td>[0.46443, 0.3773, -0.21459, -0.50768, -0.24575...</td>\n",
       "      <td>[-0.092271, -0.14855, -0.14695999999999998, 0....</td>\n",
       "      <td>[0.46443, 0.3773, -0.21459, -0.50768, -0.24575...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.4, ...</td>\n",
       "      <td>[0.20986999999999997, 0.46481000000000006, -0....</td>\n",
       "      <td>[0.59128, -0.38927, -0.16089, 0.043683, -0.438...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ZJL105</td>\n",
       "      <td>pan</td>\n",
       "      <td>[0.91643, 0.095422, 0.36995, -0.93058999999999...</td>\n",
       "      <td>[-0.014799000000000001, 0.16137, 0.23742, 0.34...</td>\n",
       "      <td>[0.91643, 0.095422, 0.36995, -0.93058999999999...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.50233, -0.36233000000000004, 0.38449, -0.13...</td>\n",
       "      <td>[-0.46858999999999995, -0.22300999999999999, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZJL106</td>\n",
       "      <td>coat</td>\n",
       "      <td>[0.033257999999999996, -0.43096, -0.0578690000...</td>\n",
       "      <td>[0.094537, 0.33866999999999997, -0.07047300000...</td>\n",
       "      <td>[0.033257999999999996, -0.43096, -0.0578690000...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6, 0.5, 0.3, ...</td>\n",
       "      <td>[0.2232, -0.45343999999999995, -0.20542, -0.05...</td>\n",
       "      <td>[-0.096183, -0.23246999999999998, -0.44756, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ZJL107</td>\n",
       "      <td>mask</td>\n",
       "      <td>[0.28328000000000003, 0.0095288, -0.1115400000...</td>\n",
       "      <td>[0.058257, 0.11672, -0.59637, 0.18753, -0.4049...</td>\n",
       "      <td>[0.28328000000000003, 0.0095288, -0.1115400000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7, 0.3, 0.0, ...</td>\n",
       "      <td>[0.32256999999999997, -0.78572, 0.23053, 0.522...</td>\n",
       "      <td>[-0.74239, 0.11094000000000001, 0.069526999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ZJL108</td>\n",
       "      <td>go-kart</td>\n",
       "      <td>[0.43715, -0.20469, 0.21794000000000002, 0.047...</td>\n",
       "      <td>[-0.11735999999999999, 0.29385, -0.099247, 0.4...</td>\n",
       "      <td>[0.43715, -0.20469, 0.21794000000000002, 0.047...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.2, 0.5, ...</td>\n",
       "      <td>[0.67255, 0.11864000000000001, 0.27234, -0.673...</td>\n",
       "      <td>[-0.096039, -0.058596, 0.11867, -0.31973, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ZJL109</td>\n",
       "      <td>gondola</td>\n",
       "      <td>[0.25874, -0.2576, -0.04796, -0.44076000000000...</td>\n",
       "      <td>[-0.11072, -0.17363, -0.003804, 0.17462, -0.12...</td>\n",
       "      <td>[0.25874, -0.2576, -0.04796, -0.44076000000000...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, ...</td>\n",
       "      <td>[0.80724, -0.62022, -0.087281, -0.20108, 0.516...</td>\n",
       "      <td>[-0.27453, -0.28143, -0.1066, -0.2500199999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ZJL11</td>\n",
       "      <td>centipede</td>\n",
       "      <td>[-0.45733999999999997, 0.65053, -0.15815, 0.08...</td>\n",
       "      <td>[0.22484, 0.049589999999999995, -0.54202, 0.42...</td>\n",
       "      <td>[-0.45733999999999997, 0.65053, -0.15815, 0.08...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.38859, -0.01813, 0.35791999999999996, -0.69...</td>\n",
       "      <td>[0.060733, 0.0056655, -0.13646, -0.79326000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ZJL110</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>[-0.43633, 0.057562, -0.19513, 0.29417, 0.3261...</td>\n",
       "      <td>[0.19329000000000002, -0.00083403, 0.216209999...</td>\n",
       "      <td>[-0.43633, 0.057562, -0.19513, 0.29417, 0.3261...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3, 0.6, 0.1, ...</td>\n",
       "      <td>[0.30480999999999997, 0.35470999999999997, -0....</td>\n",
       "      <td>[-0.6644399999999999, 0.44259, -0.16469, -0.30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ZJL111</td>\n",
       "      <td>ipod</td>\n",
       "      <td>[-0.25544, 0.07092899999999999, -0.23465999999...</td>\n",
       "      <td>[0.081179, 0.30584, 0.30259, -0.32008000000000...</td>\n",
       "      <td>[-0.25544, 0.07092899999999999, -0.23465999999...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.5, ...</td>\n",
       "      <td>[0.37897, 0.27955, -0.49711000000000005, 0.127...</td>\n",
       "      <td>[0.29821, -0.45556, -0.024387, 0.2963300000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ZJL113</td>\n",
       "      <td>kimono</td>\n",
       "      <td>[-0.43646999999999997, -0.31595, -0.064084, 0....</td>\n",
       "      <td>[-0.12088, 0.42343000000000003, -0.35503, 0.42...</td>\n",
       "      <td>[-0.43646999999999997, -0.31595, -0.064084, 0....</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, 0.3, 0.3, ...</td>\n",
       "      <td>[0.33543, -0.8378, -0.25, 0.10008, -0.212, 0.1...</td>\n",
       "      <td>[0.32889, -0.27141, -0.14526, -0.67075, 0.4593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ZJL114</td>\n",
       "      <td>lampshade</td>\n",
       "      <td>[-0.31351999999999997, -0.35048, -0.18804, -0....</td>\n",
       "      <td>[-0.063407, 0.12236, -0.33229000000000003, 0.3...</td>\n",
       "      <td>[-0.31351999999999997, -0.35048, -0.18804, -0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.8, 0.5, ...</td>\n",
       "      <td>[0.06582, 0.28188, -0.084722, 0.59576000000000...</td>\n",
       "      <td>[-0.060771000000000006, -0.38584, -0.318480000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ZJL115</td>\n",
       "      <td>mower</td>\n",
       "      <td>[0.41175, -0.083526, -0.18375999999999998, -0....</td>\n",
       "      <td>[0.27712, -0.025027, -0.28106, 0.11962, -0.155...</td>\n",
       "      <td>[0.41175, -0.083526, -0.18375999999999998, -0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.2, 0.5, ...</td>\n",
       "      <td>[0.1313, 0.8808, 0.16426, -1.001, 0.4484399999...</td>\n",
       "      <td>[-0.10315, 0.11039000000000002, -0.018491, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ZJL116</td>\n",
       "      <td>lifeboat</td>\n",
       "      <td>[-0.2953, -0.6340899999999999, 0.1097799999999...</td>\n",
       "      <td>[-0.066703, 0.25140999999999997, -0.81612, 0.9...</td>\n",
       "      <td>[-0.2953, -0.6340899999999999, 0.1097799999999...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.23453000000000002, -0.58067, 0.511869999999...</td>\n",
       "      <td>[0.037075, -0.16125, 0.30630999999999997, -0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ZJL117</td>\n",
       "      <td>limousine</td>\n",
       "      <td>[0.39878, -0.5906600000000001, -0.36968, -0.20...</td>\n",
       "      <td>[0.30633, 0.17705, -0.39937, 0.14406, -0.05432...</td>\n",
       "      <td>[0.39878, -0.5906600000000001, -0.36968, -0.20...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.4, ...</td>\n",
       "      <td>[0.7247399999999999, -0.42223, 0.3694699999999...</td>\n",
       "      <td>[0.5141100000000001, -0.13698, -0.1337, -0.306...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ZJL118</td>\n",
       "      <td>compass</td>\n",
       "      <td>[0.21789, -0.36199000000000003, 0.33075, -0.26...</td>\n",
       "      <td>[-0.16896, 0.19974, 0.044425, 0.1955, -0.45325...</td>\n",
       "      <td>[0.21789, -0.36199000000000003, 0.33075, -0.26...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.2, 0.0, ...</td>\n",
       "      <td>[0.5079600000000001, -0.044965, 0.16931, 0.368...</td>\n",
       "      <td>[0.22243000000000002, 0.24276999999999999, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ZJL119</td>\n",
       "      <td>maypole</td>\n",
       "      <td>[0.2263, -0.28943, 0.5158699999999999, -0.1743...</td>\n",
       "      <td>[-0.3459, 0.2593, -0.02922, 0.4220000000000000...</td>\n",
       "      <td>[0.2263, -0.28943, 0.5158699999999999, -0.1743...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.0356, -0.13166, -0.15897999999999998, -0.1...</td>\n",
       "      <td>[0.16385, -0.88102, -0.08139099999999999, -0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ZJL12</td>\n",
       "      <td>goose</td>\n",
       "      <td>[0.23565999999999998, 0.30683, 0.18506, 0.2682...</td>\n",
       "      <td>[-0.1804, -0.12746, -0.18237, 0.30876, 0.05533...</td>\n",
       "      <td>[0.23565999999999998, 0.30683, 0.18506, 0.2682...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, ...</td>\n",
       "      <td>[-0.40867, -0.28615999999999997, -0.0939830000...</td>\n",
       "      <td>[-0.62073, 0.082012, -0.39525, 0.034492, 0.375...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ZJL120</td>\n",
       "      <td>uniform</td>\n",
       "      <td>[-0.44696, -0.13147999999999999, 0.11385, -0.1...</td>\n",
       "      <td>[-0.032691000000000005, 0.18352000000000002, 0...</td>\n",
       "      <td>[-0.44696, -0.13147999999999999, 0.11385, -0.1...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, ...</td>\n",
       "      <td>[0.12286, -0.065718, 0.026074, 0.0558669999999...</td>\n",
       "      <td>[0.21191999999999997, -0.51664, -0.43588000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ZJL121</td>\n",
       "      <td>miniskirt</td>\n",
       "      <td>[0.041488, -0.012031, -0.75962, 0.084367, -0.0...</td>\n",
       "      <td>[0.21779, 0.27521, -0.65638, 0.273730000000000...</td>\n",
       "      <td>[0.041488, -0.012031, -0.75962, 0.084367, -0.0...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, ...</td>\n",
       "      <td>[-0.51274, -0.56379, 0.22956999999999997, 0.14...</td>\n",
       "      <td>[0.049699, -0.5247, -0.37903000000000003, 0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ZJL122</td>\n",
       "      <td>van</td>\n",
       "      <td>[0.16976, -0.49576000000000003, -0.4419, 0.275...</td>\n",
       "      <td>[0.044285000000000005, 0.03793, -0.32019000000...</td>\n",
       "      <td>[0.16976, -0.49576000000000003, -0.4419, 0.275...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.3, ...</td>\n",
       "      <td>[0.10032, 0.26011999999999996, 0.73498, -0.772...</td>\n",
       "      <td>[0.7381300000000001, 0.22448, -0.14224, 0.5984...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ZJL123</td>\n",
       "      <td>nail</td>\n",
       "      <td>[0.37637, 0.36164, -0.38082, 0.128229999999999...</td>\n",
       "      <td>[0.10393, 0.11660999999999999, -0.442789999999...</td>\n",
       "      <td>[0.37637, 0.36164, -0.38082, 0.128229999999999...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.012846999999999999, -0.34525, -0.1461299999...</td>\n",
       "      <td>[0.24656999999999998, 0.1653, -0.40645, 0.0427...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ZJL124</td>\n",
       "      <td>brace</td>\n",
       "      <td>[0.23373000000000002, -0.29489, -0.14131, -0.2...</td>\n",
       "      <td>[0.33081, 0.071895, -0.074313, 0.69019, -0.017...</td>\n",
       "      <td>[0.23373000000000002, -0.29489, -0.14131, -0.2...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.16519, -0.1456, 0.23697, 0.2792199999999999...</td>\n",
       "      <td>[0.39396, -0.010145999999999999, 0.39405999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ZJL125</td>\n",
       "      <td>obelisk</td>\n",
       "      <td>[-0.12885, -0.19426, -0.12495999999999999, -0....</td>\n",
       "      <td>[-0.25956999999999997, 0.12125999999999999, 0....</td>\n",
       "      <td>[-0.12885, -0.19426, -0.12495999999999999, -0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.63463, -0.30376, -0.11320999999999999, -0.7...</td>\n",
       "      <td>[-0.022999000000000002, -0.23999, -0.427330000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ZJL126</td>\n",
       "      <td>oboe</td>\n",
       "      <td>[-0.48135, 0.28976999999999997, -0.62751, 0.45...</td>\n",
       "      <td>[-0.12159, -0.06189, -0.11055999999999999, -0....</td>\n",
       "      <td>[-0.48135, 0.28976999999999997, -0.62751, 0.45...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, ...</td>\n",
       "      <td>[0.2428, 0.75253, -0.42863, 0.072672, 0.60411,...</td>\n",
       "      <td>[0.3057, -0.057422, -0.00081289, 0.023466, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>ZJL471</td>\n",
       "      <td>agama</td>\n",
       "      <td>[-0.53783, -0.152, 0.24159, 0.48955, 0.028544,...</td>\n",
       "      <td>[-0.12843, 0.0041765, -0.14608, 0.83854, -0.09...</td>\n",
       "      <td>[-0.53783, -0.152, 0.24159, 0.48955, 0.028544,...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.38328, -0.35111, 0.80497, 0.53471, 0.43116...</td>\n",
       "      <td>[0.19207, -0.19259, 0.14926, -0.17915999999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>ZJL472</td>\n",
       "      <td>lacewing</td>\n",
       "      <td>[0.11585999999999999, -0.27245, 0.01484, 0.576...</td>\n",
       "      <td>[-0.1201, 0.60075, -0.42141000000000006, 0.407...</td>\n",
       "      <td>[0.11585999999999999, -0.27245, 0.01484, 0.576...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.60842, -0.33603, 0.095416, 0.13685999999999...</td>\n",
       "      <td>[-0.06417300000000001, 0.4661, -0.64563, 0.163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>ZJL473</td>\n",
       "      <td>yurt</td>\n",
       "      <td>[-0.69553, -0.12761, -0.06188099999999999, 0.0...</td>\n",
       "      <td>[-0.007723600000000001, 0.16739, -0.14094, 0.1...</td>\n",
       "      <td>[-0.69553, -0.12761, -0.06188099999999999, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>[0.8066800000000001, -0.092073, -0.0052105, 0....</td>\n",
       "      <td>[0.4244, 0.08425, 0.21320999999999998, -0.1457...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>ZJL474</td>\n",
       "      <td>washbasin</td>\n",
       "      <td>[0.17372, -0.23195, -0.21595, -0.32579, -0.121...</td>\n",
       "      <td>[-0.18492, 0.20181, -0.48605, 0.46063000000000...</td>\n",
       "      <td>[0.17372, -0.23195, -0.21595, -0.32579, -0.121...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.70925, -0.70886, 0.31498000000000004, -0.58...</td>\n",
       "      <td>[-0.47503, -0.11547, -0.076226, -0.00326490000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>ZJL475</td>\n",
       "      <td>ballpoint</td>\n",
       "      <td>[-0.06717100000000001, -0.08897000000000001, 0...</td>\n",
       "      <td>[0.16055999999999998, 0.28657, -0.36071, -0.00...</td>\n",
       "      <td>[-0.06717100000000001, -0.08897000000000001, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.081456, 0.34315999999999997, -0.19744, -0.1...</td>\n",
       "      <td>[-0.36385, 0.55373, 0.85749, 0.294440000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>ZJL476</td>\n",
       "      <td>ski</td>\n",
       "      <td>[-0.2456, -0.024513999999999998, 0.17007, -0.6...</td>\n",
       "      <td>[0.29331, -0.34885, -0.5183300000000001, -0.17...</td>\n",
       "      <td>[-0.2456, -0.024513999999999998, 0.17007, -0.6...</td>\n",
       "      <td>[0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.87825, -0.37625, -0.050445, -0.25743, 0.855...</td>\n",
       "      <td>[0.013266, 0.19744, 0.028030000000000003, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>ZJL477</td>\n",
       "      <td>wreck</td>\n",
       "      <td>[0.46603999999999995, -0.0092186, -0.70795, 0....</td>\n",
       "      <td>[-0.16322, -0.0039851, -0.36935, 0.13552, -0.8...</td>\n",
       "      <td>[0.46603999999999995, -0.0092186, -0.70795, 0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.19552, 0.031567000000000005, 0.20351, 0.43...</td>\n",
       "      <td>[0.61506, 0.006174, 0.22041, -0.19977, -0.6259...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>ZJL478</td>\n",
       "      <td>mousetrap</td>\n",
       "      <td>[-0.29757, 0.10916, -0.039208, 0.08116, 0.0031...</td>\n",
       "      <td>[0.09300599999999999, -0.046221, -0.60934, 0.3...</td>\n",
       "      <td>[-0.29757, 0.10916, -0.039208, 0.08116, 0.0031...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.18648, -0.11025, -0.13226, -0.96509, -0.662...</td>\n",
       "      <td>[-0.8021699999999999, 0.29874, 0.33042, 0.0356...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>ZJL479</td>\n",
       "      <td>maraca</td>\n",
       "      <td>[0.3892, -0.12849000000000002, -0.18117, -0.31...</td>\n",
       "      <td>[-0.23006999999999997, -0.15936, -0.28283, -0....</td>\n",
       "      <td>[0.3892, -0.12849000000000002, -0.18117, -0.31...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.12749000000000002, -0.30838000000000004, 0...</td>\n",
       "      <td>[0.080948, -0.43278999999999995, -0.088029, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>ZJL480</td>\n",
       "      <td>boxer</td>\n",
       "      <td>[0.56898, 0.1318, 0.12480999999999999, -0.2858...</td>\n",
       "      <td>[-0.070242, -0.090798, 0.11889000000000001, 0....</td>\n",
       "      <td>[0.56898, 0.1318, 0.12480999999999999, -0.2858...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.094804, 0.060926, 0.072271, 0.58318, 0.1563...</td>\n",
       "      <td>[-0.31917, 0.70409, -0.34056, 0.098756, 0.1712...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>ZJL481</td>\n",
       "      <td>dogsled</td>\n",
       "      <td>[0.31743000000000005, -0.027689, 0.3448, 0.052...</td>\n",
       "      <td>[0.0091015, -0.033698, -0.54755, 0.36926, -0.1...</td>\n",
       "      <td>[0.31743000000000005, -0.027689, 0.3448, 0.052...</td>\n",
       "      <td>[0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.79776, -1.0966, 0.49346, -0.31661, 0.184770...</td>\n",
       "      <td>[0.040988, 0.38575, -0.39061, -0.12785, -0.581...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>ZJL482</td>\n",
       "      <td>apiary</td>\n",
       "      <td>[0.03403, 0.38684, -0.15054, 0.021769999999999...</td>\n",
       "      <td>[0.39909, -0.18896, -0.21218, 0.22801, -0.0572...</td>\n",
       "      <td>[0.03403, 0.38684, -0.15054, 0.021769999999999...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.65137, -0.021521000000000002, -0.05961, -0....</td>\n",
       "      <td>[0.42928, -0.58331, -0.60142, -0.26795, 0.218,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>ZJL483</td>\n",
       "      <td>joystick</td>\n",
       "      <td>[-0.36096, -0.28204, 0.42321000000000003, -0.1...</td>\n",
       "      <td>[-0.08125700000000001, 0.10639000000000001, -0...</td>\n",
       "      <td>[-0.36096, -0.28204, 0.42321000000000003, -0.1...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.47334, -0.45106999999999997, 0.43158, 0.037...</td>\n",
       "      <td>[-0.11133, 0.044267, 0.37578, -0.16876, 0.0451...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>ZJL484</td>\n",
       "      <td>rule</td>\n",
       "      <td>[0.38648000000000005, -0.58113, -0.11445999999...</td>\n",
       "      <td>[-0.14652, 0.00020665, -0.2177, 0.12092, -0.32...</td>\n",
       "      <td>[0.38648000000000005, -0.58113, -0.11445999999...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.12333, 0.18214, -0.14923, -0.070814, -0.156...</td>\n",
       "      <td>[0.088487, 0.38535, -0.12192, -0.2696300000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>ZJL485</td>\n",
       "      <td>volcano</td>\n",
       "      <td>[-0.17088, 0.62136, 0.20137, 0.12067, 0.250030...</td>\n",
       "      <td>[-0.099051, 0.14160999999999999, -0.17463, 0.1...</td>\n",
       "      <td>[-0.17088, 0.62136, 0.20137, 0.12067, 0.250030...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.1633, -0.62615, -0.14082999999999998, -0.26...</td>\n",
       "      <td>[0.5695100000000001, -0.63764, 0.35831, -1.046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>ZJL486</td>\n",
       "      <td>isopod</td>\n",
       "      <td>[0.29505, 0.26405, 0.014363999999999998, 0.267...</td>\n",
       "      <td>[0.25776, 0.34009, -0.7873399999999999, 0.1402...</td>\n",
       "      <td>[0.29505, 0.26405, 0.014363999999999998, 0.267...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.68396, -0.021064, 0.044612, 0.3205300000000...</td>\n",
       "      <td>[0.023985, -0.62839, -0.6834899999999999, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>ZJL487</td>\n",
       "      <td>racket</td>\n",
       "      <td>[0.36808, 0.08019, 0.64153, 0.51965, -0.15694,...</td>\n",
       "      <td>[0.023887000000000002, -0.25619000000000003, -...</td>\n",
       "      <td>[0.36808, 0.08019, 0.64153, 0.51965, -0.15694,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.13601, 0.39318000000000003, 0.905320000000...</td>\n",
       "      <td>[-0.30504000000000003, 0.3516, 0.23711, -0.219...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>ZJL488</td>\n",
       "      <td>greenhouse</td>\n",
       "      <td>[0.12113, 0.019457, -0.18559, -0.1184099999999...</td>\n",
       "      <td>[0.060266, 0.0034020000000000005, 0.18525, 0.2...</td>\n",
       "      <td>[0.12113, 0.019457, -0.18559, -0.1184099999999...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>[-0.12412999999999999, 0.49956000000000006, -0...</td>\n",
       "      <td>[0.044037, 0.12507000000000001, -0.067083, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>ZJL489</td>\n",
       "      <td>jersey</td>\n",
       "      <td>[-0.14861, -0.57285, -0.10121000000000001, 0.0...</td>\n",
       "      <td>[-0.46029, 0.21545, -0.26824000000000003, 0.20...</td>\n",
       "      <td>[-0.14861, -0.57285, -0.10121000000000001, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.005717100000000001, 0.13932, -0.4772, -0.2...</td>\n",
       "      <td>[-0.008202, 0.080524, 0.28069, 0.092307, -0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>ZJL490</td>\n",
       "      <td>hatchet</td>\n",
       "      <td>[-0.32777, 0.33784000000000003, -0.012699, -0....</td>\n",
       "      <td>[0.098689, 0.015606, -0.34459, 0.24534, -0.103...</td>\n",
       "      <td>[-0.32777, 0.33784000000000003, -0.012699, -0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.2562, -0.42556000000000005, 0.4253, -0.007...</td>\n",
       "      <td>[0.27318000000000003, 0.6505, 0.06275800000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>ZJL491</td>\n",
       "      <td>admiral</td>\n",
       "      <td>[0.053237, -0.3005, 0.53386, -0.84404, -0.3232...</td>\n",
       "      <td>[-0.534, 0.15883, 0.30427, 0.15452, 0.31190999...</td>\n",
       "      <td>[0.053237, -0.3005, 0.53386, -0.84404, -0.3232...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.20138, 0.16984000000000002, 0.5516300000000...</td>\n",
       "      <td>[0.43261000000000005, -0.3336, -0.57624, 0.124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>ZJL492</td>\n",
       "      <td>cello</td>\n",
       "      <td>[-0.29549000000000003, -0.2104, -0.64006000000...</td>\n",
       "      <td>[-0.095984, 0.12699000000000002, -0.39795, -0....</td>\n",
       "      <td>[-0.29549000000000003, -0.2104, -0.64006000000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "      <td>[0.28525, 0.57081, -0.20674, 0.438689999999999...</td>\n",
       "      <td>[0.48728, -0.78985, 0.50641, 0.539, 0.16630999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>ZJL493</td>\n",
       "      <td>puffer</td>\n",
       "      <td>[0.60475, 0.37146999999999997, 0.4640399999999...</td>\n",
       "      <td>[0.11824000000000001, -0.069411, -0.53354, 0.3...</td>\n",
       "      <td>[0.60475, 0.37146999999999997, 0.4640399999999...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.5636899999999999, -0.53077, -0.576469999999...</td>\n",
       "      <td>[-0.22116, 0.30507, -0.34612, -0.43244, 0.2688...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>ZJL494</td>\n",
       "      <td>coyote</td>\n",
       "      <td>[0.23928000000000002, 0.39632, -0.032925, 0.18...</td>\n",
       "      <td>[0.10211, -0.11512, -0.35106, 0.23190999999999...</td>\n",
       "      <td>[0.23928000000000002, 0.39632, -0.032925, 0.18...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.048178, 0.041510000000000005, -0.063398, -...</td>\n",
       "      <td>[0.16885999999999998, -0.07674600000000001, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>ZJL495</td>\n",
       "      <td>diaper</td>\n",
       "      <td>[-0.48094, -0.47023000000000004, -0.066839, 0....</td>\n",
       "      <td>[-0.10714000000000001, 0.00079191, -0.3952, 0....</td>\n",
       "      <td>[-0.48094, -0.47023000000000004, -0.066839, 0....</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.1472, -0.36365, -0.5222, 0.203609999999999...</td>\n",
       "      <td>[0.17210999999999999, 0.19710999999999998, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>ZJL496</td>\n",
       "      <td>lasagna</td>\n",
       "      <td>[0.12351, 0.20221, 0.12410999999999998, 0.0591...</td>\n",
       "      <td>[-0.023763999999999997, -0.1185, 0.12780999999...</td>\n",
       "      <td>[0.12351, 0.20221, 0.12410999999999998, 0.0591...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.1, ...</td>\n",
       "      <td>[-0.46092, 0.17612, 0.4718, -0.024802, -0.1768...</td>\n",
       "      <td>[-0.83254, -0.39349, 0.21775, -1.0836, 0.75835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>ZJL497</td>\n",
       "      <td>ravioli</td>\n",
       "      <td>[0.090012, -0.27005999999999997, 0.73474, 0.52...</td>\n",
       "      <td>[-0.14615999999999998, 0.30415, 0.11123, 0.095...</td>\n",
       "      <td>[0.090012, -0.27005999999999997, 0.73474, 0.52...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.1, 0.8, ...</td>\n",
       "      <td>[0.011011, 0.007280199999999999, 0.19083, -0.3...</td>\n",
       "      <td>[-0.8145, -0.0066213999999999995, -0.14902, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>ZJL498</td>\n",
       "      <td>crepe</td>\n",
       "      <td>[0.31296999999999997, 0.21861999999999998, 0.0...</td>\n",
       "      <td>[0.009517900000000001, 0.3177, -0.41242, 0.026...</td>\n",
       "      <td>[0.31296999999999997, 0.21861999999999998, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, ...</td>\n",
       "      <td>[0.043968, -0.24829, -0.093497, -0.42433000000...</td>\n",
       "      <td>[0.13965, -0.28951, 0.036315, -0.5971, 0.60287...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>ZJL499</td>\n",
       "      <td>hotpot</td>\n",
       "      <td>[-0.21215, 0.11823, 0.29638000000000003, -0.00...</td>\n",
       "      <td>[0.1664, 0.8796299999999999, -0.051648, -0.050...</td>\n",
       "      <td>[-0.21215, 0.11823, 0.29638000000000003, -0.00...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, ...</td>\n",
       "      <td>[0.33328, -0.44301999999999997, 0.42657, 0.209...</td>\n",
       "      <td>[0.1381, -0.33052, -0.62808, -0.29786, 0.40505...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>ZJL500</td>\n",
       "      <td>waffle</td>\n",
       "      <td>[-0.069418, 0.056188, -0.13912, 0.54079, 0.017...</td>\n",
       "      <td>[0.15233, -0.25386, -0.15198, 0.23751, -0.2285...</td>\n",
       "      <td>[-0.069418, 0.056188, -0.13912, 0.54079, 0.017...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.31118, -0.07368, 0.73992, -0.46829, -0.1902...</td>\n",
       "      <td>[-0.62258, -0.099615, 0.26702, 0.2378199999999...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class_id  class_name                                          emb_glove  \\\n",
       "0       ZJL1    goldfish  [-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...   \n",
       "1      ZJL10   tarantula  [-0.1564, 0.38850999999999997, -0.331580000000...   \n",
       "2     ZJL100   drumstick  [-0.15132, 0.18007, 0.12005, 0.21443, -0.40094...   \n",
       "3     ZJL101    dumbbell  [-0.3159, 0.15689, -0.037299, -0.2394800000000...   \n",
       "4     ZJL102    flagpole  [0.06352200000000001, -0.17278, 0.12012, -0.15...   \n",
       "5     ZJL103    fountain  [-0.06857, 0.22303, 0.02907, -0.59109, 0.23779...   \n",
       "6     ZJL104         car  [0.46443, 0.3773, -0.21459, -0.50768, -0.24575...   \n",
       "7     ZJL105         pan  [0.91643, 0.095422, 0.36995, -0.93058999999999...   \n",
       "8     ZJL106        coat  [0.033257999999999996, -0.43096, -0.0578690000...   \n",
       "9     ZJL107        mask  [0.28328000000000003, 0.0095288, -0.1115400000...   \n",
       "10    ZJL108     go-kart  [0.43715, -0.20469, 0.21794000000000002, 0.047...   \n",
       "11    ZJL109     gondola  [0.25874, -0.2576, -0.04796, -0.44076000000000...   \n",
       "12     ZJL11   centipede  [-0.45733999999999997, 0.65053, -0.15815, 0.08...   \n",
       "13    ZJL110   hourglass  [-0.43633, 0.057562, -0.19513, 0.29417, 0.3261...   \n",
       "14    ZJL111        ipod  [-0.25544, 0.07092899999999999, -0.23465999999...   \n",
       "15    ZJL113      kimono  [-0.43646999999999997, -0.31595, -0.064084, 0....   \n",
       "16    ZJL114   lampshade  [-0.31351999999999997, -0.35048, -0.18804, -0....   \n",
       "17    ZJL115       mower  [0.41175, -0.083526, -0.18375999999999998, -0....   \n",
       "18    ZJL116    lifeboat  [-0.2953, -0.6340899999999999, 0.1097799999999...   \n",
       "19    ZJL117   limousine  [0.39878, -0.5906600000000001, -0.36968, -0.20...   \n",
       "20    ZJL118     compass  [0.21789, -0.36199000000000003, 0.33075, -0.26...   \n",
       "21    ZJL119     maypole  [0.2263, -0.28943, 0.5158699999999999, -0.1743...   \n",
       "22     ZJL12       goose  [0.23565999999999998, 0.30683, 0.18506, 0.2682...   \n",
       "23    ZJL120     uniform  [-0.44696, -0.13147999999999999, 0.11385, -0.1...   \n",
       "24    ZJL121   miniskirt  [0.041488, -0.012031, -0.75962, 0.084367, -0.0...   \n",
       "25    ZJL122         van  [0.16976, -0.49576000000000003, -0.4419, 0.275...   \n",
       "26    ZJL123        nail  [0.37637, 0.36164, -0.38082, 0.128229999999999...   \n",
       "27    ZJL124       brace  [0.23373000000000002, -0.29489, -0.14131, -0.2...   \n",
       "28    ZJL125     obelisk  [-0.12885, -0.19426, -0.12495999999999999, -0....   \n",
       "29    ZJL126        oboe  [-0.48135, 0.28976999999999997, -0.62751, 0.45...   \n",
       "..       ...         ...                                                ...   \n",
       "460   ZJL471       agama  [-0.53783, -0.152, 0.24159, 0.48955, 0.028544,...   \n",
       "461   ZJL472    lacewing  [0.11585999999999999, -0.27245, 0.01484, 0.576...   \n",
       "462   ZJL473        yurt  [-0.69553, -0.12761, -0.06188099999999999, 0.0...   \n",
       "463   ZJL474   washbasin  [0.17372, -0.23195, -0.21595, -0.32579, -0.121...   \n",
       "464   ZJL475   ballpoint  [-0.06717100000000001, -0.08897000000000001, 0...   \n",
       "465   ZJL476         ski  [-0.2456, -0.024513999999999998, 0.17007, -0.6...   \n",
       "466   ZJL477       wreck  [0.46603999999999995, -0.0092186, -0.70795, 0....   \n",
       "467   ZJL478   mousetrap  [-0.29757, 0.10916, -0.039208, 0.08116, 0.0031...   \n",
       "468   ZJL479      maraca  [0.3892, -0.12849000000000002, -0.18117, -0.31...   \n",
       "469   ZJL480       boxer  [0.56898, 0.1318, 0.12480999999999999, -0.2858...   \n",
       "470   ZJL481     dogsled  [0.31743000000000005, -0.027689, 0.3448, 0.052...   \n",
       "471   ZJL482      apiary  [0.03403, 0.38684, -0.15054, 0.021769999999999...   \n",
       "472   ZJL483    joystick  [-0.36096, -0.28204, 0.42321000000000003, -0.1...   \n",
       "473   ZJL484        rule  [0.38648000000000005, -0.58113, -0.11445999999...   \n",
       "474   ZJL485     volcano  [-0.17088, 0.62136, 0.20137, 0.12067, 0.250030...   \n",
       "475   ZJL486      isopod  [0.29505, 0.26405, 0.014363999999999998, 0.267...   \n",
       "476   ZJL487      racket  [0.36808, 0.08019, 0.64153, 0.51965, -0.15694,...   \n",
       "477   ZJL488  greenhouse  [0.12113, 0.019457, -0.18559, -0.1184099999999...   \n",
       "478   ZJL489      jersey  [-0.14861, -0.57285, -0.10121000000000001, 0.0...   \n",
       "479   ZJL490     hatchet  [-0.32777, 0.33784000000000003, -0.012699, -0....   \n",
       "480   ZJL491     admiral  [0.053237, -0.3005, 0.53386, -0.84404, -0.3232...   \n",
       "481   ZJL492       cello  [-0.29549000000000003, -0.2104, -0.64006000000...   \n",
       "482   ZJL493      puffer  [0.60475, 0.37146999999999997, 0.4640399999999...   \n",
       "483   ZJL494      coyote  [0.23928000000000002, 0.39632, -0.032925, 0.18...   \n",
       "484   ZJL495      diaper  [-0.48094, -0.47023000000000004, -0.066839, 0....   \n",
       "485   ZJL496     lasagna  [0.12351, 0.20221, 0.12410999999999998, 0.0591...   \n",
       "486   ZJL497     ravioli  [0.090012, -0.27005999999999997, 0.73474, 0.52...   \n",
       "487   ZJL498       crepe  [0.31296999999999997, 0.21861999999999998, 0.0...   \n",
       "488   ZJL499      hotpot  [-0.21215, 0.11823, 0.29638000000000003, -0.00...   \n",
       "489   ZJL500      waffle  [-0.069418, 0.056188, -0.13912, 0.54079, 0.017...   \n",
       "\n",
       "                                          emb_fasttext  \\\n",
       "0    [0.10102, 0.1569, -0.56942, 0.2555300000000000...   \n",
       "1    [-0.12623, 0.14482, -0.064853, 0.2378900000000...   \n",
       "2    [-0.054255, -0.25582, -0.20056, 0.41381, -0.23...   \n",
       "3    [-0.05218300000000001, 0.35549000000000003, -0...   \n",
       "4    [0.15872999999999998, -0.15893, 0.57038, 0.272...   \n",
       "5    [-0.11295, 0.05284, -0.036552999999999995, -0....   \n",
       "6    [-0.092271, -0.14855, -0.14695999999999998, 0....   \n",
       "7    [-0.014799000000000001, 0.16137, 0.23742, 0.34...   \n",
       "8    [0.094537, 0.33866999999999997, -0.07047300000...   \n",
       "9    [0.058257, 0.11672, -0.59637, 0.18753, -0.4049...   \n",
       "10   [-0.11735999999999999, 0.29385, -0.099247, 0.4...   \n",
       "11   [-0.11072, -0.17363, -0.003804, 0.17462, -0.12...   \n",
       "12   [0.22484, 0.049589999999999995, -0.54202, 0.42...   \n",
       "13   [0.19329000000000002, -0.00083403, 0.216209999...   \n",
       "14   [0.081179, 0.30584, 0.30259, -0.32008000000000...   \n",
       "15   [-0.12088, 0.42343000000000003, -0.35503, 0.42...   \n",
       "16   [-0.063407, 0.12236, -0.33229000000000003, 0.3...   \n",
       "17   [0.27712, -0.025027, -0.28106, 0.11962, -0.155...   \n",
       "18   [-0.066703, 0.25140999999999997, -0.81612, 0.9...   \n",
       "19   [0.30633, 0.17705, -0.39937, 0.14406, -0.05432...   \n",
       "20   [-0.16896, 0.19974, 0.044425, 0.1955, -0.45325...   \n",
       "21   [-0.3459, 0.2593, -0.02922, 0.4220000000000000...   \n",
       "22   [-0.1804, -0.12746, -0.18237, 0.30876, 0.05533...   \n",
       "23   [-0.032691000000000005, 0.18352000000000002, 0...   \n",
       "24   [0.21779, 0.27521, -0.65638, 0.273730000000000...   \n",
       "25   [0.044285000000000005, 0.03793, -0.32019000000...   \n",
       "26   [0.10393, 0.11660999999999999, -0.442789999999...   \n",
       "27   [0.33081, 0.071895, -0.074313, 0.69019, -0.017...   \n",
       "28   [-0.25956999999999997, 0.12125999999999999, 0....   \n",
       "29   [-0.12159, -0.06189, -0.11055999999999999, -0....   \n",
       "..                                                 ...   \n",
       "460  [-0.12843, 0.0041765, -0.14608, 0.83854, -0.09...   \n",
       "461  [-0.1201, 0.60075, -0.42141000000000006, 0.407...   \n",
       "462  [-0.007723600000000001, 0.16739, -0.14094, 0.1...   \n",
       "463  [-0.18492, 0.20181, -0.48605, 0.46063000000000...   \n",
       "464  [0.16055999999999998, 0.28657, -0.36071, -0.00...   \n",
       "465  [0.29331, -0.34885, -0.5183300000000001, -0.17...   \n",
       "466  [-0.16322, -0.0039851, -0.36935, 0.13552, -0.8...   \n",
       "467  [0.09300599999999999, -0.046221, -0.60934, 0.3...   \n",
       "468  [-0.23006999999999997, -0.15936, -0.28283, -0....   \n",
       "469  [-0.070242, -0.090798, 0.11889000000000001, 0....   \n",
       "470  [0.0091015, -0.033698, -0.54755, 0.36926, -0.1...   \n",
       "471  [0.39909, -0.18896, -0.21218, 0.22801, -0.0572...   \n",
       "472  [-0.08125700000000001, 0.10639000000000001, -0...   \n",
       "473  [-0.14652, 0.00020665, -0.2177, 0.12092, -0.32...   \n",
       "474  [-0.099051, 0.14160999999999999, -0.17463, 0.1...   \n",
       "475  [0.25776, 0.34009, -0.7873399999999999, 0.1402...   \n",
       "476  [0.023887000000000002, -0.25619000000000003, -...   \n",
       "477  [0.060266, 0.0034020000000000005, 0.18525, 0.2...   \n",
       "478  [-0.46029, 0.21545, -0.26824000000000003, 0.20...   \n",
       "479  [0.098689, 0.015606, -0.34459, 0.24534, -0.103...   \n",
       "480  [-0.534, 0.15883, 0.30427, 0.15452, 0.31190999...   \n",
       "481  [-0.095984, 0.12699000000000002, -0.39795, -0....   \n",
       "482  [0.11824000000000001, -0.069411, -0.53354, 0.3...   \n",
       "483  [0.10211, -0.11512, -0.35106, 0.23190999999999...   \n",
       "484  [-0.10714000000000001, 0.00079191, -0.3952, 0....   \n",
       "485  [-0.023763999999999997, -0.1185, 0.12780999999...   \n",
       "486  [-0.14615999999999998, 0.30415, 0.11123, 0.095...   \n",
       "487  [0.009517900000000001, 0.3177, -0.41242, 0.026...   \n",
       "488  [0.1664, 0.8796299999999999, -0.051648, -0.050...   \n",
       "489  [0.15233, -0.25386, -0.15198, 0.23751, -0.2285...   \n",
       "\n",
       "                                                   emb  \\\n",
       "0    [-0.036463, 0.34304, -0.32295, 0.36496, 0.5080...   \n",
       "1    [-0.1564, 0.38850999999999997, -0.331580000000...   \n",
       "2    [-0.15132, 0.18007, 0.12005, 0.21443, -0.40094...   \n",
       "3    [-0.3159, 0.15689, -0.037299, -0.2394800000000...   \n",
       "4    [0.06352200000000001, -0.17278, 0.12012, -0.15...   \n",
       "5    [-0.06857, 0.22303, 0.02907, -0.59109, 0.23779...   \n",
       "6    [0.46443, 0.3773, -0.21459, -0.50768, -0.24575...   \n",
       "7    [0.91643, 0.095422, 0.36995, -0.93058999999999...   \n",
       "8    [0.033257999999999996, -0.43096, -0.0578690000...   \n",
       "9    [0.28328000000000003, 0.0095288, -0.1115400000...   \n",
       "10   [0.43715, -0.20469, 0.21794000000000002, 0.047...   \n",
       "11   [0.25874, -0.2576, -0.04796, -0.44076000000000...   \n",
       "12   [-0.45733999999999997, 0.65053, -0.15815, 0.08...   \n",
       "13   [-0.43633, 0.057562, -0.19513, 0.29417, 0.3261...   \n",
       "14   [-0.25544, 0.07092899999999999, -0.23465999999...   \n",
       "15   [-0.43646999999999997, -0.31595, -0.064084, 0....   \n",
       "16   [-0.31351999999999997, -0.35048, -0.18804, -0....   \n",
       "17   [0.41175, -0.083526, -0.18375999999999998, -0....   \n",
       "18   [-0.2953, -0.6340899999999999, 0.1097799999999...   \n",
       "19   [0.39878, -0.5906600000000001, -0.36968, -0.20...   \n",
       "20   [0.21789, -0.36199000000000003, 0.33075, -0.26...   \n",
       "21   [0.2263, -0.28943, 0.5158699999999999, -0.1743...   \n",
       "22   [0.23565999999999998, 0.30683, 0.18506, 0.2682...   \n",
       "23   [-0.44696, -0.13147999999999999, 0.11385, -0.1...   \n",
       "24   [0.041488, -0.012031, -0.75962, 0.084367, -0.0...   \n",
       "25   [0.16976, -0.49576000000000003, -0.4419, 0.275...   \n",
       "26   [0.37637, 0.36164, -0.38082, 0.128229999999999...   \n",
       "27   [0.23373000000000002, -0.29489, -0.14131, -0.2...   \n",
       "28   [-0.12885, -0.19426, -0.12495999999999999, -0....   \n",
       "29   [-0.48135, 0.28976999999999997, -0.62751, 0.45...   \n",
       "..                                                 ...   \n",
       "460  [-0.53783, -0.152, 0.24159, 0.48955, 0.028544,...   \n",
       "461  [0.11585999999999999, -0.27245, 0.01484, 0.576...   \n",
       "462  [-0.69553, -0.12761, -0.06188099999999999, 0.0...   \n",
       "463  [0.17372, -0.23195, -0.21595, -0.32579, -0.121...   \n",
       "464  [-0.06717100000000001, -0.08897000000000001, 0...   \n",
       "465  [-0.2456, -0.024513999999999998, 0.17007, -0.6...   \n",
       "466  [0.46603999999999995, -0.0092186, -0.70795, 0....   \n",
       "467  [-0.29757, 0.10916, -0.039208, 0.08116, 0.0031...   \n",
       "468  [0.3892, -0.12849000000000002, -0.18117, -0.31...   \n",
       "469  [0.56898, 0.1318, 0.12480999999999999, -0.2858...   \n",
       "470  [0.31743000000000005, -0.027689, 0.3448, 0.052...   \n",
       "471  [0.03403, 0.38684, -0.15054, 0.021769999999999...   \n",
       "472  [-0.36096, -0.28204, 0.42321000000000003, -0.1...   \n",
       "473  [0.38648000000000005, -0.58113, -0.11445999999...   \n",
       "474  [-0.17088, 0.62136, 0.20137, 0.12067, 0.250030...   \n",
       "475  [0.29505, 0.26405, 0.014363999999999998, 0.267...   \n",
       "476  [0.36808, 0.08019, 0.64153, 0.51965, -0.15694,...   \n",
       "477  [0.12113, 0.019457, -0.18559, -0.1184099999999...   \n",
       "478  [-0.14861, -0.57285, -0.10121000000000001, 0.0...   \n",
       "479  [-0.32777, 0.33784000000000003, -0.012699, -0....   \n",
       "480  [0.053237, -0.3005, 0.53386, -0.84404, -0.3232...   \n",
       "481  [-0.29549000000000003, -0.2104, -0.64006000000...   \n",
       "482  [0.60475, 0.37146999999999997, 0.4640399999999...   \n",
       "483  [0.23928000000000002, 0.39632, -0.032925, 0.18...   \n",
       "484  [-0.48094, -0.47023000000000004, -0.066839, 0....   \n",
       "485  [0.12351, 0.20221, 0.12410999999999998, 0.0591...   \n",
       "486  [0.090012, -0.27005999999999997, 0.73474, 0.52...   \n",
       "487  [0.31296999999999997, 0.21861999999999998, 0.0...   \n",
       "488  [-0.21215, 0.11823, 0.29638000000000003, -0.00...   \n",
       "489  [-0.069418, 0.056188, -0.13912, 0.54079, 0.017...   \n",
       "\n",
       "                                                  attr  \\\n",
       "0    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, ...   \n",
       "1    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.1, 0.3, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, ...   \n",
       "5    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, ...   \n",
       "6    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.4, ...   \n",
       "7    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...   \n",
       "8    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6, 0.5, 0.3, ...   \n",
       "9    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7, 0.3, 0.0, ...   \n",
       "10   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.2, 0.5, ...   \n",
       "11   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, ...   \n",
       "12   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "13   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3, 0.6, 0.1, ...   \n",
       "14   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.5, ...   \n",
       "15   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, 0.3, 0.3, ...   \n",
       "16   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.8, 0.5, ...   \n",
       "17   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.2, 0.5, ...   \n",
       "18   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "19   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.4, ...   \n",
       "20   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.2, 0.0, ...   \n",
       "21   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "22   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, ...   \n",
       "23   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, ...   \n",
       "24   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, ...   \n",
       "25   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.3, ...   \n",
       "26   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...   \n",
       "27   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...   \n",
       "28   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "29   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "460  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "461  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "462  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "463  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "464  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "465  [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "466  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "467  [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...   \n",
       "468  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "469  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "470  [0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "471  [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...   \n",
       "472  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...   \n",
       "473  [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...   \n",
       "474  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "475  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "476  [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...   \n",
       "477  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "478  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "479  [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...   \n",
       "480  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "481  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...   \n",
       "482  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "483  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "484  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "485  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.1, ...   \n",
       "486  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.1, 0.8, ...   \n",
       "487  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, ...   \n",
       "488  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, ...   \n",
       "489  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                       emb_glove_crawl  \\\n",
       "0    [0.023955, -0.09954099999999999, -0.4325600000...   \n",
       "1    [-0.27804, -0.5869800000000001, 0.2388, -0.020...   \n",
       "2    [-0.038967, -0.036926, 0.17812, 0.049033, 0.08...   \n",
       "3    [-0.46728000000000003, 0.19132000000000002, 0....   \n",
       "4    [0.3301, -0.16408, -0.023536, -0.12745, 0.2338...   \n",
       "5    [0.48888000000000004, 0.17752, 0.20508, -0.632...   \n",
       "6    [0.20986999999999997, 0.46481000000000006, -0....   \n",
       "7    [0.50233, -0.36233000000000004, 0.38449, -0.13...   \n",
       "8    [0.2232, -0.45343999999999995, -0.20542, -0.05...   \n",
       "9    [0.32256999999999997, -0.78572, 0.23053, 0.522...   \n",
       "10   [0.67255, 0.11864000000000001, 0.27234, -0.673...   \n",
       "11   [0.80724, -0.62022, -0.087281, -0.20108, 0.516...   \n",
       "12   [0.38859, -0.01813, 0.35791999999999996, -0.69...   \n",
       "13   [0.30480999999999997, 0.35470999999999997, -0....   \n",
       "14   [0.37897, 0.27955, -0.49711000000000005, 0.127...   \n",
       "15   [0.33543, -0.8378, -0.25, 0.10008, -0.212, 0.1...   \n",
       "16   [0.06582, 0.28188, -0.084722, 0.59576000000000...   \n",
       "17   [0.1313, 0.8808, 0.16426, -1.001, 0.4484399999...   \n",
       "18   [0.23453000000000002, -0.58067, 0.511869999999...   \n",
       "19   [0.7247399999999999, -0.42223, 0.3694699999999...   \n",
       "20   [0.5079600000000001, -0.044965, 0.16931, 0.368...   \n",
       "21   [-0.0356, -0.13166, -0.15897999999999998, -0.1...   \n",
       "22   [-0.40867, -0.28615999999999997, -0.0939830000...   \n",
       "23   [0.12286, -0.065718, 0.026074, 0.0558669999999...   \n",
       "24   [-0.51274, -0.56379, 0.22956999999999997, 0.14...   \n",
       "25   [0.10032, 0.26011999999999996, 0.73498, -0.772...   \n",
       "26   [0.012846999999999999, -0.34525, -0.1461299999...   \n",
       "27   [0.16519, -0.1456, 0.23697, 0.2792199999999999...   \n",
       "28   [0.63463, -0.30376, -0.11320999999999999, -0.7...   \n",
       "29   [0.2428, 0.75253, -0.42863, 0.072672, 0.60411,...   \n",
       "..                                                 ...   \n",
       "460  [-0.38328, -0.35111, 0.80497, 0.53471, 0.43116...   \n",
       "461  [0.60842, -0.33603, 0.095416, 0.13685999999999...   \n",
       "462  [0.8066800000000001, -0.092073, -0.0052105, 0....   \n",
       "463  [0.70925, -0.70886, 0.31498000000000004, -0.58...   \n",
       "464  [0.081456, 0.34315999999999997, -0.19744, -0.1...   \n",
       "465  [0.87825, -0.37625, -0.050445, -0.25743, 0.855...   \n",
       "466  [-0.19552, 0.031567000000000005, 0.20351, 0.43...   \n",
       "467  [0.18648, -0.11025, -0.13226, -0.96509, -0.662...   \n",
       "468  [-0.12749000000000002, -0.30838000000000004, 0...   \n",
       "469  [0.094804, 0.060926, 0.072271, 0.58318, 0.1563...   \n",
       "470  [0.79776, -1.0966, 0.49346, -0.31661, 0.184770...   \n",
       "471  [0.65137, -0.021521000000000002, -0.05961, -0....   \n",
       "472  [0.47334, -0.45106999999999997, 0.43158, 0.037...   \n",
       "473  [0.12333, 0.18214, -0.14923, -0.070814, -0.156...   \n",
       "474  [0.1633, -0.62615, -0.14082999999999998, -0.26...   \n",
       "475  [0.68396, -0.021064, 0.044612, 0.3205300000000...   \n",
       "476  [-0.13601, 0.39318000000000003, 0.905320000000...   \n",
       "477  [-0.12412999999999999, 0.49956000000000006, -0...   \n",
       "478  [-0.005717100000000001, 0.13932, -0.4772, -0.2...   \n",
       "479  [-0.2562, -0.42556000000000005, 0.4253, -0.007...   \n",
       "480  [0.20138, 0.16984000000000002, 0.5516300000000...   \n",
       "481  [0.28525, 0.57081, -0.20674, 0.438689999999999...   \n",
       "482  [0.5636899999999999, -0.53077, -0.576469999999...   \n",
       "483  [-0.048178, 0.041510000000000005, -0.063398, -...   \n",
       "484  [-0.1472, -0.36365, -0.5222, 0.203609999999999...   \n",
       "485  [-0.46092, 0.17612, 0.4718, -0.024802, -0.1768...   \n",
       "486  [0.011011, 0.007280199999999999, 0.19083, -0.3...   \n",
       "487  [0.043968, -0.24829, -0.093497, -0.42433000000...   \n",
       "488  [0.33328, -0.44301999999999997, 0.42657, 0.209...   \n",
       "489  [0.31118, -0.07368, 0.73992, -0.46829, -0.1902...   \n",
       "\n",
       "                                   emb_glove_crawl_42B  \n",
       "0    [-0.31398000000000004, -0.076149, -0.5452, -0....  \n",
       "1    [-0.018662, -0.5282600000000001, -0.77403, -0....  \n",
       "2    [-0.41523999999999994, 0.053773, -0.327, 0.705...  \n",
       "3    [-0.14802, -0.38605, -0.46418999999999994, -0....  \n",
       "4    [0.14790999999999999, -0.015684, 0.045004, -0....  \n",
       "5    [-0.6807300000000001, -0.17543, 0.36665, -0.60...  \n",
       "6    [0.59128, -0.38927, -0.16089, 0.043683, -0.438...  \n",
       "7    [-0.46858999999999995, -0.22300999999999999, 0...  \n",
       "8    [-0.096183, -0.23246999999999998, -0.44756, -0...  \n",
       "9    [-0.74239, 0.11094000000000001, 0.069526999999...  \n",
       "10   [-0.096039, -0.058596, 0.11867, -0.31973, -0.0...  \n",
       "11   [-0.27453, -0.28143, -0.1066, -0.2500199999999...  \n",
       "12   [0.060733, 0.0056655, -0.13646, -0.79326000000...  \n",
       "13   [-0.6644399999999999, 0.44259, -0.16469, -0.30...  \n",
       "14   [0.29821, -0.45556, -0.024387, 0.2963300000000...  \n",
       "15   [0.32889, -0.27141, -0.14526, -0.67075, 0.4593...  \n",
       "16   [-0.060771000000000006, -0.38584, -0.318480000...  \n",
       "17   [-0.10315, 0.11039000000000002, -0.018491, -1....  \n",
       "18   [0.037075, -0.16125, 0.30630999999999997, -0.2...  \n",
       "19   [0.5141100000000001, -0.13698, -0.1337, -0.306...  \n",
       "20   [0.22243000000000002, 0.24276999999999999, -0....  \n",
       "21   [0.16385, -0.88102, -0.08139099999999999, -0.4...  \n",
       "22   [-0.62073, 0.082012, -0.39525, 0.034492, 0.375...  \n",
       "23   [0.21191999999999997, -0.51664, -0.43588000000...  \n",
       "24   [0.049699, -0.5247, -0.37903000000000003, 0.18...  \n",
       "25   [0.7381300000000001, 0.22448, -0.14224, 0.5984...  \n",
       "26   [0.24656999999999998, 0.1653, -0.40645, 0.0427...  \n",
       "27   [0.39396, -0.010145999999999999, 0.39405999999...  \n",
       "28   [-0.022999000000000002, -0.23999, -0.427330000...  \n",
       "29   [0.3057, -0.057422, -0.00081289, 0.023466, -0....  \n",
       "..                                                 ...  \n",
       "460  [0.19207, -0.19259, 0.14926, -0.17915999999999...  \n",
       "461  [-0.06417300000000001, 0.4661, -0.64563, 0.163...  \n",
       "462  [0.4244, 0.08425, 0.21320999999999998, -0.1457...  \n",
       "463  [-0.47503, -0.11547, -0.076226, -0.00326490000...  \n",
       "464  [-0.36385, 0.55373, 0.85749, 0.294440000000000...  \n",
       "465  [0.013266, 0.19744, 0.028030000000000003, -0.1...  \n",
       "466  [0.61506, 0.006174, 0.22041, -0.19977, -0.6259...  \n",
       "467  [-0.8021699999999999, 0.29874, 0.33042, 0.0356...  \n",
       "468  [0.080948, -0.43278999999999995, -0.088029, 0....  \n",
       "469  [-0.31917, 0.70409, -0.34056, 0.098756, 0.1712...  \n",
       "470  [0.040988, 0.38575, -0.39061, -0.12785, -0.581...  \n",
       "471  [0.42928, -0.58331, -0.60142, -0.26795, 0.218,...  \n",
       "472  [-0.11133, 0.044267, 0.37578, -0.16876, 0.0451...  \n",
       "473  [0.088487, 0.38535, -0.12192, -0.2696300000000...  \n",
       "474  [0.5695100000000001, -0.63764, 0.35831, -1.046...  \n",
       "475  [0.023985, -0.62839, -0.6834899999999999, -0.0...  \n",
       "476  [-0.30504000000000003, 0.3516, 0.23711, -0.219...  \n",
       "477  [0.044037, 0.12507000000000001, -0.067083, -0....  \n",
       "478  [-0.008202, 0.080524, 0.28069, 0.092307, -0.11...  \n",
       "479  [0.27318000000000003, 0.6505, 0.06275800000000...  \n",
       "480  [0.43261000000000005, -0.3336, -0.57624, 0.124...  \n",
       "481  [0.48728, -0.78985, 0.50641, 0.539, 0.16630999...  \n",
       "482  [-0.22116, 0.30507, -0.34612, -0.43244, 0.2688...  \n",
       "483  [0.16885999999999998, -0.07674600000000001, -0...  \n",
       "484  [0.17210999999999999, 0.19710999999999998, -0....  \n",
       "485  [-0.83254, -0.39349, 0.21775, -1.0836, 0.75835...  \n",
       "486  [-0.8145, -0.0066213999999999995, -0.14902, -1...  \n",
       "487  [0.13965, -0.28951, 0.036315, -0.5971, 0.60287...  \n",
       "488  [0.1381, -0.33052, -0.62808, -0.29786, 0.40505...  \n",
       "489  [-0.62258, -0.099615, 0.26702, 0.2378199999999...  \n",
       "\n",
       "[490 rows x 8 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_id_emb_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text =  pd.read_csv(path + '/External/class_wordembeddings_glove.42B.300d', \n",
    "                        index_col = 0, sep = ' ', header = None)\n",
    "fast_text.index.name = 'class_name'\n",
    "fast_text_df = pd.DataFrame(index = fast_text.index)\n",
    "fast_text_df['emb_glove_crawl_42B'] = fast_text.apply(lambda s: np.array([float(x) for x in s])[:300], axis = 1)\n",
    "\n",
    "train_data = train_data.merge(fast_text_df, how = 'left', on = 'class_name')\n",
    "\n",
    "# train_data['emb'] = train_data.apply(lambda s: np.hstack([s['emb'], s['emb_glove_crawl']]), axis = 1)\n",
    "\n",
    "class_id_emb_attr = class_id_emb_attr.merge(fast_text_df, how = 'left', on = 'class_name')\n",
    "class_id_emb_attr['emb'] = class_id_emb_attr.apply(lambda s: np.hstack([s['emb'], s['emb_glove_crawl_42B']]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name = class_id_emb_attr.class_name.values\n",
    "glove_dict = {}\n",
    "with open(path + 'glove.twitter.27B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        ls = line.strip().split(' ')\n",
    "        if ls[0] in class_name:\n",
    "            glove_dict[ls[0]] = np.array([float(x) for x in ls[1:]])\n",
    "            if len(glove_dict) == len(class_name):\n",
    "                break\n",
    "len(glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_id_emb_attr.iloc[0].emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_df = pd.DataFrame(glove_dict).T\n",
    "glove_df.to_csv(path + 'class_wordembeddings_glove.42B.300d', header = None, sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'round2_class_id_emb_attr_add_golve_crawl_42B.pkl', 'wb') as handle:\n",
    "    pickle.dump(class_id_emb_attr, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, scores = None, cat_max = None, flags = None, model_type = None):\n",
    "        self.batch_size = 128 #flags.densenet_batch_size\n",
    "        self.epochs = 200 #flags.densenet_epochs\n",
    "        self.patience = 30 #flags.densenet_patience\n",
    "        self.scores = scores\n",
    "        self.cat_max = cat_max\n",
    "        self.model_type = model_type\n",
    "        self.aug_data = True #flags.aug_data\n",
    "        self.lr = 1e-3 #flags.lr\n",
    "        self.verbose = 2 #flags.train_verbose\n",
    "        self.OneHotEncoder = sklearn.preprocessing.OneHotEncoder()\n",
    "        self.model = self.small_densenet(\n",
    "                blocks = [6, 12, 24, 16], #[int(b.strip()) for b in flags.blocks.strip().split(',')], \n",
    "                weight_decay = 1e-4, #flags.weight_decay, \n",
    "                kernel_initializer = 'he_normal', #flags.kernel_initializer,\n",
    "                init_filters = 128, #flags.init_filters,\n",
    "                reduction = 0.5, #flags.reduction,\n",
    "                growth_rate = 32, #flags.growth_rate,\n",
    "                init_stride = 1 #flags.init_stride\n",
    "                )\n",
    "\n",
    "    def dense_block(self, x, blocks, name, \n",
    "            weight_decay = 1e-4, \n",
    "            kernel_initializer = 'he_normal',\n",
    "            growth_rate = None):\n",
    "        \"\"\"A dense block.\n",
    "        # Arguments\n",
    "            x: input tensor.\n",
    "            blocks: integer, the number of building blocks.\n",
    "            name: string, block label.\n",
    "        # Returns\n",
    "            output tensor for the block.\n",
    "        \"\"\"\n",
    "        for i in range(blocks):\n",
    "            x = self.conv_block(x, growth_rate, name=name + '_block' + str(i + 1), \n",
    "                weight_decay = weight_decay,\n",
    "                kernel_initializer = kernel_initializer)\n",
    "        return x\n",
    "\n",
    "    def transition_block(self, x, reduction, name, weight_decay = 1e-4, kernel_initializer = 'he_normal'):\n",
    "        \"\"\"A transition block.\n",
    "        # Arguments\n",
    "            x: input tensor.\n",
    "            reduction: float, compression rate at transition layers.\n",
    "            name: string, block label.\n",
    "        # Returns\n",
    "            output tensor for the block.\n",
    "        \"\"\"\n",
    "        bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "        x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                    name=name + '_bn')(x)\n",
    "        x = layers.Activation('relu', name=name + '_relu')(x)\n",
    "        x = layers.Conv2D(int(K.int_shape(x)[bn_axis] * reduction), 1,\n",
    "                        use_bias=False,\n",
    "                        kernel_initializer = kernel_initializer,\n",
    "                        kernel_regularizer = l2(weight_decay),\n",
    "                        name=name + '_conv')(x)\n",
    "        x = layers.AveragePooling2D(2, strides=2, name=name + '_pool')(x)\n",
    "        return x\n",
    "\n",
    "    def conv_block(self, x, growth_rate, name, weight_decay = 1e-4, kernel_initializer = 'he_normal'):\n",
    "        \"\"\"A building block for a dense block.\n",
    "        # Arguments\n",
    "            x: input tensor.\n",
    "            growth_rate: float, growth rate at dense layers.\n",
    "            name: string, block label.\n",
    "        # Returns\n",
    "            Output tensor for the block.\n",
    "        \"\"\"\n",
    "        bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "        x1 = layers.BatchNormalization(axis=bn_axis,\n",
    "                                    epsilon=1.001e-5,\n",
    "                                    name=name + '_0_bn')(x)\n",
    "        x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
    "        x1 = layers.Conv2D(4 * growth_rate, 1,\n",
    "                        use_bias=False,\n",
    "                        kernel_initializer = kernel_initializer,\n",
    "                        kernel_regularizer = l2(weight_decay),\n",
    "                        name=name + '_1_conv')(x1)\n",
    "        x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                    name=name + '_1_bn')(x1)\n",
    "        x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
    "        x1 = layers.Conv2D(growth_rate, 3,\n",
    "                        padding='same',\n",
    "                        use_bias=False,\n",
    "                        kernel_initializer = kernel_initializer,\n",
    "                        kernel_regularizer = l2(weight_decay),\n",
    "                        name=name + '_2_conv')(x1)\n",
    "        x = layers.Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
    "        return x\n",
    "\n",
    "    def small_densenet(self, img_input_shape = (64, 64, 3), \n",
    "        blocks = [6, 12, 24, 16], \n",
    "        weight_decay = 1e-4, \n",
    "        kernel_initializer = 'he_normal',\n",
    "        init_filters = None,\n",
    "        reduction = None,\n",
    "        growth_rate = None,\n",
    "        init_stride = None\n",
    "        ):\n",
    "        img_input = layers.Input(shape = (img_input_shape))\n",
    "\n",
    "        x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n",
    "        x = layers.Conv2D(init_filters, 3, strides=init_stride, use_bias=False, \n",
    "            kernel_initializer = kernel_initializer, \n",
    "            kernel_regularizer = l2(weight_decay),\n",
    "            name='conv1/conv')(x)\n",
    "        x = layers.BatchNormalization(\n",
    "            axis=3, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "        x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "        x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
    "        x = layers.AveragePooling2D(3, strides=2, name='pool1')(x)\n",
    "        \n",
    "        for i, block in enumerate(blocks):\n",
    "            scope_num_str = str(i + 2)\n",
    "            x = self.dense_block(x, block, name='conv' + scope_num_str, \n",
    "                                 growth_rate = growth_rate,\n",
    "                                 weight_decay = weight_decay, \n",
    "                                 kernel_initializer = kernel_initializer)\n",
    "            if i != len(blocks) - 1:\n",
    "                x = self.transition_block(x, reduction, name='pool' + scope_num_str, \n",
    "                                          weight_decay = weight_decay, kernel_initializer = kernel_initializer)\n",
    "        x = layers.BatchNormalization(\n",
    "            axis=3, epsilon=1.001e-5, name='bn')(x)\n",
    "        x = layers.Activation('relu', name='relu')(x)\n",
    "\n",
    "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = layers.Dense(self.cat_max, activation='softmax',\n",
    "            kernel_initializer = kernel_initializer, \n",
    "            name='fc')(x)\n",
    "        \n",
    "        model = Model(img_input, x)\n",
    "        model.compile(optimizer = Adam(lr=self.lr), loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def DNN_DataSet(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return preprocess_img(df['img'])\n",
    "\n",
    "    def train(self, train_part_df, train_part_label, validate_part_df, validate_part_label):\n",
    "        \"\"\"\n",
    "        Keras Training\n",
    "        \"\"\"\n",
    "        print(\"-----DNN training-----\")\n",
    "\n",
    "        DNN_Train_Data = self.DNN_DataSet(train_part_df)\n",
    "        DNN_validate_Data = self.DNN_DataSet(validate_part_df)\n",
    "\n",
    "        callbacks = [\n",
    "                EarlyStopping(monitor='val_categorical_accuracy', patience=self.patience, verbose=0),\n",
    "                ]\n",
    "        if self.aug_data:\n",
    "            datagen = preprocessing.image.ImageDataGenerator(\n",
    "                    rotation_range=45,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True)\n",
    "\n",
    "            datagen.fit(DNN_Train_Data)\n",
    "\n",
    "            h = self.model.fit_generator(datagen.flow(DNN_Train_Data, train_part_label, batch_size=self.batch_size), \n",
    "                    validation_data=(DNN_validate_Data, validate_part_label), steps_per_epoch = DNN_Train_Data.shape[0]//self.batch_size,\n",
    "                    epochs=self.epochs, shuffle=True, verbose = self.verbose, workers=1, use_multiprocessing=False, \n",
    "                    callbacks=callbacks)\n",
    "        else:\n",
    "            h = self.model.fit(DNN_Train_Data, train_part_label, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                        shuffle=True, verbose=self.verbose,\n",
    "                        validation_data=(DNN_validate_Data, validate_part_label)\n",
    "                        , callbacks=callbacks\n",
    "                        )\n",
    "        self.scores.append(pd.DataFrame(h.history))\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, test_part, batch_size = 1024, verbose=2):\n",
    "        \"\"\"\n",
    "        Keras Training\n",
    "        \"\"\"\n",
    "        print(\"-----DNN Test-----\")\n",
    "        pred = self.model.predict(self.DNN_DataSet(test_part), verbose=verbose)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119145/119145 [==============================] - 12271s \n",
      "7817/7817 [==============================] - 817s   \n"
     ]
    }
   ],
   "source": [
    "img_model = DenseNet(scores = None, \n",
    "                     cat_max = 205, #flags.cat_max, \n",
    "                     flags = None, \n",
    "                     model_type = 'DenseNet').model\n",
    "# model_file_name = glob.glob(model_path + '/imgmodel_*.h5')[0]\n",
    "# print ('Model file name: ', model_file_name)\n",
    "# img_model.load_weights(model_file_name)\n",
    "img_model.load_weights('../../Data/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/model_0_2018_09_24_03_07_15.h5')\n",
    "# img_model_flat = Model(input = img_model.input, output = img_model.get_layer(name = 'avg_pool').output)\n",
    "train_data['target'] = list(model_eval(img_model, 'DenseNet', train_data))\n",
    "test_data['target'] = list(model_eval(img_model, 'DenseNet', test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69314718, 0.69314718, 0.69314718])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 1], [1, 1], [1, 1]])\n",
    "# y = np.array([[1, 1], [0.5, 0.5], [0.2, 0.2]])\n",
    "y = np.array([[0.5, 0.5]])\n",
    "multi_labels_cross_entropy(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def extract_array_from_series(s):\n",
    "    return np.asarray(list(s))\n",
    "\n",
    "def create_dem_data(df, only_emb = False):\n",
    "    if only_emb:\n",
    "        emb = extract_array_from_series(df['emb'])[:, :]\n",
    "        attr = np.zeros((emb.shape[0], 50))\n",
    "        return [attr, emb]\n",
    "    else:\n",
    "        return [extract_array_from_series(df['attr'])[:, :50], extract_array_from_series(df['emb'])[:, :]]\n",
    "\n",
    "def create_gcn_data(df, class_to_id):\n",
    "    return np.array([class_to_id[c] for c in df['class_id'].values]).astype('int32')\n",
    "\n",
    "def create_qfsl_data(df, class_to_id, categories = 205):\n",
    "    OneHotEncoder = sklearn.preprocessing.OneHotEncoder(n_values = categories )\n",
    "    train_target = df['class_id'].apply(lambda c: class_to_id[c]).values\n",
    "    train_target = OneHotEncoder.fit_transform(np.reshape(train_target, (-1, 1))).toarray()\n",
    "    return [extract_array_from_series(df['target']), train_target]\n",
    "\n",
    "def neg_aug_data(pos_data, train_class_id):\n",
    "    pos_len = pos_data[0].shape[0]\n",
    "    ind_array = np.array(range(pos_len))\n",
    "    perm_ind_array = np.random.permutation(ind_array)\n",
    "    perm_label = np.zeros(pos_len)\n",
    "    perm_label[train_class_id == train_class_id[perm_ind_array]] = 1\n",
    "    perm_img_feature_map = pos_data[0][perm_ind_array] \n",
    "        \n",
    "    neg_data = [perm_img_feature_map] + pos_data[1:3] + [perm_label]\n",
    "    return neg_data\n",
    "        \n",
    "def create_dem_bc_data(df, neg_aug = 0, only_emb = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    train_data = [extract_array_from_series(df['target'])] + create_dem_data(df, only_emb)\n",
    "    train_len = train_data[0].shape[0]\n",
    "    train_data = train_data + [np.ones(train_len)]\n",
    "    merge_data = train_data\n",
    "    if neg_aug > 0:\n",
    "        train_class_id = extract_array_from_series(df['class_id'])\n",
    "        for i in range(neg_aug):\n",
    "            neg_data = neg_aug_data(train_data, train_class_id)\n",
    "            merge_data = [np.r_[merge_data[i], neg_data[i]] for i in range(len(merge_data))]\n",
    "        print ('DEM BC Data Train Len, Pos, Neg:', train_len, np.sum(merge_data[-1]), np.sum(merge_data[-1] == 0))\n",
    "        return merge_data\n",
    "    else:\n",
    "        return train_data\n",
    "            \n",
    "def preprocess_numpy_input(x, data_format = 'channels_last', mode = 'torch', **kwargs):\n",
    "    \"\"\"Preprocesses a Numpy array encoding a batch of images.\n",
    "\n",
    "    # Arguments\n",
    "        x: Input array, 3D or 4D.\n",
    "        data_format: Data format of the image array.\n",
    "        mode: One of \"caffe\", \"tf\" or \"torch\".\n",
    "            - caffe: will convert the images from RGB to BGR,\n",
    "                then will zero-center each color channel with\n",
    "                respect to the ImageNet dataset,\n",
    "                without scaling.\n",
    "            - tf: will scale pixels between -1 and 1,\n",
    "                sample-wise.\n",
    "            - torch: will scale pixels between 0 and 1 and then\n",
    "                will normalize each channel with respect to the\n",
    "                ImageNet dataset.\n",
    "\n",
    "    # Returns\n",
    "        Preprocessed Numpy array.\n",
    "    \"\"\"\n",
    "    if not issubclass(x.dtype.type, np.floating):\n",
    "        x = x.astype(K.floatx(), copy=False)\n",
    "\n",
    "    if mode == 'tf':\n",
    "        x /= 127.5\n",
    "        x -= 1.\n",
    "        return x\n",
    "\n",
    "    if mode == 'torch':\n",
    "        x /= 255.\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    else:\n",
    "        if data_format == 'channels_first':\n",
    "            # 'RGB'->'BGR'\n",
    "            if x.ndim == 3:\n",
    "                x = x[::-1, ...]\n",
    "            else:\n",
    "                x = x[:, ::-1, ...]\n",
    "        else:\n",
    "            # 'RGB'->'BGR'\n",
    "            x = x[..., ::-1]\n",
    "        mean = [103.939, 116.779, 123.68]\n",
    "        std = None\n",
    "\n",
    "    # Zero-center by mean pixel\n",
    "    if data_format == 'channels_first':\n",
    "        if x.ndim == 3:\n",
    "            x[0, :, :] -= mean[0]\n",
    "            x[1, :, :] -= mean[1]\n",
    "            x[2, :, :] -= mean[2]\n",
    "            if std is not None:\n",
    "                x[0, :, :] /= std[0]\n",
    "                x[1, :, :] /= std[1]\n",
    "                x[2, :, :] /= std[2]\n",
    "        else:\n",
    "            x[:, 0, :, :] -= mean[0]\n",
    "            x[:, 1, :, :] -= mean[1]\n",
    "            x[:, 2, :, :] -= mean[2]\n",
    "            if std is not None:\n",
    "                x[:, 0, :, :] /= std[0]\n",
    "                x[:, 1, :, :] /= std[1]\n",
    "                x[:, 2, :, :] /= std[2]\n",
    "    else:\n",
    "        x[..., 0] -= mean[0]\n",
    "        x[..., 1] -= mean[1]\n",
    "        x[..., 2] -= mean[2]\n",
    "        if std is not None:\n",
    "            x[..., 0] /= std[0]\n",
    "            x[..., 1] /= std[1]\n",
    "            x[..., 2] /= std[2]\n",
    "    return x\n",
    "\n",
    "def preprocess_img(img_series):\n",
    "    return preprocess_numpy_input(extract_array_from_series(img_series))\n",
    "\n",
    "def multi_labels_cross_entropy(y_true, y_preds, eps = 1e-6):\n",
    "#     multi_labels_loss = [sklearn.metrics.log_loss]\n",
    "#     y_preds_clip = max(eps, min(1 - eps, y_preds))\n",
    "    y_preds_clip = np.clip(y_preds, eps, 1- eps)\n",
    "    multi_loss = -(y_true * np.log(y_preds_clip) + (1 - y_true) * np.log(1 - y_preds_clip))\n",
    "    return np.mean(multi_loss, axis = -1)\n",
    "\n",
    "def find_nearest_class(class_id_emb_attr, eval_df, cand_feature_map = None, img_feature_map = None,\n",
    "                      model_type = None, attr_preds = None, zs_model = None, dis_arr = None):\n",
    "    nearest_class_id = ['ZJL'] * img_feature_map.shape[0]\n",
    "    for i in range(img_feature_map.shape[0]):\n",
    "        if model_type == 'I2A':\n",
    "            dis = multi_labels_cross_entropy(extract_array_from_series(class_id_emb_attr['attr'])[:, :50],\n",
    "                                            attr_preds[i])\n",
    "        elif model_type == 'DEM_BC':\n",
    "            img = img_feature_map[i]\n",
    "#             pred_data = [cand_feature_map, np.array([img] * class_id_emb_attr.shape[0])]\n",
    "            pred_data = cand_feature_map * img\n",
    "#             pred_data = np.c_[cand_feature_map, np.array([img] * cand_feature_map.shape[0])]\n",
    "#             print (pred_data[2].shape)\n",
    "            dis = 1 - zs_model.predict(pred_data)\n",
    "        elif model_type == 'QFSL':\n",
    "            dis = 1 - dis_arr[i]\n",
    "        else:\n",
    "            img = img_feature_map[i]\n",
    "            dis = np.linalg.norm(img - cand_feature_map, axis = 1)\n",
    "        min_ind = np.where(dis == np.amin(dis))[0]\n",
    "        nearest_class_id[i] = class_id_emb_attr.iloc[min_ind[0]]['class_id']\n",
    "    return np.asarray(nearest_class_id)\n",
    "        \n",
    "def calc_accuracy(eval_df, eval_class, preds):\n",
    "    eval_mask = eval_df.class_id.isin(eval_class)\n",
    "    eval_num = np.sum(eval_mask)\n",
    "    right_num = np.sum(preds[eval_mask] == eval_df.class_id[eval_mask])\n",
    "    return right_num / np.sum(eval_mask), right_num, eval_num\n",
    "    \n",
    "def calc_detailed_accuracy(eval_df, preds, class_id_dict):\n",
    "    print (\"\\n\")\n",
    "    for class_set_name in sorted(class_id_dict):\n",
    "        class_set = class_id_dict[class_set_name]\n",
    "        re = calc_accuracy(eval_df, class_set, preds)\n",
    "        print(\"%s: \\t%.6f\\t%.0f\\t%.0f\" % ((class_set_name,) + re))\n",
    "#     print (\"\\n\")\n",
    "\n",
    "def multi_preds_vote(preds):\n",
    "    vote_preds = []\n",
    "    for single_img_vote in preds:\n",
    "        uniq_val, counts = np.unique(single_img_vote, return_counts = True)\n",
    "        vote_preds.append(uniq_val[np.argmax(counts)])\n",
    "    vote_preds = np.asarray(vote_preds)\n",
    "    return vote_preds\n",
    "    \n",
    "def multi_models_vote(models, eval_df = None, cand_class_id_emb_attr = None, img_feature_map = None, \n",
    "                      class_id_dict = None):\n",
    "    print ('cand shape: ', cand_class_id_emb_attr.shape[0])\n",
    "    preds = models_eval(models, eval_df, cand_class_id_emb_attr, img_feature_map, \n",
    "                class_id_dict)\n",
    "    preds = np.asarray(preds).T\n",
    "    vote_preds = multi_preds_vote(preds)\n",
    "    # print (vote_preds)\n",
    "    print ('Multi model votes results:')\n",
    "    if 'class_id' in eval_df.columns: \n",
    "        calc_detailed_accuracy(eval_df, vote_preds, class_id_dict)\n",
    "    return vote_preds\n",
    "\n",
    "def model_eval(model, model_type, eval_df, cand_class_id_emb_attr = None, img_feature_map = None, \n",
    "                class_id_dict = None, class_to_id = None, TTA = None, img_model = None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if model_type == 'DenseNet':\n",
    "        flat_model = Model(inputs = model.inputs, outputs = model.get_layer(name = 'avg_pool').output)\n",
    "        pred = flat_model.predict(preprocess_img(eval_df['img']), verbose = 1)\n",
    "    elif model_type == 'DEM' or model_type == 'AE':\n",
    "        zs_model = Model(inputs = model.inputs[:2], outputs = model.outputs[0])\n",
    "        cand_feature_map = zs_model.predict(create_dem_data(cand_class_id_emb_attr), verbose = 2)\n",
    "        if TTA is not None:\n",
    "            batch_size = 32\n",
    "            datagen = MixedImageDataGenerator(\n",
    "                    rotation_range=20,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True)\n",
    "            img_feature_map = img_model.predict_generator(\n",
    "                datagen.flow((preprocess_img(eval_df['img'])), None, shuffle = False, batch_size = batch_size), \n",
    "                steps = np.ceil(eval_df.shape[0] / batch_size) * TTA, verbose = 1)\n",
    "            pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map, img_feature_map)\n",
    "            pred = np.reshape(pred, (TTA, eval_df.shape[0])).T\n",
    "            pred = multi_preds_vote(pred)\n",
    "        else:\n",
    "            pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map, img_feature_map)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'DEM_BC':\n",
    "        zs_model = Model(inputs = model.inputs[1:3], outputs = model.outputs[0])\n",
    "        cand_feature_map = zs_model.predict(create_dem_data(cand_class_id_emb_attr, only_emb = True), verbose = 1)\n",
    "        zs_model = Model(inputs = model.get_layer('attr_x_img_model').inputs, \n",
    "                         outputs = model.get_layer('attr_x_img_model').outputs)\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map = cand_feature_map, img_feature_map = img_feature_map,\n",
    "                                zs_model = zs_model, model_type = model_type)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'GCN':\n",
    "        zs_model = Model(inputs = model.inputs[2:], outputs = model.outputs[0])\n",
    "        cand_class_to_id = [class_to_id[c] for c in cand_class_id_emb_attr.class_id.values]\n",
    "        cand_feature_map = zs_model.predict(None, steps = 1)[cand_class_to_id]\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map, img_feature_map)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'QFSL':\n",
    "        cand_class_to_id = [class_to_id[c] for c in cand_class_id_emb_attr.class_id.values]\n",
    "        zs_model = Model(inputs = [model.inputs[0]] + model.inputs[2:], outputs = model.outputs[0])\n",
    "        dis_arr = zs_model.predict(img_feature_map, verbose = 2)[:, cand_class_to_id]\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, model_type = model_type, dis_arr = dis_arr,\n",
    "                                 img_feature_map = img_feature_map)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'I2A':\n",
    "        zs_model = Model(inputs = model.inputs[-1], outputs = model.outputs[0])\n",
    "        attr_preds = zs_model.predict(extract_array_from_series(img_feature_map), verbose = 2)\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, None, img_feature_map, \n",
    "                                  model_type, attr_preds)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    elif model_type == 'DEM_AUG':\n",
    "        img_model = Model(inputs = model.inputs[0], outputs = model.outputs[-1])\n",
    "        img_feature_map = img_model.predict(preprocess_img(eval_df['img']), verbose = 2)\n",
    "        zs_model = Model(inputs = model.inputs[1:], outputs = model.outputs[0])\n",
    "        cand_feature_map = zs_model.predict(create_dem_data(cand_class_id_emb_attr), verbose = 2)\n",
    "        pred = find_nearest_class(cand_class_id_emb_attr, eval_df, cand_feature_map, img_feature_map)\n",
    "        if 'class_id' in eval_df.columns:\n",
    "            calc_detailed_accuracy(eval_df, pred, class_id_dict)\n",
    "    return pred\n",
    "\n",
    "def models_eval(models, eval_df, cand_class_id_emb_attr = None, img_feature_map = None, \n",
    "                class_id_dict = None, class_to_id = None, img_model = None):\n",
    "    preds = []\n",
    "    for (model, model_type) in models:\n",
    "        pred = model_eval(model, model_type, eval_df = eval_df, cand_class_id_emb_attr = cand_class_id_emb_attr, \n",
    "            img_feature_map = img_feature_map, class_id_dict = class_id_dict, class_to_id = class_to_id,\n",
    "            img_model = img_model)\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "class MixedImageDataGenerator(ImageDataGenerator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MixedImageDataGenerator, self).__init__(**kwargs)\n",
    "        \n",
    "    def flow(self,\n",
    "           x,\n",
    "           y=None,\n",
    "           batch_size=32,\n",
    "           shuffle=True,\n",
    "           seed=None,\n",
    "           save_to_dir=None,\n",
    "           save_prefix='',\n",
    "           save_format='png'):\n",
    "        return MixedNumpyArrayIterator(\n",
    "        x,\n",
    "        y,\n",
    "        self,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=seed,\n",
    "        data_format=self.data_format,\n",
    "        save_to_dir=save_to_dir,\n",
    "        save_prefix=save_prefix,\n",
    "        save_format=save_format)\n",
    "\n",
    "\n",
    "class MixedNumpyArrayIterator(NumpyArrayIterator):\n",
    "    \"\"\"Iterator yielding data from a Numpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               x,\n",
    "               y,\n",
    "               image_data_generator,\n",
    "               **kwargs):\n",
    "        self.x_misc = None\n",
    "        if (type(x) is list) or (type(x) is tuple):\n",
    "            super(MixedNumpyArrayIterator, self).__init__(x[0],\n",
    "                   y,\n",
    "                   image_data_generator,\n",
    "                   **kwargs)\n",
    "            self.x_misc = [np.asarray(xx) for xx in x[1]]\n",
    "        else:\n",
    "            super(MixedNumpyArrayIterator, self).__init__(x,\n",
    "               y,\n",
    "               image_data_generator,\n",
    "               **kwargs)\n",
    "            \n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x.\n",
    "\n",
    "        Returns:\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            index_array, current_index, current_batch_size = next(\n",
    "              self.index_generator)\n",
    "#         print (index_array)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        batch_x = np.zeros(\n",
    "            tuple([current_batch_size] + list(self.x.shape)[1:]), dtype=K.floatx())\n",
    "        for i, j in enumerate(index_array):\n",
    "            x = self.x[j]\n",
    "#             x_bak = x\n",
    "            x = self.image_data_generator.random_transform(x.astype(K.floatx()))\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "#             print (np.all(x_bak == x))\n",
    "            batch_x[i] = x\n",
    "        if self.x_misc is None:\n",
    "            batch_x = [batch_x]\n",
    "        else:\n",
    "            batch_x_miscs = [xx[index_array] for xx in self.x_misc]\n",
    "            batch_x = [batch_x] + batch_x_miscs\n",
    "        return (batch_x, None, None)\n",
    "    \n",
    "#     def next(self):\n",
    "#         \"\"\"For python 2.x.\n",
    "\n",
    "#         Returns:\n",
    "#             The next batch.\n",
    "#         \"\"\"\n",
    "#         with self.lock:\n",
    "#             index_array, current_index, current_batch_size = next(\n",
    "#               self.index_generator)\n",
    "# #         print (index_array)\n",
    "#         # The transformation of images is not under thread lock\n",
    "#         # so it can be done in parallel\n",
    "#         batch_x = np.zeros(\n",
    "#             tuple([current_batch_size] + list(self.x.shape)[1:]), dtype=K.floatx())\n",
    "#         for i, j in enumerate(index_array):\n",
    "#             x = self.x[j]\n",
    "# #             x_bak = x\n",
    "#             x = self.image_data_generator.random_transform(x.astype(K.floatx()))\n",
    "#             x = self.image_data_generator.standardize(x)\n",
    "# #             print (np.all(x_bak == x))\n",
    "#             batch_x[i] = x\n",
    "#         if self.x_misc is None:\n",
    "#             batch_x = [batch_x]\n",
    "#         else:\n",
    "#             batch_x_miscs = [xx[index_array] for xx in self.x_misc]\n",
    "#             batch_x = [batch_x] + batch_x_miscs\n",
    "#         return (batch_x, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over all training size:\n",
      "(119145, 10)\n",
      "Fold:  0\n",
      "Seen unseen Classes:  292 73\n",
      "Seen round1, round2:  163 129\n",
      "Unseen round1, round2:  42 31\n",
      "WARNING:tensorflow:Output \"activation_150\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"activation_150\" during training.\n",
      "WARNING:tensorflow:Output \"attr_x_img_model\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"attr_x_img_model\" during training.\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "wv (InputLayer)                  (None, 900)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchNo (None, 900)           3600        wv[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_147 (Dense)                (None, 2064)          1859664     batch_normalization_147[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_147 (Activation)      (None, 2064)          0           dense_147[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchNo (None, 2064)          8256        activation_147[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_148 (Dense)                (None, 1548)          3196620     batch_normalization_148[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_148 (Activation)      (None, 1548)          0           dense_148[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchNo (None, 1548)          6192        activation_148[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_149 (Dense)                (None, 1290)          1998210     batch_normalization_149[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_149 (Activation)      (None, 1290)          0           dense_149[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchNo (None, 1290)          5160        activation_149[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_150 (Dense)                (None, 1032)          1332312     batch_normalization_150[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_150 (Activation)      (None, 1032)          0           dense_150[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "img (InputLayer)                 (None, 1032)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "attr_x_img (Lambda)              (None, 1032)          0           activation_150[0][0]             \n",
      "                                                                   img[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "attr_x_img_model (Model)         (None, 1)             5161        attr_x_img[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 8,415,175\n",
      "Trainable params: 8,401,507\n",
      "Non-trainable params: 13,668\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "-----DNN training-----\n",
      "DEM BC Data Train Len, Pos, Neg: 95144 95549.0 94739\n",
      "Train on 190288 samples, validate on 24001 samples\n",
      "Epoch 1/1\n",
      "32/73 [============>.................] - ETA: 1s ETA: 0s - loss: 1.9570\n",
      "\n",
      "Unseen_class: \t0.130661\t3136\t24001\n",
      "Unseen_round2_id: \t0.130497\t816\t6253\n",
      "190288/190288 [==============================] - 146s - loss: 1.9570 - val_loss: 2.4351\n",
      "190288/190288 [==============================] - 15s    \n",
      "23744/24001 [============================>.] - ETA: 0s-----LGBM training-----\n",
      "lightgbm params: {'boosting_type': 'gbdt', 'num_leaves': 7, 'learning_rate': 0.025, 'early_stopping_round': 1500, 'verbose': 0, 'objective': 'binary', 'num_boost_round': 1500, 'max_depth': 3, 'task': 'train', 'metric': ['auc', 'binary_logloss']}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/engine.py:104: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1500 rounds.\n",
      "[200]\ttraining's binary_logloss: 0.693126\ttraining's auc: 0.501737\tvalid_1's binary_logloss: 0.689711\tvalid_1's auc: 1\n",
      "[400]\ttraining's binary_logloss: 0.693119\ttraining's auc: 0.501873\tvalid_1's binary_logloss: 0.690008\tvalid_1's auc: 1\n",
      "[600]\ttraining's binary_logloss: 0.693114\ttraining's auc: 0.501945\tvalid_1's binary_logloss: 0.690206\tvalid_1's auc: 1\n",
      "[800]\ttraining's binary_logloss: 0.69311\ttraining's auc: 0.501999\tvalid_1's binary_logloss: 0.690377\tvalid_1's auc: 1\n",
      "[1000]\ttraining's binary_logloss: 0.693107\ttraining's auc: 0.502035\tvalid_1's binary_logloss: 0.690474\tvalid_1's auc: 1\n",
      "[1200]\ttraining's binary_logloss: 0.693105\ttraining's auc: 0.502061\tvalid_1's binary_logloss: 0.690523\tvalid_1's auc: 1\n",
      "[1400]\ttraining's binary_logloss: 0.693103\ttraining's auc: 0.50208\tvalid_1's binary_logloss: 0.690578\tvalid_1's auc: 1\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\ttraining's binary_logloss: 0.693102\ttraining's auc: 0.502087\tvalid_1's binary_logloss: 0.690611\tvalid_1's auc: 1\n",
      "32/73 [============>.................] - ETA: 0s\n",
      "\n",
      "Unseen_class: \t0.014124\t339\t24001\n",
      "Unseen_round2_id: \t0.010395\t65\t6253\n",
      "Fold:  1\n",
      "Seen unseen Classes:  292 73\n",
      "Seen round1, round2:  160 132\n",
      "Unseen round1, round2:  45 28\n",
      "WARNING:tensorflow:Output \"activation_155\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"activation_155\" during training.\n",
      "WARNING:tensorflow:Output \"attr_x_img_model\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"attr_x_img_model\" during training.\n",
      "-----DNN training-----\n",
      "DEM BC Data Train Len, Pos, Neg: 95630 96033.0 95227\n",
      "Train on 191260 samples, validate on 23515 samples\n",
      "Epoch 1/1\n",
      " 77056/191260 [===========>..................] - ETA: 37s - loss: 2.1483"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-fa4608752ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    590\u001b[0m        \u001b[0mround1_class_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround1_class_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m        \u001b[0mround2_class_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround2_class_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m        img_model = img_flat_model)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-fa4608752ef1>\u001b[0m in \u001b[0;36mtrain_zs_model\u001b[0;34m(train_data, class_id_emb_attr, flags, img_flat_len, round1_class_id, round2_class_id, img_model)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_fold\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mzs_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mzs_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_part_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_part_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-fa4608752ef1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_part_df, validate_part_df)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             h = self.model.fit(DNN_Train_Data,  validation_data = (DNN_validate_Data, None),\n\u001b[0;32m--> 460\u001b[0;31m                         epochs=self.epochs, batch_size = self.batch_size, shuffle=True, verbose = self.verbose, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1193\u001b[0m           \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2477\u001b[0;31m         **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2478\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class AccuracyEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1, batch_interval = 1000000, verbose = 2, \\\n",
    "            scores = [], cand_class_id_emb_attr = None, eval_df = None, threshold = None, \\\n",
    "                 seen_class = None, unseen_class = None, gamma = None, model_type = None, \n",
    "                 class_id_dict = None, class_to_id = None, TTA = None, img_model = None):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        # print (validation_data)\n",
    "#         self.X_val, _, \n",
    "        self.y_val = validation_data[-1]\n",
    "        if model_type == 'DEM_BC':\n",
    "            self.y_val = validation_data[0]\n",
    "        elif model_type == 'QFSL':\n",
    "            self.y_val = validation_data[0]\n",
    "        self.verbose = verbose\n",
    "        self.scores = scores\n",
    "        self.cand_class_id_emb_attr = cand_class_id_emb_attr\n",
    "        self.eval_df = eval_df\n",
    "        self.seen_class = seen_class\n",
    "        self.unseen_class = unseen_class\n",
    "        self.model_type = model_type\n",
    "        self.class_id_dict = class_id_dict\n",
    "        self.class_to_id = class_to_id\n",
    "        self.TTA = TTA\n",
    "        self.img_model = img_model\n",
    "#         self.class_id_dict['All'] = self.eval_df.class_id.unique()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            model_eval(self.model, self.model_type, self.eval_df, self.cand_class_id_emb_attr, \n",
    "#                 seen_class = self.seen_class, \n",
    "#                 unseen_class = self.unseen_class, \n",
    "                img_feature_map = self.y_val,\n",
    "                class_id_dict = self.class_id_dict,\n",
    "                class_to_id = self.class_to_id,\n",
    "                TTA = self.TTA,\n",
    "                img_model = self.img_model)\n",
    "\n",
    "class DEM:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, scores = None, flags = None, model_type = None, seen_class = None, \n",
    "            unseen_class = None, class_id_emb_attr = None, img_flat_len = None, \n",
    "                    unseen_round1_id = None,\n",
    "                    unseen_round2_id = None,\n",
    "                    img_model = None,\n",
    "                    only_emb = False):\n",
    "        self.batch_size = 64 #flags.dem_batch_size\n",
    "        self.epochs = 1 #flags.dem_epochs\n",
    "        self.patience = 100 #flags.dem_patience\n",
    "        self.scores = scores\n",
    "        self.model_type = model_type\n",
    "        self.verbose = 1 #flags.train_verbose\n",
    "        self.seen_class = seen_class\n",
    "        self.unseen_class = unseen_class\n",
    "        self.cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)]\n",
    "        self.class_id_emb_attr = class_id_emb_attr\n",
    "        class_ids = class_id_emb_attr.class_id.values\n",
    "        self.class_to_id = dict([(c, i) for i, c in enumerate(class_ids)])\n",
    "        self.img_flat_len = img_flat_len\n",
    "        self.TTA = None #flags.TTA\n",
    "        self.img_model = img_model\n",
    "        self.only_emb = only_emb\n",
    "        if model_type == 'DEM':\n",
    "            self.model = self.create_dem(img_flat_len = img_flat_len)\n",
    "        elif model_type == 'GCN':\n",
    "            self.model = self.create_gcn(img_flat_len = img_flat_len)\n",
    "        elif model_type == 'I2A':\n",
    "            self.model = self.create_img2attr(img_flat_len = img_flat_len)\n",
    "        elif model_type == 'AE':\n",
    "            self.model = self.create_ae(img_flat_len = img_flat_len)\n",
    "        elif model_type == 'DEM_AUG':\n",
    "            self.model = self.create_dem_aug(img_flat_len = img_flat_len)\n",
    "        elif model_type == 'DEM_BC':\n",
    "            self.model = self.create_dem_bc(img_flat_len = img_flat_len)\n",
    "        elif model_type == 'QFSL':\n",
    "            self.model = self.create_qfsl(img_flat_len = img_flat_len)\n",
    "        self.class_id_dict = {\n",
    "#                              'seen_class': seen_class,\n",
    "                             'Unseen_class': unseen_class,\n",
    "#                              'Unseen_round1_id': unseen_round1_id,\n",
    "                             'Unseen_round2_id': unseen_round2_id,}\n",
    "\n",
    "    def create_dem(self, kernel_initializer = 'he_normal', img_flat_len = 1024):\n",
    "        attr_input = layers.Input(shape = (50,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (900,), name = 'wv')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "        \n",
    "        attr_dense = layers.Dense(600, use_bias = True, kernel_initializer=kernel_initializer, \n",
    "                        kernel_regularizer = l2(1e-4), name = 'attr_dense')(attr_input)\n",
    "        attr_word_emb = layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "                                                                            int(img_flat_len * 2),\n",
    "                                                                            int(img_flat_len * 1.5), \n",
    "                                                                            int(img_flat_len * 1.25), \n",
    "#                                                                             int(img_flat_len * 1.125),\n",
    "#                                                                             int(img_flat_len * 1.0625)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb_dense, hidden_dim = [img_flat_len], \n",
    "                                                activation = 'relu')\n",
    "\n",
    "        mse_loss = K.mean(mean_squared_error(imag_classifier, attr_word_emb_dense))\n",
    "        \n",
    "        model = Model([attr_input, word_emb, imag_classifier], outputs = attr_word_emb_dense) #, vgg_output])\n",
    "        model.add_loss(mse_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "        return model\n",
    "\n",
    "    def create_dem_bc(self, kernel_initializer = 'he_normal', img_flat_len = 1024):\n",
    "        attr_input = layers.Input(shape = (50,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (900,), name = 'wv')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "        label = layers.Input(shape = (1,), name = 'label')\n",
    "        \n",
    "        attr_dense = layers.Dense(600, use_bias = True, kernel_initializer=kernel_initializer, \n",
    "                        kernel_regularizer = l2(1e-4), name = 'attr_dense')(attr_input)\n",
    "        if self.only_emb:\n",
    "            attr_word_emb = word_emb\n",
    "        else:\n",
    "            attr_word_emb = layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "                                                                            int(img_flat_len * 2),\n",
    "                                                                            int(img_flat_len * 1.5), \n",
    "                                                                            int(img_flat_len * 1.25), \n",
    "#                                                                             int(img_flat_len * 1.125),\n",
    "#                                                                             int(img_flat_len * 1.0625)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = None)\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb_dense, hidden_dim = [img_flat_len], \n",
    "                                                activation = 'relu')\n",
    "        \n",
    "        attr_x_img = layers.Lambda(lambda x: x[0] * x[1], name = 'attr_x_img')([attr_word_emb_dense, imag_classifier])\n",
    "#         attr_x_img = layers.Concatenate(name = 'attr_x_img')([attr_word_emb_dense, imag_classifier])\n",
    "    \n",
    "        attr_img_input = layers.Input(shape = (img_flat_len,), name = 'attr_img_input')\n",
    "#         attr_img_input = layers.Input(shape = (img_flat_len * 2,), name = 'attr_img_input')\n",
    "        proba = self.full_connect_layer(attr_img_input, hidden_dim = [1], activation = 'sigmoid')\n",
    "        attr_img_model = Model(inputs = attr_img_input, outputs = proba, name = 'attr_x_img_model')\n",
    "        \n",
    "        out = attr_img_model([attr_x_img])\n",
    "        \n",
    "        bc_loss = K.mean(binary_crossentropy(label, out))\n",
    "        model = Model([imag_classifier, attr_input, word_emb, label], outputs = [attr_word_emb_dense, out])\n",
    "        model.add_loss(bc_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "        return model\n",
    "    \n",
    "    def create_dem_aug(self, kernel_initializer = 'he_normal', img_flat_len = 1024):\n",
    "        attr_input = layers.Input(shape = (50,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (600,), name = 'wv')\n",
    "        img_input = layers.Input(shape = (64, 64, 3))\n",
    "#         imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "\n",
    "        img_model = DenseNet(scores = None, \n",
    "                     cat_max = 365, #flags.cat_max, \n",
    "                     flags = None, \n",
    "                     model_type = 'DenseNet').model\n",
    "#         print ('Load DenseNet Weights---')\n",
    "        img_model.load_weights(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05108_TF14_DensetNetProcess/imgmodel_20181013_092715.h5')\n",
    "        img_flat_model = Model(inputs = img_model.inputs, outputs = img_model.get_layer(name = 'avg_pool').output)\n",
    "        img_flat_model.trainable = False\n",
    "        imag_classifier = img_flat_model(img_input)\n",
    "        \n",
    "        attr_dense = layers.Dense(600, use_bias = True, kernel_initializer=kernel_initializer, \n",
    "                        kernel_regularizer = l2(1e-4), name = 'attr_dense')(attr_input)\n",
    "        attr_word_emb = layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "                                                                            int(img_flat_len * 2),\n",
    "                                                                            int(img_flat_len * 1.5), \n",
    "                                                                            int(img_flat_len * 1.25), \n",
    "#                                                                             int(img_flat_len * 1.125),\n",
    "#                                                                             int(img_flat_len * 1.0625)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb_dense, hidden_dim = [img_flat_len], \n",
    "                                                activation = 'relu')\n",
    "\n",
    "        mse_loss = K.mean(mean_squared_error(imag_classifier, attr_word_emb_dense))\n",
    "        \n",
    "        model = Model([img_input, attr_input, word_emb], outputs = [attr_word_emb_dense, imag_classifier]) #, vgg_output])\n",
    "        model.add_loss(mse_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "        return model\n",
    "    \n",
    "    def create_img2attr(self, kernel_initializer = 'he_normal', img_flat_len = 1024):\n",
    "        attr_input = layers.Input(shape = (50,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (600,), name = 'wv')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "\n",
    "        attr_dense = layers.Dense(600, use_bias = True, kernel_initializer=kernel_initializer, \n",
    "                        kernel_regularizer = l2(1e-4), name = 'attr_dense')(attr_input)\n",
    "        attr_word_emb = layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        out_size = 50\n",
    "        \n",
    "        attr_preds = self.full_connect_layer(imag_classifier, hidden_dim = [\n",
    "                                                                            int(out_size * 20),\n",
    "                                                                            int(out_size * 15), \n",
    "#                                                                             int(out_size * 7), \n",
    "#                                                                             int(img_flat_len * 1.125),\n",
    "#                                                                             int(img_flat_len * 1.0625)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "        attr_preds = self.full_connect_layer(attr_preds, hidden_dim = [out_size], activation = 'sigmoid')\n",
    "        log_loss = K.mean(binary_crossentropy(attr_input, attr_preds))\n",
    "        \n",
    "        model = Model([attr_input, word_emb, imag_classifier], outputs = [attr_preds]) #, vgg_output])\n",
    "        model.add_loss(log_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-5), loss=None)\n",
    "        return model\n",
    "    \n",
    "    def create_ae(self, kernel_initializer = 'he_normal', img_flat_len = 1024):\n",
    "        gamma = 0.5\n",
    "        attr_input = layers.Input(shape = (50,), name = 'attr')\n",
    "        word_emb = layers.Input(shape = (600,), name = 'wv')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "\n",
    "        attr_dense = layers.Dense(600, use_bias = True, kernel_initializer=kernel_initializer, \n",
    "                        kernel_regularizer = l2(1e-4), name = 'attr_dense')(attr_input)\n",
    "#         attr_dense = self.full_connect_layer(attr_dense, hidden_dim = [int(img_flat_len * 1.5), \n",
    "#                                                                             int(img_flat_len * 1.25), \n",
    "# #                                                                             int(img_flat_len * 1.125),\n",
    "# #                                                                               int(img_flat_len * 0.5)\n",
    "#                                                                             ], \\\n",
    "#                                                 activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "        attr_word_emb = layers.Concatenate(name = 'attr_word_emb')([word_emb, attr_dense])\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "                                                                            int(img_flat_len * 2),\n",
    "                                                                            int(img_flat_len * 1.5), \n",
    "                                                                            int(img_flat_len * 1.25), \n",
    "#                                                                             int(img_flat_len * 1.125),\n",
    "#                                                                             int(img_flat_len * 1.0625)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "        attr_word_emb_dense = self.full_connect_layer(attr_word_emb_dense, hidden_dim = [img_flat_len], \n",
    "                                                activation = 'relu')\n",
    "\n",
    "        mse_loss = K.mean(mean_squared_error(imag_classifier, attr_word_emb_dense))\n",
    "        \n",
    "        out_size = 50\n",
    "        attr_preds = self.full_connect_layer(attr_word_emb_dense, hidden_dim = [\n",
    "                                                                            int(out_size * 20),\n",
    "                                                                            int(out_size * 15), \n",
    "                                                                            int(out_size * 7), \n",
    "#                                                                             int(img_flat_len * 1.125),\n",
    "#                                                                             int(img_flat_len * 1.0625)\n",
    "                                                                            ], \\\n",
    "                                                activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "        attr_preds = self.full_connect_layer(attr_preds, hidden_dim = [out_size], activation = 'sigmoid')\n",
    "        log_loss = K.mean(binary_crossentropy(attr_input, attr_preds))\n",
    "        \n",
    "        loss = (1 - gamma) * mse_loss + gamma * log_loss\n",
    "        \n",
    "        model = Model([attr_input, word_emb, imag_classifier], outputs = [attr_word_emb_dense, attr_preds])\n",
    "        model.add_loss(loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "        return model\n",
    "    \n",
    "    def create_gcn(self, img_flat_len = 1024):\n",
    "        adj_graph = 1 - sklearn.metrics.pairwise.pairwise_distances(\n",
    "            np.array(list(self.class_id_emb_attr['emb']))[:, :300], metric = 'cosine')\n",
    "        attr_input = layers.Input(tensor=\n",
    "                            tf.constant(np.array(list(self.class_id_emb_attr['attr']), \n",
    "                                                 dtype = 'float32')))\n",
    "        all_word_emb = layers.Input(tensor=\n",
    "                        tf.constant(extract_array_from_series(self.class_id_emb_attr['emb']), \n",
    "                                    dtype = 'float32')) #Input(shape = (230, 300,), name = 'wv')\n",
    "        class_index = layers.Input(shape = (1, ), name = 'class_index', dtype = 'int32')\n",
    "        adj_graphs = layers.Input(tensor=tf.constant(adj_graph, dtype = 'float32')) #Input(shape = (230, 230,), name = 'adj_graph')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "\n",
    "        attr_dense = layers.Dense(600, use_bias = False, kernel_initializer='he_normal', \n",
    "                        kernel_regularizer = l2(1e-4))(attr_input)\n",
    "        attr_word_emb = layers.Concatenate()([all_word_emb, attr_dense])\n",
    "        \n",
    "        all_classifier = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "                                                                        int(img_flat_len * 2),\n",
    "#                                                                         int(img_flat_len * 1.5), \n",
    "#                                                                         int(img_flat_len * 1.25 ),\n",
    "#                                                                         img_flat_len\n",
    "                                                                            ], \n",
    "                                    activation = 'relu', adj_graphs = adj_graphs, drop_out_ratio = 0.2)\n",
    "        all_classifier = self.full_connect_layer(all_classifier, hidden_dim = [img_flat_len], \n",
    "                                    activation = 'relu', adj_graphs = adj_graphs)\n",
    "        x = tf.gather_nd(all_classifier, class_index)\n",
    "\n",
    "        mse_loss = K.mean(mean_squared_error(imag_classifier, x))\n",
    "\n",
    "        model = Model([class_index, imag_classifier, attr_input, all_word_emb, adj_graphs], \n",
    "                      outputs = [all_classifier]) #, vgg_output])\n",
    "        model.add_loss(mse_loss)\n",
    "        model.compile(optimizer=Adam(lr=5e-4), loss=None)\n",
    "    #     model.summary()\n",
    "        return model\n",
    "\n",
    "    def create_qfsl(self, img_flat_len = 1024):\n",
    "#         adj_graph = 1 - sklearn.metrics.pairwise.pairwise_distances(\n",
    "#             np.array(list(self.class_id_emb_attr['emb']))[:, :300], metric = 'cosine')\n",
    "        class_num = self.class_id_emb_attr.shape[0]\n",
    "        print ('Class name in model: ', class_num)\n",
    "        attr_input = layers.Input(tensor=\n",
    "                            tf.constant(np.array(list(self.class_id_emb_attr['attr']), \n",
    "                                                 dtype = 'float32')), name = 'attr')\n",
    "        all_word_emb = layers.Input(tensor=\n",
    "                        tf.constant(extract_array_from_series(self.class_id_emb_attr['emb']), \n",
    "                                    dtype = 'float32'), name = 'wv') #Input(shape = (230, 300,), name = 'wv')\n",
    "        classes = layers.Input(shape = (class_num, ), name = 'classes')\n",
    "        imag_classifier = layers.Input(shape = (img_flat_len,), name = 'img')\n",
    "\n",
    "        attr_dense = layers.Dense(600, use_bias = False, kernel_initializer='he_normal', \n",
    "                        kernel_regularizer = l2(1e-4))(attr_input)\n",
    "        attr_word_emb = all_word_emb #layers.Concatenate()([all_word_emb, attr_input])\n",
    "        attr_word_emb_size = 600\n",
    "        \n",
    "        all_classifier = self.full_connect_layer(attr_word_emb, hidden_dim = [\n",
    "                                                                        int(img_flat_len * 2),\n",
    "#                                                                         int(img_flat_len * 1.5), \n",
    "#                                                                         int(img_flat_len * 1.25 ),\n",
    "#                                                                         img_flat_len\n",
    "                                                                            ], \n",
    "                                    activation = 'relu', drop_out_ratio = 0.2)\n",
    "        all_classifier = self.full_connect_layer(all_classifier, hidden_dim = [img_flat_len], \n",
    "                                    activation = 'relu')\n",
    "        \n",
    "        attr_emb_from_img = self.full_connect_layer(imag_classifier, hidden_dim = [\n",
    "                                                                        int(attr_word_emb_size * 2),\n",
    "#                                                                         int(attr_word_emb_size * 1.5), \n",
    "#                                                                         int(attr_word_emb_size * 1.25 ),\n",
    "                                                                        attr_word_emb_size\n",
    "                                                                            ], \n",
    "                                    activation = 'relu')\n",
    "        scoring_sub = layers.Lambda(lambda x: K.dot(x[0], K.transpose(x[1])), \n",
    "                             name = 'scoring_sub')([attr_emb_from_img, attr_word_emb])\n",
    "        out = self.full_connect_layer(scoring_sub, hidden_dim = [class_num], \n",
    "                                    activation = 'softmax')\n",
    "\n",
    "        log_loss = K.mean(K.categorical_crossentropy(classes, out))\n",
    "\n",
    "        model = Model([imag_classifier, classes, attr_input, all_word_emb], outputs = [out])\n",
    "        model.add_loss(log_loss)\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss=None)\n",
    "    #     model.summary()\n",
    "        return model\n",
    "    \n",
    "    def full_connect_layer(self, input, hidden_dim, activation, resnet = False, adj_graphs = None, \n",
    "                        drop_out_ratio = None, kernel_initializer = 'he_normal'):\n",
    "        full_connect = input\n",
    "        for i, hn in enumerate(hidden_dim):\n",
    "            fc_in = full_connect\n",
    "            if drop_out_ratio is not None:\n",
    "                full_connect = layers.Dropout(drop_out_ratio)(full_connect)\n",
    "            full_connect = layers.BatchNormalization(epsilon=1.001e-5)(full_connect)\n",
    "            full_connect = layers.Dense(hn, kernel_initializer=kernel_initializer, kernel_regularizer = l2(1e-4), \n",
    "                    activation = None)(full_connect)\n",
    "            if adj_graphs is not None:\n",
    "                full_connect = layers.Lambda(lambda x: K.dot(x[1], x[0]))([full_connect, adj_graphs])\n",
    "            full_connect = layers.Activation(activation)(full_connect)\n",
    "            if resnet:\n",
    "                full_connect = layers.Concatenate()([fc_in, full_connect])\n",
    "        return full_connect\n",
    "\n",
    "    def DNN_DataSet(self, df, neg_aug = 0):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.model_type == 'DEM' or self.model_type == 'I2A' or self.model_type == 'AE':\n",
    "#             return create_dem_data(df) + [extract_array_from_series(df['target'])]\n",
    "            return create_dem_data(df) + [sklearn.preprocessing.normalize(extract_array_from_series(df['target']))]\n",
    "        elif self.model_type == 'DEM_AUG':\n",
    "            return [preprocess_img(df['img'])] + create_dem_data(df)\n",
    "        elif self.model_type == 'GCN':\n",
    "            return [create_gcn_data(df, self.class_to_id), \n",
    "                    sklearn.preprocessing.normalize(extract_array_from_series(df['target']))]\n",
    "        elif self.model_type == 'DEM_BC':\n",
    "            return create_dem_bc_data(df, neg_aug, self.only_emb)\n",
    "        elif self.model_type == 'QFSL':\n",
    "            return create_qfsl_data(df, self.class_to_id)\n",
    "\n",
    "    def lgbm_train(self, train_part, train_part_label, valide_part, valide_part_label, fold_seed = None,\n",
    "        fold = 5, train_weight = None, valide_weight = None, flags = None):\n",
    "        \"\"\"\n",
    "        LGBM Training\n",
    "        \"\"\"\n",
    "        print(\"-----LGBM training-----\")\n",
    "\n",
    "        d_train = lgb.Dataset(train_part, train_part_label)\n",
    "        d_valide = lgb.Dataset(valide_part, valide_part_label)\n",
    "        params = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt', #'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': ['auc', 'binary_logloss'],\n",
    "                'num_leaves': 7, #60, #40, # 60,\n",
    "#                 'min_sum_hessian_in_leaf': 10,\n",
    "                'max_depth': 3,#12, #6, # 10,\n",
    "                'learning_rate': 0.025, # 0.025,\n",
    "               # 'feature_fraction': 0.5,#0.35, # 0.6\n",
    "                'verbose': 0,\n",
    "                'num_boost_round': 1500, #361,\n",
    "#                 'feature_fraction_seed': fold_seed,\n",
    "                #'drop_rate': 0.05,\n",
    "                # 'bagging_fraction': 0.8,\n",
    "                # 'bagging_freq': 20,\n",
    "                # 'bagging_seed': fold_seed,\n",
    "                 'early_stopping_round': 1500,\n",
    "                # 'random_state': 10\n",
    "                # 'verbose_eval': 20\n",
    "                #'min_data_in_leaf': 665\n",
    "            }\n",
    "#         params.update(config.all_params)\n",
    "        print (\"lightgbm params: {0}\\n\".format(params))\n",
    "\n",
    "        bst = lgb.train(\n",
    "                        params ,\n",
    "                        d_train,\n",
    "                        verbose_eval = 200,\n",
    "                        valid_sets = [d_train, d_valide],\n",
    "                        # feature_name= keras_train.DENSE_FEATURE_LIST,\n",
    "                        #feval = gini_lgbm\n",
    "                        #num_boost_round = 1\n",
    "                        )\n",
    "        return bst\n",
    "\n",
    "    def train(self, train_part_df, validate_part_df):\n",
    "        \"\"\"\n",
    "        Keras Training\n",
    "        \"\"\"\n",
    "        print(\"-----DNN training-----\")\n",
    "\n",
    "        DNN_Train_Data = self.DNN_DataSet(train_part_df, neg_aug = 1)\n",
    "        DNN_validate_Data = self.DNN_DataSet(validate_part_df)\n",
    "        \n",
    "        callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=self.patience, verbose=0),\n",
    "        AccuracyEvaluation(validation_data=DNN_validate_Data, interval=1,\n",
    "                            cand_class_id_emb_attr = self.cand_class_id_emb_attr,\n",
    "                            eval_df = validate_part_df,\n",
    "                            model_type = self.model_type,\n",
    "                            class_id_dict = self.class_id_dict,\n",
    "                            class_to_id = self.class_to_id,\n",
    "                            TTA = self.TTA,\n",
    "                            img_model = self.img_model)\n",
    "        ]\n",
    "        if self.model_type == 'DEM_AUG':\n",
    "            datagen = MixedImageDataGenerator(\n",
    "                    rotation_range=20,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True)\n",
    "            datagen.fit(DNN_Train_Data[0])\n",
    "#             print (DNN_Train_Data[0])\n",
    "            h = self.model.fit_generator(\n",
    "                    datagen.flow((DNN_Train_Data[0], DNN_Train_Data[1:]), None, batch_size=self.batch_size), \n",
    "                    validation_data=(DNN_validate_Data, None), steps_per_epoch = DNN_Train_Data[0].shape[0]//self.batch_size,\n",
    "                    epochs=self.epochs, shuffle=True, verbose = self.verbose, workers=1, use_multiprocessing=False, \n",
    "                    callbacks=callbacks)\n",
    "#         elif self.model_type == 'DEM_BC':\n",
    "#             h = self.model.fit(DNN_Train_Data[0],  DNN_Train_Data[1], validation_data = DNN_validate_Data,\n",
    "#                         epochs=self.epochs, batch_size = self.batch_size, shuffle=True, verbose = self.verbose, callbacks=callbacks)\n",
    "        else:\n",
    "            h = self.model.fit(DNN_Train_Data,  validation_data = (DNN_validate_Data, None),\n",
    "                        epochs=self.epochs, batch_size = self.batch_size, shuffle=True, verbose = self.verbose, callbacks=callbacks)\n",
    "        self.scores.append(pd.DataFrame(h.history))\n",
    "        \n",
    "        zs_model = Model(inputs = self.model.inputs[1:3], outputs = self.model.outputs[0])\n",
    "        train_attr_x_img = zs_model.predict(DNN_Train_Data[1:3], verbose = 1)\n",
    "        train_label = DNN_Train_Data[-1].flatten()\n",
    "        validate_attr_x_img = zs_model.predict(DNN_validate_Data[1:3], verbose = 1)\n",
    "        validate_label = DNN_validate_Data[-1].flatten()\n",
    "#         print (train_label)\n",
    "#         print (validate_label)\n",
    "        bst = self.lgbm_train(train_attr_x_img, train_label, validate_attr_x_img, validate_label)\n",
    "        \n",
    "        cand_feature_map = zs_model.predict(create_dem_data(self.cand_class_id_emb_attr, only_emb = True), verbose = 1)\n",
    "        pred = find_nearest_class(self.cand_class_id_emb_attr, \n",
    "                                  validate_part_df, \n",
    "                                  cand_feature_map = cand_feature_map, \n",
    "                                img_feature_map = DNN_validate_Data[0],\n",
    "                                zs_model = bst, model_type = self.model_type)\n",
    "        calc_detailed_accuracy(validate_part_df, pred, self.class_id_dict)\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    def predict(self, test_part, batch_size = 1024, verbose=2):\n",
    "        \"\"\"\n",
    "        Keras Training\n",
    "        \"\"\"\n",
    "        print(\"-----DNN Test-----\")\n",
    "        pred = self.model.predict(self.DNN_DataSet(test_part), verbose=verbose)\n",
    "        if self.model_type == 'r':\n",
    "            pred = pred[:, -1]\n",
    "        return pred\n",
    "\n",
    "def train_zs_model(train_data, class_id_emb_attr, flags, img_flat_len,\n",
    "                   round1_class_id = None,\n",
    "                   round2_class_id = None,\n",
    "                   img_model = None):\n",
    "    print(\"Over all training size:\")\n",
    "    print(train_data.shape)\n",
    "\n",
    "    fold = 5 #flags.dem_nfold\n",
    "    ensemble_nfold = 10 #flags.dem_ensemble_nfold\n",
    "    kf = KFold(n_splits=fold, shuffle=True, random_state = 100)\n",
    "    num_fold = 0\n",
    "    models = []\n",
    "    model_type = 'DEM_BC'\n",
    "    scores = []\n",
    "    classes = train_data.class_id.unique()\n",
    "    model_file_names_0 = glob.glob(path + '../submit/image_05108_10fold_zs_DEMBC_02161/zsmodel_*.h5')\n",
    "    model_file_names_1 = glob.glob(path + '../submit/image_05108_10fold_zs_DEMBC_negaug5_/zsmodel_*.h5')\n",
    "    model_file_names_2 = glob.glob(path + '../submit/image_05108_10fold_zs_DEMBC_negaug2_/zsmodel_*.h5')\n",
    "    model_file_names = [model_file_names_0, model_file_names_1, model_file_names_2]\n",
    "    \n",
    "    for train_index, test_index in kf.split(classes):\n",
    "        print ('Fold: ', num_fold)\n",
    "        seen_class = classes[train_index]\n",
    "        unseen_class = classes[test_index]\n",
    "        \n",
    "        train_part_df = train_data[train_data.class_id.isin(seen_class)]\n",
    "        validate_part_df = train_data[train_data.class_id.isin(unseen_class)]\n",
    "\n",
    "        seen_round1_id = np.intersect1d(seen_class, round1_class_id)\n",
    "        seen_round2_id = np.intersect1d(seen_class, round2_class_id)\n",
    "        unseen_round1_id = np.intersect1d(unseen_class, round1_class_id)\n",
    "        unseen_round2_id = np.intersect1d(unseen_class, round2_class_id)\n",
    "        print ('Seen unseen Classes: ', seen_class.shape[0], unseen_class.shape[0])\n",
    "        print ('Seen round1, round2: ', seen_round1_id.shape[0], seen_round2_id.shape[0])\n",
    "        print ('Unseen round1, round2: ', unseen_round1_id.shape[0], unseen_round2_id.shape[0])\n",
    "\n",
    "        zs_model = DEM(scores = scores, flags = flags, model_type = model_type, \n",
    "                    seen_class = seen_class, img_flat_len = img_flat_len, \n",
    "                    unseen_class = unseen_class,\n",
    "                    class_id_emb_attr = class_id_emb_attr,\n",
    "                    unseen_round1_id = unseen_round1_id,\n",
    "                    unseen_round2_id = unseen_round2_id,\n",
    "                    img_model = img_model,\n",
    "                    only_emb = True)\n",
    "        \n",
    "        if num_fold == 0:\n",
    "            print (zs_model.model.summary())\n",
    "        zs_model.train(train_part_df, validate_part_df)\n",
    "        models.append((zs_model.model, model_type))\n",
    "        \n",
    "#         for model_files in model_file_names:\n",
    "#             zs_model = DEM(scores = scores, flags = flags, model_type = model_type, \n",
    "#                         seen_class = seen_class, img_flat_len = img_flat_len, \n",
    "#                         unseen_class = unseen_class,\n",
    "#                         class_id_emb_attr = class_id_emb_attr,\n",
    "#                         unseen_round1_id = unseen_round1_id,\n",
    "#                         unseen_round2_id = unseen_round2_id,\n",
    "#                         img_model = img_model)\n",
    "#             print ('model file name: ', model_files[num_fold])\n",
    "#             zs_model.model.load_weights(model_files[num_fold])\n",
    "#             zs_model.model.trainable = False\n",
    "# #             model_eval(zs_model.model, model_type, validate_part_df, \n",
    "# #                        cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \n",
    "# #                     img_feature_map = extract_array_from_series(validate_part_df['target']),\n",
    "# #                     class_id_dict = {\n",
    "# #     #                              'seen_class': seen_class,\n",
    "# #                                  'Unseen_class': unseen_class,\n",
    "# #     #                              'Unseen_round1_id': unseen_round1_id,\n",
    "# #                                  'Unseen_round2_id': unseen_round2_id,},\n",
    "# #                     class_to_id = dict([(c, i) for i, c in enumerate(class_id_emb_attr.class_id.values)]))\n",
    "#             models.append((zs_model.model, model_type))\n",
    "#         print ('Multi models votes-------')\n",
    "#         multi_models_vote(models = models[-3:], \n",
    "#                       eval_df = validate_part_df,\n",
    "#                     cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)],\n",
    "#                     img_feature_map = extract_array_from_series(validate_part_df['target']), \n",
    "#                     class_id_dict = {\n",
    "#                          'Unseen_class': unseen_class,\n",
    "#                          'Unseen_round2_id': unseen_round2_id,})\n",
    "        \n",
    "        num_fold += 1\n",
    "        if num_fold == ensemble_nfold:\n",
    "            break\n",
    "    return models\n",
    "\n",
    "# img_model = DenseNet(scores = None, \n",
    "#              cat_max = 365, #flags.cat_max, \n",
    "#              flags = None, \n",
    "#              model_type = 'DenseNet').model\n",
    "# img_model.load_weights(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05108_TF14_DensetNetProcess/imgmodel_20181013_092715.h5')\n",
    "# img_flat_model = Model(inputs = img_model.inputs, outputs = img_model.get_layer(name = 'avg_pool').output)\n",
    "img_flat_model = None\n",
    "\n",
    "round1_class_id = list(set(train_data.class_id.unique()) - set(round2_class_id))\n",
    "zs_models = train_zs_model(train_data, #[train_data.class_id.isin(round2_class_id)].iloc[:1000], \n",
    "       class_id_emb_attr = class_id_emb_attr, #[class_id_emb_attr.class_id.isin(round2_class_id)], \n",
    "       flags = None, \n",
    "       img_flat_len = 1032,                     \n",
    "       round1_class_id = round1_class_id,\n",
    "       round2_class_id = round2_class_id,\n",
    "       img_model = img_flat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(round2_class_id)]\n",
    "cand_class_id_emb_attr = cand_class_id_emb_attr[~cand_class_id_emb_attr.class_id.isin(train_data.class_id.unique())]\n",
    "vote_preds = multi_models_vote(models = zs_models[-3:], \n",
    "                      eval_df = test_data,\n",
    "                    cand_class_id_emb_attr = cand_class_id_emb_attr,\n",
    "                    img_feature_map = extract_array_from_series(test_data['target']))\n",
    "sub = pd.DataFrame(vote_preds, index = test_data['img_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_label = time.strftime('%Y%m%d_%H%M%S')\n",
    "sub.to_csv(path + \"../submit/submit_\"+ time_label + \".txt\", header = False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 11915\n",
      "class size  365\n",
      "\n",
      "Initializing search.\n",
      "Initialization finished.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 0               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "Saving model.\n",
      "+--------------------------------------------------------------------------+\n",
      "|        Model ID        |          Loss          |      Metric Value      |\n",
      "+--------------------------------------------------------------------------+\n",
      "|           0            |    70.7620168685913    |  0.009264853977844914  |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 1               |\n",
      "+----------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fd/study/TC/ZJC/sub/code/autokeras/bayesian.py:151: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+--------------------------------------------------------------------------+\n",
      "|    Father Model ID     |                 Added Operation                 |\n",
      "+--------------------------------------------------------------------------+\n",
      "|           0            |           ('to_add_skip_model', 1, 5)           |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "Saving model.\n",
      "+--------------------------------------------------------------------------+\n",
      "|        Model ID        |          Loss          |      Metric Value      |\n",
      "+--------------------------------------------------------------------------+\n",
      "|           1            |   58.29625339508057    |  0.007452165156092649  |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 2               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------+\n",
      "|    Father Model ID     |                 Added Operation                 |\n",
      "+--------------------------------------------------------------------------+\n",
      "|                        |          ('to_conv_deeper_model', 9, 3)         |\n",
      "|                        |          ('to_conv_deeper_model', 1, 3)         |\n",
      "|                        |           ('to_add_skip_model', 5, 9)           |\n",
      "|                        |          ('to_concat_skip_model', 5, 9)         |\n",
      "|           0            |            ('to_wider_model', 14, 64)           |\n",
      "|                        |            ('to_wider_model', 9, 64)            |\n",
      "|                        |            ('to_wider_model', 9, 128)           |\n",
      "|                        |          ('to_conv_deeper_model', 1, 3)         |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "Saving model.\n",
      "+--------------------------------------------------------------------------+\n",
      "|        Model ID        |          Loss          |      Metric Value      |\n",
      "+--------------------------------------------------------------------------+\n",
      "|           2            |   104.62083415985107   |  0.007250755287009064  |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 3               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------+\n",
      "|    Father Model ID     |                 Added Operation                 |\n",
      "+--------------------------------------------------------------------------+\n",
      "|                        |          ('to_conv_deeper_model', 9, 3)         |\n",
      "|                        |         ('to_conv_deeper_model', 18, 3)         |\n",
      "|                        |           ('to_add_skip_model', 5, 18)          |\n",
      "|                        |            ('to_wider_model', 14, 64)           |\n",
      "|                        |          ('to_concat_skip_model', 5, 9)         |\n",
      "|                        |           ('to_add_skip_model', 1, 18)          |\n",
      "|                        |         ('to_concat_skip_model', 1, 18)         |\n",
      "|           0            |           ('to_add_skip_model', 9, 18)          |\n",
      "|                        |          ('to_concat_skip_model', 1, 5)         |\n",
      "|                        |         ('to_concat_skip_model', 5, 18)         |\n",
      "|                        |          ('to_concat_skip_model', 1, 9)         |\n",
      "|                        |         ('to_concat_skip_model', 9, 18)         |\n",
      "|                        |           ('to_add_skip_model', 1, 9)           |\n",
      "|                        |          ('to_conv_deeper_model', 9, 3)         |\n",
      "|                        |           ('to_add_skip_model', 1, 5)           |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "Saving model.\n",
      "+--------------------------------------------------------------------------+\n",
      "|        Model ID        |          Loss          |      Metric Value      |\n",
      "+--------------------------------------------------------------------------+\n",
      "|           3            |   125.46946926116944   |  0.00825780463242699   |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 4               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------+\n",
      "|    Father Model ID     |                 Added Operation                 |\n",
      "+--------------------------------------------------------------------------+\n",
      "|                        |          ('to_conv_deeper_model', 5, 3)         |\n",
      "|                        |         ('to_conv_deeper_model', 18, 3)         |\n",
      "|                        |            ('to_wider_model', 1, 64)            |\n",
      "|                        |           ('to_add_skip_model', 1, 21)          |\n",
      "|                        |          ('to_add_skip_model', 18, 21)          |\n",
      "|           0            |            ('to_wider_model', 1, 128)           |\n",
      "|                        |         ('to_concat_skip_model', 1, 21)         |\n",
      "|                        |          ('to_conv_deeper_model', 1, 3)         |\n",
      "|                        |           ('to_add_skip_model', 1, 5)           |\n",
      "|                        |            ('to_wider_model', 14, 64)           |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "Saving model.\n",
      "+--------------------------------------------------------------------------+\n",
      "|        Model ID        |          Loss          |      Metric Value      |\n",
      "+--------------------------------------------------------------------------+\n",
      "|           4            |   135.45114707946777   |  0.007653575025176234  |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 5               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "Current model size is too big. Discontinuing training this model to search for other models.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 5               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "Current model size is too big. Discontinuing training this model to search for other models.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 5               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "Current model size is too big. Discontinuing training this model to search for other models.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 5               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "Current model size is too big. Discontinuing training this model to search for other models.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 5               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "Current model size is too big. Discontinuing training this model to search for other models.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 5               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "Current model size is too big. Discontinuing training this model to search for other models.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 5               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "Current model size is too big. Discontinuing training this model to search for other models.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 5               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "Current model size is too big. Discontinuing training this model to search for other models.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 5               |\n",
      "+----------------------------------------------+\n",
      "\n",
      "Current model size is too big. Discontinuing training this model to search for other models.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 5               |\n",
      "+----------------------------------------------+\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ad54327e5733>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_part_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcategory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# y_test = test_part_data['class_id'].apply(lambda id: category_dict[id]).values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# results = clf.predict(x_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/TC/ZJC/sub/code/autokeras/image_supervised.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, x_test, y_test, time_limit)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mtime_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m24\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_output_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/TC/ZJC/sub/code/autokeras/cnn_module.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_output_node, input_shape, train_data, test_data, time_limit)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mtime_remain\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0m_run_searcher_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_remain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_searcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mConstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_MODEL_NUM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/TC/ZJC/sub/code/autokeras/cnn_module.py\u001b[0m in \u001b[0;36m_run_searcher_once\u001b[0;34m(train_data, test_data, path, timeout)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msearcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'searcher'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/TC/ZJC/sub/code/autokeras/search.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, train_data, test_data, timeout)\u001b[0m\n\u001b[1;32m    192\u001b[0m                     new_graph, new_father_id = self.bo.optimize_acq(self.search_tree.adj_list.keys(),\n\u001b[1;32m    193\u001b[0m                                                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescriptors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                                                                     remaining_time)\n\u001b[0m\u001b[1;32m    195\u001b[0m                 \u001b[0mnew_model_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/TC/ZJC/sub/code/autokeras/bayesian.py\u001b[0m in \u001b[0;36moptimize_acq\u001b[0;34m(self, model_ids, descriptors, timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0map\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mtemp_graph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcontain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_descriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/TC/ZJC/sub/code/autokeras/net_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtemp_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mtemp_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_deeper_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mtemp_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_wider_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/TC/ZJC/sub/code/autokeras/net_transformer.py\u001b[0m in \u001b[0;36mto_deeper_graph\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_conv_deeper_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense_deeper_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/TC/ZJC/sub/code/autokeras/graph.py\u001b[0m in \u001b[0;36mto_conv_deeper_model\u001b[0;34m(self, target_id, kernel_size)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperation_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'to_conv_deeper_model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mnew_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeeper_conv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0moutput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_block_end_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/TC/ZJC/sub/code/autokeras/layer_transformer.py\u001b[0m in \u001b[0;36mdeeper_conv_block\u001b[0;34m(conv_layer, kernel_size, weighted)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfilter_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mn_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilter_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mcenter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# del test_data\n",
    "import autokeras as ak\n",
    "\n",
    "clf = ak.ImageClassifier(verbose = True)\n",
    "fold = 10 #flags.dem_nfold\n",
    "ensemble_nfold = 10 #flags.dem_ensemble_nfold\n",
    "kf = KFold(n_splits=fold, shuffle=True, random_state = 100)\n",
    "for train_index, test_index in kf.split(train_data):\n",
    "    train_part_data = train_data.iloc[test_index]\n",
    "#     test_part_data = train_data.iloc[test_index]\n",
    "#     del train_data\n",
    "    break\n",
    "print ('train size', train_part_data.shape[0])\n",
    "x_train = preprocess_img(train_part_data['img'])\n",
    "# x_test = preprocess_img(test_part_data['img'])\n",
    "\n",
    "category = train_part_data['class_id'].unique()\n",
    "print ('class size ', category.shape[0])\n",
    "category_dict = dict((category[i], i) for i in range(category.shape[0]))\n",
    "y_train = train_part_data['class_id'].apply(lambda id: category_dict[id]).values\n",
    "# y_test = test_part_data['class_id'].apply(lambda id: category_dict[id]).values\n",
    "clf.fit(x_train, y_train)\n",
    "# results = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28 * 90 / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((2, 2)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.411916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.435035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.444240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.437913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.438824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.441759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.424387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.426253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.425569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.419721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.428164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.417860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.408120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.401526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.408207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.394091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.387690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.380116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.413169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.402638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.408803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.371400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Re\n",
       "Epoch          \n",
       "1      0.411916\n",
       "2      0.435035\n",
       "3      0.444240\n",
       "4      0.437913\n",
       "5      0.438824\n",
       "6      0.441759\n",
       "7      0.424387\n",
       "8      0.426253\n",
       "9      0.425569\n",
       "10     0.419721\n",
       "11     0.428164\n",
       "12     0.417860\n",
       "13     0.408120\n",
       "14     0.401526\n",
       "15     0.408207\n",
       "16     0.394091\n",
       "17     0.387690\n",
       "18     0.380116\n",
       "19     0.413169\n",
       "20     0.402638\n",
       "21     0.408803\n",
       "22     0.371400"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log = pd.read_csv(path + 'log_negaug4_crawl_glove', header = None, names = ['Fold', 'Epoch', 'Re'], sep = ' ')\n",
    "log.groupby('Epoch').agg({'Re': ['mean', 'median']}).plot(grid = True, title = 'mean')\n",
    "log.groupby('Epoch').agg({'Re': 'std'}).plot(grid = True, title = 'std')\n",
    "# log.groupby('Epoch').agg({'Re': 'median'}).plot(grid = True, title = 'median')\n",
    "# log.groupby('Epoch').agg({'Re': 'count'}).plot(grid = True)\n",
    "log.groupby('Epoch').agg({'Re': 'mean'})\n",
    "# log[log.Fold == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cand shape:  16\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.377448\t1195\t3166\n",
      "\n",
      "\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.263108\t833\t3166\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unseen_class: \t0.385028\t1219\t3166\n",
      "\n",
      "\n",
      "cand shape:  16\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.612995\t1934\t3155\n",
      "\n",
      "\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.556894\t1757\t3155\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unseen_class: \t0.584152\t1843\t3155\n",
      "\n",
      "\n",
      "cand shape:  16\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.590740\t1901\t3218\n",
      "\n",
      "\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.631137\t2031\t3218\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unseen_class: \t0.628962\t2024\t3218\n",
      "\n",
      "\n",
      "cand shape:  16\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.572295\t1793\t3133\n",
      "\n",
      "\n",
      "16/16 [==============================] - 0s\n",
      "\n",
      "\n",
      "Unseen_class: \t0.575806\t1804\t3133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unseen_class: \t0.547399\t1715\t3133\n",
      "\n",
      "\n",
      "cand shape:  16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b714bdda1934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     multi_models_vote(models = zs_model_list, eval_df = validate_part_df,             cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr['class_id'].isin(unseen_class)],             img_feature_map = extract_array_from_series(validate_part_df['target']),\n\u001b[1;32m     20\u001b[0m             class_id_dict = {\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;34m'Unseen_class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munseen_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             })\n",
      "\u001b[0;32m<ipython-input-26-6ca63de5e674>\u001b[0m in \u001b[0;36mmulti_models_vote\u001b[0;34m(models, eval_df, cand_class_id_emb_attr, img_feature_map, class_id_dict)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'cand shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_class_id_emb_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     preds = models_eval(models, eval_df, cand_class_id_emb_attr, img_feature_map, \n\u001b[0;32m---> 41\u001b[0;31m                 class_id_dict)\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# print (preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-6ca63de5e674>\u001b[0m in \u001b[0;36mmodels_eval\u001b[0;34m(models, eval_df, cand_class_id_emb_attr, img_feature_map, class_id_dict, class_to_id)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         pred = model_eval(model, model_type, eval_df = eval_df, cand_class_id_emb_attr = cand_class_id_emb_attr, \n\u001b[0;32m---> 81\u001b[0;31m             img_feature_map = img_feature_map, class_id_dict = class_id_dict, class_to_id = class_to_id)\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-6ca63de5e674>\u001b[0m in \u001b[0;36mmodel_eval\u001b[0;34m(model, model_type, eval_df, cand_class_id_emb_attr, img_feature_map, class_id_dict, class_to_id)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DEM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mzs_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mcand_feature_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzs_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_dem_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand_class_id_emb_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_nearest_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand_class_id_emb_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_feature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_feature_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'class_id'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     return self._predict_loop(\n\u001b[0;32m-> 1739\u001b[0;31m         f, ins, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m           \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2477\u001b[0;31m         **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2478\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1291\u001b[0m                 run_metadata):\n\u001b[1;32m   1292\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1354\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def lgbm_train(train_part, train_part_label, valide_part, valide_part_label, fold_seed,\n",
    "        fold = 5, train_weight = None, valide_weight = None, flags = None):\n",
    "    \"\"\"\n",
    "    LGBM Training\n",
    "    \"\"\"\n",
    "    if flags.stacking:\n",
    "        FEATURE_LIST += ['emb_' + str(i) for i in range(len(CATEGORY_FEATURES) * 5)] + ['k_pred']\n",
    "    print(\"-----LGBM training-----\")\n",
    "\n",
    "    d_train = lgb.Dataset(train_part[FEATURE_LIST].values, train_part_label, weight = train_weight, \n",
    "            feature_name = FEATURE_LIST, categorical_feature = CATEGORY_FEATURES,)#, init_score = train_part[:, -1])\n",
    "    d_valide = lgb.Dataset(valide_part[FEATURE_LIST].values, valide_part_label, weight = valide_weight,\n",
    "            feature_name = FEATURE_LIST, categorical_feature = CATEGORY_FEATURES,)#, init_score = valide_part[:, -1])\n",
    "    params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt', #'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'num_leaves': 240, #60, #40, # 60,\n",
    "            'min_sum_hessian_in_leaf': 10,\n",
    "            'max_depth': 50,#12, #6, # 10,\n",
    "            'learning_rate': 0.025, # 0.025,\n",
    "           # 'feature_fraction': 0.5,#0.35, # 0.6\n",
    "            'verbose': 0,\n",
    "            'num_boost_round': 500, #361,\n",
    "            'feature_fraction_seed': fold_seed,\n",
    "            #'drop_rate': 0.05,\n",
    "            # 'bagging_fraction': 0.8,\n",
    "            # 'bagging_freq': 20,\n",
    "            # 'bagging_seed': fold_seed,\n",
    "             'early_stopping_round': 1500,\n",
    "            # 'random_state': 10\n",
    "            # 'verbose_eval': 20\n",
    "            #'min_data_in_leaf': 665\n",
    "        }\n",
    "    params.update(config.all_params)\n",
    "    print (\"lightgbm params: {0}\\n\".format(params))\n",
    "\n",
    "    bst = lgb.train(\n",
    "                    params ,\n",
    "                    d_train,\n",
    "                    verbose_eval = 50,\n",
    "                    valid_sets = [d_train, d_valide],\n",
    "                    # feature_name= keras_train.DENSE_FEATURE_LIST,\n",
    "                    #feval = gini_lgbm\n",
    "                    #num_boost_round = 1\n",
    "                    )\n",
    "    #pred = model_eval(bst, 'l', valide_part)\n",
    "    #print(pred[:10])\n",
    "    #print(valide_part_label[:10])\n",
    "    #print(valide_part[:10, -1])\n",
    "    # exit(0)\n",
    "    feature_imp = bst.feature_importance(importance_type = 'gain')\n",
    "    sort_ind = np.argsort(feature_imp)[::-1]\n",
    "    print (np.c_[np.array(FEATURE_LIST)[sort_ind], feature_imp[sort_ind]])\n",
    "    # print (np.array(keras_train.FEATURE_LIST)[np.argsort(feature_imp)])\n",
    "    # exit(0)\n",
    "    # cv_result = lgb.cv(params, d_train, nfold=fold) #, feval = gini_lgbm)\n",
    "    # pd.DataFrame(cv_result).to_csv('cv_result', index = False)\n",
    "    # exit(0)\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cand shape:  45\n",
      "32/45 [====================>.........] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "def sub(models, train_data, test_data, class_id_emb_attr, img_model, output_model_path):\n",
    "    train_id = train_data['class_id'].unique()\n",
    "    test_img_feature_map = extract_array_from_series(test_data['target'])\n",
    "    preds = multi_models_vote(models = models, eval_df = test_data, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[~class_id_emb_attr['class_id'].isin(train_id)], \\\n",
    "            img_feature_map = test_img_feature_map)\n",
    "    sub = pd.DataFrame(preds, index = test_data['img_id'])\n",
    "    time_label = time.strftime('%Y%m%d_%H%M%S')\n",
    "    tmp_model_dir = \"./model_sub/\"\n",
    "    if not os.path.isdir(tmp_model_dir):\n",
    "        os.makedirs(tmp_model_dir, exist_ok=True)\n",
    "    sub_name = tmp_model_dir + \"/submit_\"+ time_label + \".txt\"\n",
    "    sub.to_csv(sub_name, header = False, sep = '\\t')\n",
    "\n",
    "#     model_name = tmp_model_dir + \"imgmodel_\" + time_label + \".h5\"\n",
    "#     img_model[0].save(model_name)\n",
    "#     for i, model in enumerate(models):\n",
    "#         model_name = tmp_model_dir + \"zsmodel_\" + str(i) + time_label + \".h5\"\n",
    "#         model[0].save(model_name)\n",
    "\n",
    "    if not os.path.isdir(output_model_path):\n",
    "        os.makedirs(output_model_path, exist_ok=True)\n",
    "    for fileName in os.listdir(tmp_model_dir):\n",
    "        dst_file = os.path.join(output_model_path, fileName)\n",
    "        if os.path.exists(dst_file):\n",
    "            os.remove(dst_file)\n",
    "        shutil.move(os.path.join(tmp_model_dir, fileName), output_model_path)\n",
    "        \n",
    "cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(round2_class_id)]\n",
    "sub(models = zs_models, train_data = train_data, test_data = test_data, \n",
    "        class_id_emb_attr = cand_class_id_emb_attr, img_model = img_model, \n",
    "        output_model_path = '../submit/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model_dir = path + \"./model_dir/6_12_24_16_ini_64_grow_32_03535/\"\n",
    "# time_label = time.strftime('_%Y_%m_%d_%H_%M_%S', time.gmtime())\n",
    "# if not os.path.isdir(tmp_model_dir):\n",
    "#     os.makedirs(tmp_model_dir, exist_ok=True)\n",
    "# model_name = tmp_model_dir + \"model\" + time_label + \".h5\"\n",
    "# img_model.model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38221/38221 [==============================] - 2s 45us/step\n"
     ]
    }
   ],
   "source": [
    "pred_class_probas = img_classifi_model.predict(train_image_feature_map, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['preds'] = list(pred_class_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "891 * 80 / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n03598930', 'jigsaw_puzzle', 0.023861792), ('n02095314', 'wire-haired_fox_terrier', 0.022931756), ('n04589890', 'window_screen', 0.021361042)]\n"
     ]
    }
   ],
   "source": [
    "# np.asarray(list(train_data['img'])).shape\n",
    "# train_data[train_data['class_id'] == 'ZJL1']\n",
    "# train_data['class_id'].value_counts().hist()\n",
    "# train_data.head()\n",
    "# class_id_emb_attr.iloc[0].name\n",
    "# resnet50_model = ResNet50(weights='imagenet', input_shape=(224, 224, 3))\n",
    "img = image.load_img(path + '/DatasetA_train_20180813/train/000c0d617f5b67d116dee15c40d1d47d.jpeg', target_size=(224, 224))\n",
    "img = image.img_to_array(img)\n",
    "img = vgg16.preprocess_input(img)\n",
    "preds = vgg_model.predict(np.expand_dims(img, axis=0))\n",
    "print('Predicted:', vgg16.decode_predictions(preds, top=3)[0])\n",
    "# imread(path + '/DatasetA_train_20180813/train/000c0d617f5b67d116dee15c40d1d47d.jpeg')\n",
    "# image.img_to_array(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = setA_train_data\n",
    "\n",
    "img_model = DenseNet(blocks = [6, 12, 24, 16], \n",
    "                                cat_max = 205,\n",
    "                                weight_decay = 1e-4, \n",
    "                                kernel_initializer = 'glorot_normal',\n",
    "                                reduction = 0.5, \n",
    "                                init_filters = 128, \n",
    "                                growth_rate = 32).model\n",
    "img_model.load_weights(path + '/model_sub/6_12_24_16_ini64_growth32_inistride2_03621/model_0_2018_09_21_21_10_59.h5')\n",
    "img_model_flat = Model(input = img_model.input, output = img_model.get_layer(name = 'avg_pool').output)\n",
    "train_data['target'] = list(img_model_flat.predict(preprocess_img(train_data['img']), verbose = 1))\n",
    "# train_data['preds'] = list(img_model.predict(train_img, verbose = 1))\n",
    "# train_data['target'] = list(train_y) #\n",
    "# with open('../..//Data/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/model_0_2018_09_24_03_07_15.h5', 'rb') as handle:\n",
    "#     flat_train_re = pickle.load(handle)\n",
    "# train_data['target'] = list(flat_train_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0262023 , 1.0496653 , 0.42650044, ..., 0.09345146, 0.14242867,\n",
       "       0.        ], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_train_re[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2266190e+00, 1.6327184e-01, 2.8384233e-01, ..., 4.1941926e-04,\n",
       "       3.3683911e-02, 7.6473856e-01], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['target'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_part_img_id = pd.read_csv(path + '/model_sub/6_12_24_16_ini64_growth16_02962/train_part_img_id_0.csv', header = None)\n",
    "validate_part_img_id = pd.read_csv(path + '/model_sub/6_12_24_16_ini64_growth16_02962/validate_part_img_id_0.csv', header = None)\n",
    "train_part_img_id = train_part_img_id[0].values\n",
    "validate_part_img_id = validate_part_img_id[0].values\n",
    "\n",
    "train_part_df = train_data[train_data['img_id'].isin(train_part_img_id)]\n",
    "validate_part_df = train_data[train_data['img_id'].isin(validate_part_img_id)]\n",
    "\n",
    "seen_class = train_part_df.append(validate_part_df).class_id.unique()\n",
    "\n",
    "train_part_df = train_data[train_data['class_id'].isin(seen_class)]\n",
    "validate_part_df = train_data[~train_data['class_id'].isin(seen_class)]\n",
    "# unseen_class_df = train_data[~train_data['class_id'].isin(seen_class)]\n",
    "unseen_class = validate_part_df.class_id.unique()\n",
    "\n",
    "# validate_part_df = validate_part_df.append(unseen_class_df)\n",
    "# train_part_df = train_part_df.append(validate_part_df)\n",
    "# validate_part_df = unseen_class_df\n",
    "\n",
    "train_part_data = create_dnn_data(train_part_df)\n",
    "train_part_target = extract_array_from_series(train_part_df['target'])\n",
    "\n",
    "validate_part_data = create_dnn_data(validate_part_df)\n",
    "validate_part_target = extract_array_from_series(validate_part_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39184, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setB_train_data[setB_train_data.class_id.isin(seen_class)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3713906765013109"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 / 10.77032961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 6 4]\n",
      " [8 7 1]\n",
      " [2 2 5]\n",
      " [2 1 5]]\n",
      "[10.77032961 10.67707825  5.74456265  5.47722558]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.74278135, 0.55708601, 0.37139068],\n",
       "       [0.74926865, 0.65561007, 0.09365858],\n",
       "       [0.34815531, 0.34815531, 0.87038828],\n",
       "       [0.36514837, 0.18257419, 0.91287093]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(1, 10, (4,3))\n",
    "print (x)\n",
    "print (np.linalg.norm(x, axis = 1))\n",
    "sklearn.preprocessing.normalize(x, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "        \n",
    "def find_nearest_class(class_id_emb_attr, model, eval_df, cand_feature_map, img_feature_map, \n",
    "                       threshold = None, gamma = None, seen_class = None):\n",
    "#     cand_feature_map = model.predict(create_dnn_data(class_id_emb_attr))\n",
    "    # img_feature_map = np.repeat(img_feature_map, [cand_feature_map.shape[0]] * img_feature_map.shape[0], axis = 0)\n",
    "    nearest_class_id = ['ZJL'] * eval_df.shape[0]\n",
    "#     seen_class_bias = np.zeros(class_id_emb_attr.shape[0])\n",
    "#     seen_class_bias[class_id_emb_attr.class_id.isin(seen_class)] = gamma\n",
    "    \n",
    "#     seen_indice = [category_dict[c] for c in seen_class]\n",
    "#     preds = np.asarray(list(eval_df['preds']))[:, seen_indice]\n",
    "    for i in range(img_feature_map.shape[0]):\n",
    "        if False: #np.amax(preds[i]) > threshold:\n",
    "            nearest_class_id[i] = seen_class[np.argmax(preds[i])]\n",
    "        else:\n",
    "            img = img_feature_map[i]\n",
    "#             dis = 1 - sklearn.metrics.pairwise.cosine_similarity([img], \n",
    "#                                     cand_feature_map).reshape((cand_feature_map.shape[0]))\n",
    "            dis = np.linalg.norm(img - cand_feature_map, axis = 1)\n",
    "#             print (dis.shape)\n",
    "#             if np.amin(dis[class_id_emb_attr.class_id.isin(seen_class)]) > \\\n",
    "#                     gamma * np.amin(dis[~class_id_emb_attr.class_id.isin(seen_class)]):\n",
    "#                 dis[class_id_emb_attr.class_id.isin(seen_class)] = 200\n",
    "#             dis += seen_class_bias\n",
    "#             print(dis)\n",
    "#             plt.hist(dis)\n",
    "#             return\n",
    "            min_ind = np.where(dis == np.amin(dis))[0]\n",
    "            if len(min_ind) > 1:\n",
    "                print ('eval img id: ', eval_df.iloc[i]['class_id'], 'has multiple best candidates: ', len(min_ind), 'min val: ', np.amin(dis))\n",
    "    #         print (i, img, class_id_emb_attr.iloc[min_ind[0]].name)\n",
    "            nearest_class_id[i] = class_id_emb_attr.iloc[min_ind[0]]['class_id']\n",
    "#     print (nearest_class_id)\n",
    "    return np.asarray(nearest_class_id)\n",
    "        \n",
    "def calc_accuracy(eval_df, eval_class, preds):\n",
    "    eval_mask = eval_df.class_id.isin(eval_class)\n",
    "    eval_num = np.sum(eval_mask)\n",
    "    right_num = np.sum(preds[eval_mask] == eval_df.class_id[eval_mask])\n",
    "    return right_num / np.sum(eval_mask), right_num, eval_num\n",
    "    \n",
    "def calc_detailed_accuracy(eval_df, preds, seen_class, unseen_class):\n",
    "    all_re = calc_accuracy(eval_df, eval_df['class_id'].values, preds)\n",
    "    seen_re = calc_accuracy(eval_df, seen_class, preds)\n",
    "    unseen_re = calc_accuracy(eval_df, unseen_class, preds)\n",
    "    print(\"\\nAll_re: \\t%.6f\\t%.0f\\t%.0f\" % all_re)\n",
    "    print(\"Seen_re: \\t%.6f\\t%.0f\\t%.0f\" % seen_re)\n",
    "    print(\"Unseen_re: \\t%.6f\\t%.0f\\t%.0f\" % unseen_re)\n",
    "\n",
    "def model_eval(model, model_type, eval_df, cand_class_id_emb_attr, img_feature_map, seen_class = None, unseen_class = None):\n",
    "    if model_type == 'DEM':\n",
    "        zs_model = Model(inputs = model.inputs[:2], outputs = model.outputs[0])\n",
    "        cand_feature_map = zs_model.predict(create_dnn_data(cand_class_id_emb_attr))\n",
    "    elif model_type == 'GCN':\n",
    "        zs_model = Model(inputs = model.inputs[2:], outputs = model.outputs[0])\n",
    "        cand_class_to_id = [class_to_id[c] for c in cand_class_id_emb_attr.class_id.values]\n",
    "        cand_feature_map = zs_model.predict(None, steps = 1)[cand_class_to_id]\n",
    "    preds = find_nearest_class(cand_class_id_emb_attr, zs_model, eval_df, cand_feature_map, \n",
    "                               img_feature_map)\n",
    "    if 'class_id' in eval_df.columns:\n",
    "        calc_detailed_accuracy(eval_df, preds, seen_class, unseen_class)\n",
    "    return preds\n",
    "\n",
    "def models_eval(models, eval_df, cand_class_id_emb_attr, img_feature_map, \n",
    "                seen_class = None, unseen_class = None):\n",
    "    preds = []\n",
    "    for model, model_type in models:\n",
    "        pred = model_eval(model, model_type, eval_df, cand_class_id_emb_attr, img_feature_map, \n",
    "                          seen_class, unseen_class)\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "        \n",
    "def multi_models_vote(models, eval_df = None, cand_class_id_emb_attr = None, img_feature_map = None, \n",
    "                      seen_class = None, unseen_class = None):\n",
    "    preds = models_eval(models, eval_df, cand_class_id_emb_attr, img_feature_map, \n",
    "                seen_class, unseen_class)\n",
    "    preds = np.asarray(preds).T\n",
    "    print (preds)\n",
    "    vote_preds = []\n",
    "    for single_img_vote in preds:\n",
    "        uniq_val, counts = np.unique(single_img_vote, return_counts = True)\n",
    "        vote_preds.append(uniq_val[np.argmax(counts)])\n",
    "    vote_preds = np.asarray(vote_preds)\n",
    "    print (vote_preds)\n",
    "    if 'class_id' in eval_df.columns: \n",
    "        calc_detailed_accuracy(eval_df, vote_preds, seen_class, unseen_class)\n",
    "    return vote_preds\n",
    "    \n",
    "class AccuracyEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1, batch_interval = 1000000, verbose = 2, \\\n",
    "            scores = [], class_id_emb_attr = None, eval_df = None, threshold = None, \\\n",
    "                 seen_class = None, unseen_class = None, gamma = None, model_type = None):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        # print(\"y_val shape:{0}\".format(self.y_val.shape))\n",
    "        self.batch_interval = batch_interval\n",
    "        self.verbose = verbose\n",
    "        self.scores = scores\n",
    "        self.class_id_emb_attr = class_id_emb_attr\n",
    "        self.eval_df = eval_df\n",
    "        self.threshold = threshold\n",
    "        self.seen_class = seen_class\n",
    "        self.unseen_class = unseen_class\n",
    "        self.gamma = gamma\n",
    "        self.model_type = model_type\n",
    "#         seen_indice = [category_dict[c] for c in seen_class]\n",
    "#         self.preds = np.asarray(list(eval_df['preds']))[:, seen_indice]\n",
    "#         self.img_feature_map = self.y_val\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            model_eval(self.model, self.model_type, self.eval_df, self.class_id_emb_attr, \n",
    "                seen_class = self.seen_class, unseen_class = self.unseen_class, img_feature_map = self.y_val)\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "        if(self.verbose >= 2) and (batch % self.batch_interval == 0):\n",
    "            # y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            # loss = metrics.log_loss(self.y_val, y_pred)\n",
    "            print(\"Hi! on_batch_end() , batch=\",batch,\",logs:\",logs)\n",
    "            # print(\"Valide size=\",y_pred.shape[0], \"  Valide loss=\",loss)\n",
    "\n",
    "def full_connect_layer(input, hidden_dim, activation, resnet = False, adj_graphs = None, \n",
    "                       drop_out_ratio = None, kernel_initializer = 'he_normal'):\n",
    "    full_connect = input\n",
    "    for i, hn in enumerate(hidden_dim):\n",
    "        fc_in = full_connect\n",
    "        if drop_out_ratio is not None:\n",
    "            full_connect = Dropout(drop_out_ratio)(full_connect)\n",
    "        full_connect = BatchNormalization(epsilon=1.001e-5)(full_connect)\n",
    "        # full_connect = Dense(hn, kernel_regularizer = l2(0.001), activity_regularizer = l1(0.001))(full_connect)\n",
    "#         full_connect = Concatenate()([Dense(hn, kernel_initializer='lecun_uniform', activation = 'relu')(full_connect), \n",
    "#             Dense(hn, kernel_initializer='lecun_uniform', activation = 'sigmoid')(full_connect)])\n",
    "        full_connect = Dense(hn, kernel_initializer=kernel_initializer, kernel_regularizer = l2(1e-4), activation = activation)(full_connect)\n",
    "#         full_connect = LeakyReLU(alpha=0.02)(full_connect)\n",
    "        # full_connect = self.act_blend(full_connect)\n",
    "        # if self.full_connect_dropout > 0:\n",
    "#             full_connect = Dropout(self.full_connect_dropout)(full_connect) #Dropout(self.full_connect_dropout)(full_connect)\n",
    "        if adj_graphs is not None:\n",
    "            full_connect = Lambda(lambda x: K.dot(x[1], x[0]), \\\n",
    "                                  name = 'rela_' + str(i))([full_connect, adj_graphs])\n",
    "        if resnet:\n",
    "            full_connect = Concatenate()([fc_in, full_connect])\n",
    "    return full_connect\n",
    "\n",
    "def extract_array_from_series(s):\n",
    "    return np.asarray(list(s))\n",
    "\n",
    "def create_dnn_data(df):\n",
    "    # return [extract_array_from_series(df['attr']), extract_array_from_series(df['emb'])[:, :]]\n",
    "    return [extract_array_from_series(df['attr']), extract_array_from_series(df['emb'])[:, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD...\n"
     ]
    }
   ],
   "source": [
    "print ('SVD...')\n",
    "svd = decomposition.TruncatedSVD(n_components=10, n_iter=50, random_state=12)\n",
    "svd_features = svd.fit_transform(extract_array_from_series(class_id_emb_attr['attr']))\n",
    "class_id_emb_attr['svd_attr'] = list(svd_features)\n",
    "train_data = train_data.merge(class_id_emb_attr[['class_id', 'svd_attr']], how = 'left', on = 'class_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87249, 10)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen unseen Classes:  164 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_140\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_140\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attr (InputLayer)               (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "wv (InputLayer)                 (None, 600)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_137 (Dense)               (None, 300)          9000        attr[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 900)          0           wv[0][0]                         \n",
      "                                                                 dense_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 900)          0           concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 900)          3600        dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_139 (Dense)               (None, 1536)         1383936     batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 1536)         6144        dense_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_140 (Dense)               (None, 1024)         1573888     batch_normalization_92[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,976,568\n",
      "Trainable params: 2,971,696\n",
      "Non-trainable params: 4,872\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 70408 samples, validate on 16841 samples\n",
      "Epoch 1/50\n",
      "70408/70408 [==============================] - 104s 1ms/step - loss: 0.5655 - val_loss: 0.4114\n",
      "\n",
      "All_re: \t0.199216\t3355\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.199216\t3355\t16841\n",
      "Epoch 2/50\n",
      "  128/70408 [..............................] - ETA: 1:08 - loss: 0.3791"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70408/70408 [==============================] - 31s 436us/step - loss: 0.3235 - val_loss: 0.3294\n",
      "\n",
      "All_re: \t0.219999\t3705\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.219999\t3705\t16841\n",
      "Epoch 3/50\n",
      "70408/70408 [==============================] - 28s 394us/step - loss: 0.2883 - val_loss: 0.3257\n",
      "\n",
      "All_re: \t0.207945\t3502\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.207945\t3502\t16841\n",
      "Epoch 4/50\n",
      "70408/70408 [==============================] - 29s 418us/step - loss: 0.2808 - val_loss: 0.3216\n",
      "\n",
      "All_re: \t0.222315\t3744\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.222315\t3744\t16841\n",
      "Epoch 5/50\n",
      "70408/70408 [==============================] - 28s 402us/step - loss: 0.2787 - val_loss: 0.3213\n",
      "\n",
      "All_re: \t0.235972\t3974\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.235972\t3974\t16841\n",
      "Epoch 6/50\n",
      "70408/70408 [==============================] - 32s 449us/step - loss: 0.2776 - val_loss: 0.3168\n",
      "\n",
      "All_re: \t0.238525\t4017\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.238525\t4017\t16841\n",
      "Epoch 7/50\n",
      "70408/70408 [==============================] - 28s 399us/step - loss: 0.2766 - val_loss: 0.3047\n",
      "\n",
      "All_re: \t0.234547\t3950\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.234547\t3950\t16841\n",
      "Epoch 8/50\n",
      "70408/70408 [==============================] - 30s 432us/step - loss: 0.2744 - val_loss: 0.3047\n",
      "\n",
      "All_re: \t0.231993\t3907\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.231993\t3907\t16841\n",
      "Epoch 9/50\n",
      "70408/70408 [==============================] - 28s 396us/step - loss: 0.2723 - val_loss: 0.3039\n",
      "\n",
      "All_re: \t0.211626\t3564\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.211626\t3564\t16841\n",
      "Epoch 10/50\n",
      "70408/70408 [==============================] - 32s 451us/step - loss: 0.2705 - val_loss: 0.3030\n",
      "\n",
      "All_re: \t0.229321\t3862\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.229321\t3862\t16841\n",
      "Epoch 11/50\n",
      "70408/70408 [==============================] - 28s 402us/step - loss: 0.2700 - val_loss: 0.3027\n",
      "\n",
      "All_re: \t0.223621\t3766\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.223621\t3766\t16841\n",
      "Epoch 12/50\n",
      "70408/70408 [==============================] - 30s 420us/step - loss: 0.2694 - val_loss: 0.3019\n",
      "\n",
      "All_re: \t0.212695\t3582\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.212695\t3582\t16841\n",
      "Epoch 13/50\n",
      "70408/70408 [==============================] - 28s 394us/step - loss: 0.2692 - val_loss: 0.3031\n",
      "\n",
      "All_re: \t0.211033\t3554\t16841\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.211033\t3554\t16841\n",
      "Seen unseen Classes:  164 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_144\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_144\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71411 samples, validate on 15838 samples\n",
      "Epoch 1/50\n",
      "71411/71411 [==============================] - 106s 1ms/step - loss: 0.5631 - val_loss: 0.4024\n",
      "\n",
      "All_re: \t0.194090\t3074\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.194090\t3074\t15838\n",
      "Epoch 2/50\n",
      "71411/71411 [==============================] - 28s 398us/step - loss: 0.3217 - val_loss: 0.3223\n",
      "\n",
      "All_re: \t0.224081\t3549\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.224081\t3549\t15838\n",
      "Epoch 3/50\n",
      "71411/71411 [==============================] - 29s 402us/step - loss: 0.2867 - val_loss: 0.3117\n",
      "\n",
      "All_re: \t0.208675\t3305\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.208675\t3305\t15838\n",
      "Epoch 4/50\n",
      "71411/71411 [==============================] - 29s 401us/step - loss: 0.2788 - val_loss: 0.3085\n",
      "\n",
      "All_re: \t0.203687\t3226\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.203687\t3226\t15838\n",
      "Epoch 5/50\n",
      "71411/71411 [==============================] - 28s 393us/step - loss: 0.2757 - val_loss: 0.3052\n",
      "\n",
      "All_re: \t0.207791\t3291\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.207791\t3291\t15838\n",
      "Epoch 6/50\n",
      "71411/71411 [==============================] - 28s 395us/step - loss: 0.2735 - val_loss: 0.3035\n",
      "\n",
      "All_re: \t0.218146\t3455\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.218146\t3455\t15838\n",
      "Epoch 7/50\n",
      "71411/71411 [==============================] - 28s 396us/step - loss: 0.2720 - val_loss: 0.3026\n",
      "\n",
      "All_re: \t0.229385\t3633\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.229385\t3633\t15838\n",
      "Epoch 8/50\n",
      "71411/71411 [==============================] - 29s 400us/step - loss: 0.2711 - val_loss: 0.3006\n",
      "\n",
      "All_re: \t0.231216\t3662\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.231216\t3662\t15838\n",
      "Epoch 9/50\n",
      "71411/71411 [==============================] - 28s 396us/step - loss: 0.2706 - val_loss: 0.3014\n",
      "\n",
      "All_re: \t0.245107\t3882\t15838\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.245107\t3882\t15838\n",
      "Seen unseen Classes:  164 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_148\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_148\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69141 samples, validate on 18108 samples\n",
      "Epoch 1/50\n",
      "69141/69141 [==============================] - 107s 2ms/step - loss: 0.5679 - val_loss: 0.4168\n",
      "\n",
      "All_re: \t0.197095\t3569\t18108\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.197095\t3569\t18108\n",
      "Epoch 2/50\n",
      "69141/69141 [==============================] - 30s 439us/step - loss: 0.3233 - val_loss: 0.3322\n",
      "\n",
      "All_re: \t0.204385\t3701\t18108\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.204385\t3701\t18108\n",
      "Epoch 3/50\n",
      "69141/69141 [==============================] - 30s 438us/step - loss: 0.2866 - val_loss: 0.3208\n",
      "\n",
      "All_re: \t0.193837\t3510\t18108\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.193837\t3510\t18108\n",
      "Epoch 4/50\n",
      "69141/69141 [==============================] - 30s 436us/step - loss: 0.2784 - val_loss: 0.3116\n",
      "\n",
      "All_re: \t0.206980\t3748\t18108\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.206980\t3748\t18108\n",
      "Epoch 5/50\n",
      "69141/69141 [==============================] - 30s 433us/step - loss: 0.2749 - val_loss: 0.3123\n",
      "\n",
      "All_re: \t0.200961\t3639\t18108\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.200961\t3639\t18108\n",
      "Seen unseen Classes:  164 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_152\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_152\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69739 samples, validate on 17510 samples\n",
      "Epoch 1/50\n",
      "69739/69739 [==============================] - 108s 2ms/step - loss: 0.5657 - val_loss: 0.4218\n",
      "\n",
      "All_re: \t0.141005\t2469\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.141005\t2469\t17510\n",
      "Epoch 2/50\n",
      "69739/69739 [==============================] - 30s 434us/step - loss: 0.3225 - val_loss: 0.3367\n",
      "\n",
      "All_re: \t0.165677\t2901\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.165677\t2901\t17510\n",
      "Epoch 3/50\n",
      "69739/69739 [==============================] - 30s 430us/step - loss: 0.2867 - val_loss: 0.3231\n",
      "\n",
      "All_re: \t0.139406\t2441\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.139406\t2441\t17510\n",
      "Epoch 4/50\n",
      "69739/69739 [==============================] - 30s 425us/step - loss: 0.2796 - val_loss: 0.3168\n",
      "\n",
      "All_re: \t0.153113\t2681\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.153113\t2681\t17510\n",
      "Epoch 5/50\n",
      "69739/69739 [==============================] - 29s 419us/step - loss: 0.2770 - val_loss: 0.3163\n",
      "\n",
      "All_re: \t0.141576\t2479\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.141576\t2479\t17510\n",
      "Epoch 6/50\n",
      "69739/69739 [==============================] - 30s 430us/step - loss: 0.2748 - val_loss: 0.3158\n",
      "\n",
      "All_re: \t0.171845\t3009\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.171845\t3009\t17510\n",
      "Epoch 7/50\n",
      "69739/69739 [==============================] - 30s 431us/step - loss: 0.2740 - val_loss: 0.3157\n",
      "\n",
      "All_re: \t0.150885\t2642\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.150885\t2642\t17510\n",
      "Epoch 8/50\n",
      "69739/69739 [==============================] - 30s 426us/step - loss: 0.2720 - val_loss: 0.3145\n",
      "\n",
      "All_re: \t0.170017\t2977\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.170017\t2977\t17510\n",
      "Epoch 9/50\n",
      "69739/69739 [==============================] - 30s 425us/step - loss: 0.2700 - val_loss: 0.3129\n",
      "\n",
      "All_re: \t0.168247\t2946\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.168247\t2946\t17510\n",
      "Epoch 10/50\n",
      "69739/69739 [==============================] - 30s 433us/step - loss: 0.2690 - val_loss: 0.3129\n",
      "\n",
      "All_re: \t0.159794\t2798\t17510\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.159794\t2798\t17510\n",
      "Seen unseen Classes:  164 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_156\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_156\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68297 samples, validate on 18952 samples\n",
      "Epoch 1/50\n",
      "68297/68297 [==============================] - 97s 1ms/step - loss: 0.5718 - val_loss: 0.4250\n",
      "\n",
      "All_re: \t0.190903\t3618\t18952\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.190903\t3618\t18952\n",
      "Epoch 2/50\n",
      "68297/68297 [==============================] - 30s 441us/step - loss: 0.3240 - val_loss: 0.3366\n",
      "\n",
      "All_re: \t0.209529\t3971\t18952\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.209529\t3971\t18952\n",
      "Epoch 3/50\n",
      "68297/68297 [==============================] - 30s 433us/step - loss: 0.2860 - val_loss: 0.3216\n",
      "\n",
      "All_re: \t0.201192\t3813\t18952\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.201192\t3813\t18952\n",
      "Epoch 4/50\n",
      "68297/68297 [==============================] - 29s 432us/step - loss: 0.2777 - val_loss: 0.3179\n",
      "\n",
      "All_re: \t0.212854\t4034\t18952\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.212854\t4034\t18952\n",
      "Epoch 5/50\n",
      "68297/68297 [==============================] - 29s 430us/step - loss: 0.2744 - val_loss: 0.3180\n",
      "\n",
      "All_re: \t0.206205\t3908\t18952\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.206205\t3908\t18952\n"
     ]
    }
   ],
   "source": [
    "def create_dnn(kernel_initializer = 'he_normal', img_flat_len = 1024):\n",
    "    alpha = 0.03\n",
    "    img_flat_len = img_flat_len\n",
    "    attr_input = Input(shape = (30,), name = 'attr')\n",
    "    word_emb = Input(shape = (600,), name = 'wv')\n",
    "    imag_classifier = Input(shape = (img_flat_len,), name = 'img')\n",
    "\n",
    "    attr_dense = Dense(300, use_bias = False, kernel_initializer=kernel_initializer, \n",
    "                       kernel_regularizer = l2(1e-4))(attr_input)\n",
    "#     attr_dense = Dense(512)(attr_dense)\n",
    "#     attr_dense = LeakyReLU(alpha=alpha)(attr_dense)\n",
    "#     attr_dense = Dense(256)(attr_dense)\n",
    "#     attr_dense = LeakyReLU(alpha=alpha)(attr_dense)\n",
    "#     attr_dense = Dense(300, activation=\"relu\")(attr_dense)\n",
    "    # attr_dense = Dense(512, activation=\"relu\")(attr_dense)\n",
    "    word_emb_dense = Dense(300, use_bias = False, kernel_initializer=kernel_initializer, \n",
    "                           kernel_regularizer = l2(1e-4))(word_emb)\n",
    "\n",
    "    attr_word_emb = Concatenate()([word_emb, attr_dense])\n",
    "#     attr_word_emb = word_emb #Add()([word_emb_dense, attr_dense])\n",
    "    attr_word_emb_dense = full_connect_layer(attr_word_emb, hidden_dim = [int(img_flat_len * 1.5), \n",
    "#                                                                           int(img_flat_len * 1.25), \n",
    "#                                                                           int(img_flat_len * 1.125),\n",
    "#                                                                           int(img_flat_len * 0.5)\n",
    "                                                                         ], \\\n",
    "                                             activation = 'relu', resnet = False, drop_out_ratio = 0.2)\n",
    "    attr_word_emb_dense = full_connect_layer(attr_word_emb_dense, hidden_dim = [img_flat_len], \n",
    "                                             activation = 'relu')\n",
    "\n",
    "    mse_loss = K.mean(keras.losses.mean_squared_error(imag_classifier, attr_word_emb_dense))\n",
    "    \n",
    "    model = Model([attr_input, word_emb, imag_classifier], outputs = attr_word_emb_dense) #, vgg_output])\n",
    "    model.add_loss(mse_loss)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=5e-4), loss=None)\n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state = 100)\n",
    "num_fold = 0\n",
    "zs_model_list = []\n",
    "classes = train_data.class_id.unique()\n",
    "\n",
    "for train_index, test_index in kf.split(classes):\n",
    "    seen_class = classes[train_index]\n",
    "    unseen_class = classes[test_index]\n",
    "    \n",
    "    train_part_df = train_data[train_data.class_id.isin(seen_class)]\n",
    "    validate_part_df = train_data[train_data.class_id.isin(unseen_class)]\n",
    "    \n",
    "    train_part_data = create_dnn_data(train_part_df)\n",
    "    validate_part_data = create_dnn_data(validate_part_df)\n",
    "    \n",
    "    train_part_target = extract_array_from_series(train_part_df['target']) #sklearn.preprocessing.normalize(extract_array_from_series(train_part_df['target']), norm='l2')\n",
    "    validate_part_target = extract_array_from_series(validate_part_df['target']) #sklearn.preprocessing.normalize(extract_array_from_series(validate_part_df['target']), norm='l2')\n",
    "\n",
    "#     print ('Train Validation Classes: ', train_part_df.class_id.unique().shape[0], \n",
    "#            validate_part_df.class_id.unique().shape[0])\n",
    "    print ('Seen unseen Classes: ', seen_class.shape[0], unseen_class.shape[0])\n",
    "\n",
    "    callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=1, verbose=0),\n",
    "            AccuracyEvaluation(validation_data=(validate_part_data,  validate_part_target), interval=1, \\\n",
    "    #                         class_id_emb_attr = class_id_emb_attr, \\\n",
    "                               eval_df = validate_part_df, threshold= 0.3, \\\n",
    "                              seen_class = seen_class, unseen_class = unseen_class, \\\n",
    "                class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \\\n",
    "                              gamma = 0.8, model_type = 'DEM')\n",
    "            ]\n",
    "#     for i in range(5):\n",
    "    zs_model = create_dnn(kernel_initializer = 'he_normal', img_flat_len = 1024)\n",
    "    if num_fold == 0:\n",
    "        print (zs_model.summary())\n",
    "    zs_model.fit(train_part_data + [train_part_target],  validation_data = (validate_part_data + [validate_part_target], None),\n",
    "                  epochs=50, batch_size = 128, shuffle=True, verbose = 1, callbacks=callbacks)\n",
    "    zs_model_list.append((zs_model, 'DEM'))\n",
    "    num_fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(zs_model_list):\n",
    "    model_name = path + '/model_sub/zs_model_' + str(i) + \"_05011.txt\"\n",
    "    zs_model_list[i][0].save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All_re: \t0.213811\t3570\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.213811\t3570\t16697\n",
      "\n",
      "All_re: \t0.227226\t3794\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.227226\t3794\t16697\n",
      "\n",
      "All_re: \t0.235911\t3939\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.235911\t3939\t16697\n",
      "\n",
      "All_re: \t0.216566\t3616\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.216566\t3616\t16697\n",
      "\n",
      "All_re: \t0.070492\t1177\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.070492\t1177\t16697\n",
      "[['ZJL200' 'ZJL200' 'ZJL200' 'ZJL19' 'ZJL101']\n",
      " ['ZJL4' 'ZJL100' 'ZJL41' 'ZJL4' 'ZJL101']\n",
      " ['ZJL192' 'ZJL192' 'ZJL192' 'ZJL192' 'ZJL101']\n",
      " ...\n",
      " ['ZJL261' 'ZJL102' 'ZJL254' 'ZJL254' 'ZJL261']\n",
      " ['ZJL254' 'ZJL254' 'ZJL261' 'ZJL254' 'ZJL137']\n",
      " ['ZJL261' 'ZJL261' 'ZJL261' 'ZJL254' 'ZJL261']]\n",
      "['ZJL200' 'ZJL4' 'ZJL192' ... 'ZJL254' 'ZJL254' 'ZJL261']\n",
      "\n",
      "All_re: \t0.234533\t3916\t16697\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.234533\t3916\t16697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['ZJL200', 'ZJL4', 'ZJL192', ..., 'ZJL254', 'ZJL254', 'ZJL261'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_models_vote(models = zs_model_list, eval_df = validate_part_df, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \\\n",
    "            img_feature_map = validate_part_target, seen_class = seen_class, unseen_class = unseen_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 5, 9],\n",
       "       [8, 7, 7],\n",
       "       [1, 1, 7],\n",
       "       [7, 1, 9]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(1, 10, (4, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 7 8 9] [3 1 4 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_val, counts = np.unique(x, return_counts = True)\n",
    "print (uniq_val, counts)\n",
    "# uniq_val[np.argmax(counts)]\n",
    "np.argmin(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = np.array(list(class_id_emb_attr['attr']))\n",
    "# adj_graph = scipy.eye(attr.shape[0]) #1 - sklearn.metrics.pairwise.pairwise_distances(attr, metric = 'cosine')\n",
    "adj_graph = 1 - sklearn.metrics.pairwise.pairwise_distances(\n",
    "    np.array(list(class_id_emb_attr['emb'])), metric = 'cosine')\n",
    "# th = 0.99999\n",
    "# adj_graph[adj_graph > th] = 1\n",
    "# adj_graph[adj_graph <= th] = 0\n",
    "# adj_graph = adj_graph / np.linalg.norm(adj_graph)\n",
    "# adj_graph = adj_graph[:, np.argsort(adj_graph)[:]]\n",
    "# adj_graph[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = class_id_emb_attr.class_id.values\n",
    "class_to_id = dict([(c, i) for i, c in enumerate(class_ids)])\n",
    "# class_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen unseen Classes:  164 41\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_32 (InputLayer)            (285, 30)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_33 (InputLayer)            (285, 300)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_84 (Dense)                 (285, 300)            9000        input_32[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)     (285, 600)            0           input_33[0][0]                   \n",
      "                                                                   dense_84[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNor (285, 600)            2400        concatenate_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_85 (Dense)                 (285, 1548)           930348      batch_normalization_58[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "input_34 (InputLayer)            (285, 285)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "rela_0 (Lambda)                  (285, 1548)           0           dense_85[0][0]                   \n",
      "                                                                   input_34[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNor (285, 1548)           6192        rela_0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_86 (Dense)                 (285, 1290)           1998210     batch_normalization_59[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "rela_1 (Lambda)                  (285, 1290)           0           dense_86[0][0]                   \n",
      "                                                                   input_34[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNor (285, 1290)           5160        rela_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_87 (Dense)                 (285, 1032)           1332312     batch_normalization_60[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "rela_2 (Lambda)                  (285, 1032)           0           dense_87[0][0]                   \n",
      "                                                                   input_34[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 4,283,622\n",
      "Trainable params: 4,276,746\n",
      "Non-trainable params: 6,876\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:32: UserWarning: Output \"rela_2\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"rela_2\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69913 samples, validate on 17336 samples\n",
      "Epoch 1/25\n",
      "69792/69913 [============================>.] - ETA: 0s - loss: 1.7330eval img id:  ZJL263 has multiple best candidates:  2 min val:  18.49614\n",
      "\n",
      "All_re: \t0.038417\t666\t17336\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.038417\t666\t17336\n",
      "69913/69913 [==============================] - 33s - loss: 1.7317 - val_loss: 1.0597\n",
      "Epoch 2/25\n",
      "  416/69913 [..............................] - ETA: 29s - loss: 0.9983"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69856/69913 [============================>.] - ETA: 0s - loss: 0.9736\n",
      "All_re: \t0.068066\t1180\t17336\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.068066\t1180\t17336\n",
      "69913/69913 [==============================] - 32s - loss: 0.9736 - val_loss: 1.0059\n",
      "Epoch 3/25\n",
      "69792/69913 [============================>.] - ETA: 0s - loss: 0.8904\n",
      "All_re: \t0.082603\t1432\t17336\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.082603\t1432\t17336\n",
      "69913/69913 [==============================] - 32s - loss: 0.8903 - val_loss: 0.9059\n",
      "Epoch 4/25\n",
      " 9536/69913 [===>..........................] - ETA: 24s - loss: 0.8300"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4eef8651ed11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     zs_model.fit([train_part_data, train_part_target],  \n\u001b[1;32m     74\u001b[0m                  \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidate_part_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_part_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                   epochs=25, batch_size = 32, shuffle=True, verbose = 1, callbacks=callbacks)\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mzs_model_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GCN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mnum_fold\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# adj_graph = 1 - sklearn.metrics.pairwise.pairwise_distances(\n",
    "#     np.array(list(class_id_emb_attr['emb']))[:, :300], metric = 'cosine')\n",
    "\n",
    "def create_gcn():\n",
    "    alpha = 0.03\n",
    "    img_flat_len = 1032\n",
    "    attr_input = Input(tensor=tf.constant(np.array(list(class_id_emb_attr['attr']), dtype = 'float32')))\n",
    "    all_word_emb = Input(tensor=tf.constant(np.array(list(class_id_emb_attr['emb']))[:, :300], dtype = 'float32')) #Input(shape = (230, 300,), name = 'wv')\n",
    "    class_index = Input(shape = (1, ), name = 'class_index', dtype = 'int32')\n",
    "    adj_graphs = Input(tensor=tf.constant(adj_graph, dtype = 'float32')) #Input(shape = (230, 230,), name = 'adj_graph')\n",
    "    imag_classifier = Input(shape = (img_flat_len,), name = 'img')\n",
    "    \n",
    "    attr_dense = Dense(300, use_bias = False, kernel_initializer='he_normal', \n",
    "                    kernel_regularizer = l2(1e-4))(attr_input)\n",
    "    attr_word_emb = Concatenate()([all_word_emb, attr_dense])\n",
    "#     x = Lambda(lambda xx: all_word_emb)(class_index)\n",
    "#     x = Dense(516, kernel_initializer='he_normal', kernel_regularizer = l2(1e-4), \n",
    "#               activation = 'relu', name = 'conv')(all_word_emb)\n",
    "#     all_classifier = Lambda(lambda x: K.dot(x[1], x[0]), name = 'rela')([x, adj_graphs])\n",
    "    all_classifier = full_connect_layer(attr_word_emb, hidden_dim = [int(img_flat_len * 1.5), \n",
    "                                                                    int(img_flat_len * 1.25 ),\n",
    "                                                                    img_flat_len], \n",
    "                                activation = 'relu', adj_graphs = adj_graphs)\n",
    "    x = tf.gather_nd(all_classifier, class_index)\n",
    "\n",
    "    mse_loss = K.mean(keras.losses.mean_squared_error(imag_classifier, x))\n",
    "    \n",
    "    model = Model([class_index, imag_classifier, attr_input, all_word_emb, adj_graphs], outputs = [all_classifier]) #, vgg_output])\n",
    "    model.add_loss(mse_loss)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=None)\n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state = 1)\n",
    "num_fold = 0\n",
    "zs_model_list = []\n",
    "classes = train_data.class_id.unique()\n",
    "class_ids = class_id_emb_attr.class_id.values\n",
    "class_to_id = dict([(c, i) for i, c in enumerate(class_ids)])\n",
    "\n",
    "for train_index, test_index in kf.split(classes):\n",
    "#     print (train_index)\n",
    "    seen_class = classes[train_index]\n",
    "    unseen_class = classes[test_index]\n",
    "    \n",
    "    train_part_df = train_data[train_data.class_id.isin(seen_class)]\n",
    "    validate_part_df = train_data[train_data.class_id.isin(unseen_class)]\n",
    "    \n",
    "    train_part_data = np.array([class_to_id[c] for c in train_part_df['class_id'].values]).astype('int32')\n",
    "    validate_part_data = np.array([class_to_id[c] for c in validate_part_df['class_id'].values]).astype('int32')\n",
    "\n",
    "    train_part_target = extract_array_from_series(train_part_df['target'])\n",
    "    validate_part_target = extract_array_from_series(validate_part_df['target'])\n",
    "\n",
    "#     print ('Train Validation Classes: ', train_part_df.class_id.unique().shape[0], \n",
    "#            validate_part_df.class_id.unique().shape[0])\n",
    "    print ('Seen unseen Classes: ', seen_class.shape[0], unseen_class.shape[0])\n",
    "\n",
    "    callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=50, verbose=0),\n",
    "            AccuracyEvaluation(validation_data=(validate_part_data,  validate_part_target), interval=1, \\\n",
    "    #                         class_id_emb_attr = class_id_emb_attr, \\\n",
    "                               eval_df = validate_part_df, threshold= 0.3, \\\n",
    "                              seen_class = seen_class, unseen_class = unseen_class, \\\n",
    "                class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \\\n",
    "                              gamma = 0.8, model_type = 'GCN')\n",
    "            ]\n",
    "    zs_model = create_gcn()\n",
    "    if num_fold == 0:\n",
    "        print (zs_model.summary())\n",
    "    zs_model.fit([train_part_data, train_part_target],  \n",
    "                 validation_data = ([validate_part_data, validate_part_target], None),\n",
    "                  epochs=25, batch_size = 32, shuffle=True, verbose = 1, callbacks=callbacks)\n",
    "    zs_model_list.append((zs_model, 'GCN'))\n",
    "    num_fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All_re: \t0.156834\t2140\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.156834\t2140\t13645\n",
      "\n",
      "All_re: \t0.155148\t2117\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.155148\t2117\t13645\n",
      "\n",
      "All_re: \t0.151118\t2062\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.151118\t2062\t13645\n",
      "\n",
      "All_re: \t0.153316\t2092\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.153316\t2092\t13645\n",
      "\n",
      "All_re: \t0.157640\t2151\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.157640\t2151\t13645\n",
      "[['ZJL264' 'ZJL264' 'ZJL264' 'ZJL264' 'ZJL264']\n",
      " ['ZJL102' 'ZJL102' 'ZJL102' 'ZJL102' 'ZJL102']\n",
      " ['ZJL254' 'ZJL276' 'ZJL254' 'ZJL254' 'ZJL254']\n",
      " ...\n",
      " ['ZJL254' 'ZJL254' 'ZJL276' 'ZJL254' 'ZJL254']\n",
      " ['ZJL276' 'ZJL276' 'ZJL276' 'ZJL276' 'ZJL254']\n",
      " ['ZJL168' 'ZJL125' 'ZJL50' 'ZJL168' 'ZJL254']]\n",
      "['ZJL264' 'ZJL102' 'ZJL254' ... 'ZJL254' 'ZJL276' 'ZJL168']\n",
      "\n",
      "All_re: \t0.166215\t2268\t13645\n",
      "Seen_re: \tnan\t0\t0\n",
      "Unseen_re: \t0.166215\t2268\t13645\n"
     ]
    }
   ],
   "source": [
    "multi_models_vote(models = zs_model_list, eval_df = validate_part_df, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[class_id_emb_attr.class_id.isin(unseen_class)], \\\n",
    "            img_feature_map = validate_part_target, seen_class = seen_class, unseen_class = unseen_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'model_sub/train_data_2018_09_23_10_32_21.pickle', 'rb') as handle:\n",
    "    test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>img_id</th>\n",
       "      <th>img</th>\n",
       "      <th>target</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>a6394b0f513290f4651cc46792e5ac86.jpeg</td>\n",
       "      <td>[[[20.0, 33.0, 7.0], [19.0, 32.0, 6.0], [22.0,...</td>\n",
       "      <td>[2.2266192, 0.16327204, 0.2838421, 0.2219766, ...</td>\n",
       "      <td>[0.9520665, 4.647786e-09, 7.417136e-08, 3.9506...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>2fb89ef2ace869d3eb3bdd3afe184e1c.jpeg</td>\n",
       "      <td>[[[140.0, 45.0, 51.0], [144.0, 47.0, 54.0], [1...</td>\n",
       "      <td>[0.17702743, 0.55263615, 0.0, 0.030876435, 1.2...</td>\n",
       "      <td>[1.30949e-05, 5.735788e-10, 6.5571693e-12, 5.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>eda9f3bef2bd8da038f6acbc8355fc25.jpeg</td>\n",
       "      <td>[[[81.0, 69.0, 21.0], [86.0, 74.0, 26.0], [85....</td>\n",
       "      <td>[0.0, 0.42584094, 0.0, 0.034428038, 0.527664, ...</td>\n",
       "      <td>[0.9993531, 4.2289447e-09, 1.2198088e-07, 3.68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>7d93ef45972154aae150b4f9980a79c0.jpeg</td>\n",
       "      <td>[[[16.0, 14.0, 15.0], [18.0, 16.0, 17.0], [19....</td>\n",
       "      <td>[0.6332717, 0.23473893, 0.0, 0.779357, 1.38354...</td>\n",
       "      <td>[0.9999671, 4.578459e-10, 6.6002594e-12, 8.168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZJL1</td>\n",
       "      <td>fb901b4f9a8e396c1d0155bccc5e5671.jpeg</td>\n",
       "      <td>[[[120.0, 124.0, 127.0], [89.0, 93.0, 96.0], [...</td>\n",
       "      <td>[0.37087774, 1.1033719, 0.0, 0.23497638, 3.003...</td>\n",
       "      <td>[0.86532485, 3.0740804e-08, 3.727919e-08, 1.21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_id                                 img_id  \\\n",
       "0     ZJL1  a6394b0f513290f4651cc46792e5ac86.jpeg   \n",
       "1     ZJL1  2fb89ef2ace869d3eb3bdd3afe184e1c.jpeg   \n",
       "2     ZJL1  eda9f3bef2bd8da038f6acbc8355fc25.jpeg   \n",
       "3     ZJL1  7d93ef45972154aae150b4f9980a79c0.jpeg   \n",
       "4     ZJL1  fb901b4f9a8e396c1d0155bccc5e5671.jpeg   \n",
       "\n",
       "                                                 img  \\\n",
       "0  [[[20.0, 33.0, 7.0], [19.0, 32.0, 6.0], [22.0,...   \n",
       "1  [[[140.0, 45.0, 51.0], [144.0, 47.0, 54.0], [1...   \n",
       "2  [[[81.0, 69.0, 21.0], [86.0, 74.0, 26.0], [85....   \n",
       "3  [[[16.0, 14.0, 15.0], [18.0, 16.0, 17.0], [19....   \n",
       "4  [[[120.0, 124.0, 127.0], [89.0, 93.0, 96.0], [...   \n",
       "\n",
       "                                              target  \\\n",
       "0  [2.2266192, 0.16327204, 0.2838421, 0.2219766, ...   \n",
       "1  [0.17702743, 0.55263615, 0.0, 0.030876435, 1.2...   \n",
       "2  [0.0, 0.42584094, 0.0, 0.034428038, 0.527664, ...   \n",
       "3  [0.6332717, 0.23473893, 0.0, 0.779357, 1.38354...   \n",
       "4  [0.37087774, 1.1033719, 0.0, 0.23497638, 3.003...   \n",
       "\n",
       "                                               preds  \n",
       "0  [0.9520665, 4.647786e-09, 7.417136e-08, 3.9506...  \n",
       "1  [1.30949e-05, 5.735788e-10, 6.5571693e-12, 5.9...  \n",
       "2  [0.9993531, 4.2289447e-09, 1.2198088e-07, 3.68...  \n",
       "3  [0.9999671, 4.578459e-10, 6.6002594e-12, 8.168...  \n",
       "4  [0.86532485, 3.0740804e-08, 3.727919e-08, 1.21...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(path + '/DatasetA_test_20180813/DatasetA_test/image.txt', header = None, names = ['img_id'])\n",
    "imag_path = path + '/DatasetA_test_20180813/DatasetA_test/test/'\n",
    "test_data['img'] = test_data['img_id'].apply(lambda id: read_image(imag_path, id))\n",
    "# with open(path + 'test_data.pickle', 'rb') as handle:\n",
    "#     test_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.12423625, 0.1190791, 0.7566846]\n",
       "1       [1.7260123e-13, 2.1865514e-08, 1.0]\n",
       "2       [0.2117803, 0.17989996, 0.60831976]\n",
       "3        [0.2420589, 0.17926142, 0.5786797]\n",
       "4    [0.041276723, 0.95284086, 0.005882429]\n",
       "dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(path + 'test_data.pickle', 'wb+') as handle:\n",
    "#     pickle.dump(test_data, handle)\n",
    "# test_data.head()\n",
    "with open(path + '/model_sub/stacking_train_label_2018_09_12_12_10_06.pickle', 'rb') as handle:\n",
    "    stacking_train_data = pickle.load(handle)\n",
    "stacking_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_17 (InputLayer)            (None, 64, 64, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D) (None, 70, 70, 3)     0           input_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)              (None, 68, 68, 64)    1728        zero_padding2d_3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)    (None, 68, 68, 64)    256         conv1/conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1/relu (Activation)          (None, 68, 68, 64)    0           conv1/bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D) (None, 70, 70, 64)    0           conv1/relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (AveragePooling2D)         (None, 34, 34, 64)    0           zero_padding2d_4[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormaliz (None, 34, 34, 64)    256         pool1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation) (None, 34, 34, 64)    0           conv2_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)     (None, 34, 34, 64)    4096        conv2_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenate (None, 34, 34, 80)    0           pool1[0][0]                      \n",
      "                                                                   conv2_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormaliz (None, 34, 34, 80)    320         conv2_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation) (None, 34, 34, 80)    0           conv2_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)     (None, 34, 34, 64)    5120        conv2_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenate (None, 34, 34, 96)    0           conv2_block1_concat[0][0]        \n",
      "                                                                   conv2_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormaliz (None, 34, 34, 96)    384         conv2_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation) (None, 34, 34, 96)    0           conv2_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)     (None, 34, 34, 64)    6144        conv2_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenate (None, 34, 34, 112)   0           conv2_block2_concat[0][0]        \n",
      "                                                                   conv2_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormaliz (None, 34, 34, 112)   448         conv2_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation) (None, 34, 34, 112)   0           conv2_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)     (None, 34, 34, 64)    7168        conv2_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenate (None, 34, 34, 128)   0           conv2_block3_concat[0][0]        \n",
      "                                                                   conv2_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormaliz (None, 34, 34, 128)   512         conv2_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation) (None, 34, 34, 128)   0           conv2_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)     (None, 34, 34, 64)    8192        conv2_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenate (None, 34, 34, 144)   0           conv2_block4_concat[0][0]        \n",
      "                                                                   conv2_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormaliz (None, 34, 34, 144)   576         conv2_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation) (None, 34, 34, 144)   0           conv2_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)     (None, 34, 34, 64)    9216        conv2_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormaliz (None, 34, 34, 64)    256         conv2_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation) (None, 34, 34, 64)    0           conv2_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)     (None, 34, 34, 16)    9216        conv2_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenate (None, 34, 34, 160)   0           conv2_block5_concat[0][0]        \n",
      "                                                                   conv2_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)    (None, 34, 34, 160)   640         conv2_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "pool2_relu (Activation)          (None, 34, 34, 160)   0           pool2_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)              (None, 34, 34, 80)    12800       pool2_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)    (None, 17, 17, 80)    0           pool2_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormaliz (None, 17, 17, 80)    320         pool2_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation) (None, 17, 17, 80)    0           conv3_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)     (None, 17, 17, 64)    5120        conv3_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenate (None, 17, 17, 96)    0           pool2_pool[0][0]                 \n",
      "                                                                   conv3_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormaliz (None, 17, 17, 96)    384         conv3_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation) (None, 17, 17, 96)    0           conv3_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)     (None, 17, 17, 64)    6144        conv3_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenate (None, 17, 17, 112)   0           conv3_block1_concat[0][0]        \n",
      "                                                                   conv3_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormaliz (None, 17, 17, 112)   448         conv3_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation) (None, 17, 17, 112)   0           conv3_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)     (None, 17, 17, 64)    7168        conv3_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenate (None, 17, 17, 128)   0           conv3_block2_concat[0][0]        \n",
      "                                                                   conv3_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormaliz (None, 17, 17, 128)   512         conv3_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation) (None, 17, 17, 128)   0           conv3_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)     (None, 17, 17, 64)    8192        conv3_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenate (None, 17, 17, 144)   0           conv3_block3_concat[0][0]        \n",
      "                                                                   conv3_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormaliz (None, 17, 17, 144)   576         conv3_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation) (None, 17, 17, 144)   0           conv3_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)     (None, 17, 17, 64)    9216        conv3_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenate (None, 17, 17, 160)   0           conv3_block4_concat[0][0]        \n",
      "                                                                   conv3_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormaliz (None, 17, 17, 160)   640         conv3_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation) (None, 17, 17, 160)   0           conv3_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)     (None, 17, 17, 64)    10240       conv3_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenate (None, 17, 17, 176)   0           conv3_block5_concat[0][0]        \n",
      "                                                                   conv3_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormaliz (None, 17, 17, 176)   704         conv3_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation) (None, 17, 17, 176)   0           conv3_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)     (None, 17, 17, 64)    11264       conv3_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenate (None, 17, 17, 192)   0           conv3_block6_concat[0][0]        \n",
      "                                                                   conv3_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormaliz (None, 17, 17, 192)   768         conv3_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation) (None, 17, 17, 192)   0           conv3_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)     (None, 17, 17, 64)    12288       conv3_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenate (None, 17, 17, 208)   0           conv3_block7_concat[0][0]        \n",
      "                                                                   conv3_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormaliz (None, 17, 17, 208)   832         conv3_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation) (None, 17, 17, 208)   0           conv3_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)     (None, 17, 17, 64)    13312       conv3_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormaliz (None, 17, 17, 64)    256         conv3_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation) (None, 17, 17, 64)    0           conv3_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)     (None, 17, 17, 16)    9216        conv3_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenate (None, 17, 17, 224)   0           conv3_block8_concat[0][0]        \n",
      "                                                                   conv3_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormali (None, 17, 17, 224)   896         conv3_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activation (None, 17, 17, 224)   0           conv3_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)    (None, 17, 17, 64)    14336       conv3_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activation (None, 17, 17, 64)    0           conv3_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatenat (None, 17, 17, 240)   0           conv3_block9_concat[0][0]        \n",
      "                                                                   conv3_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormali (None, 17, 17, 240)   960         conv3_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activation (None, 17, 17, 240)   0           conv3_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)    (None, 17, 17, 64)    15360       conv3_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activation (None, 17, 17, 64)    0           conv3_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatenat (None, 17, 17, 256)   0           conv3_block10_concat[0][0]       \n",
      "                                                                   conv3_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormali (None, 17, 17, 256)   1024        conv3_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activation (None, 17, 17, 256)   0           conv3_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)    (None, 17, 17, 64)    16384       conv3_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormali (None, 17, 17, 64)    256         conv3_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activation (None, 17, 17, 64)    0           conv3_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)    (None, 17, 17, 16)    9216        conv3_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatenat (None, 17, 17, 272)   0           conv3_block11_concat[0][0]       \n",
      "                                                                   conv3_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)    (None, 17, 17, 272)   1088        conv3_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool3_relu (Activation)          (None, 17, 17, 272)   0           pool3_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)              (None, 17, 17, 136)   36992       pool3_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)    (None, 8, 8, 136)     0           pool3_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormaliz (None, 8, 8, 136)     544         pool3_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation) (None, 8, 8, 136)     0           conv4_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)     (None, 8, 8, 64)      8704        conv4_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenate (None, 8, 8, 152)     0           pool3_pool[0][0]                 \n",
      "                                                                   conv4_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormaliz (None, 8, 8, 152)     608         conv4_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation) (None, 8, 8, 152)     0           conv4_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)     (None, 8, 8, 64)      9728        conv4_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenate (None, 8, 8, 168)     0           conv4_block1_concat[0][0]        \n",
      "                                                                   conv4_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormaliz (None, 8, 8, 168)     672         conv4_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation) (None, 8, 8, 168)     0           conv4_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)     (None, 8, 8, 64)      10752       conv4_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenate (None, 8, 8, 184)     0           conv4_block2_concat[0][0]        \n",
      "                                                                   conv4_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormaliz (None, 8, 8, 184)     736         conv4_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation) (None, 8, 8, 184)     0           conv4_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)     (None, 8, 8, 64)      11776       conv4_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenate (None, 8, 8, 200)     0           conv4_block3_concat[0][0]        \n",
      "                                                                   conv4_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormaliz (None, 8, 8, 200)     800         conv4_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation) (None, 8, 8, 200)     0           conv4_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)     (None, 8, 8, 64)      12800       conv4_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenate (None, 8, 8, 216)     0           conv4_block4_concat[0][0]        \n",
      "                                                                   conv4_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormaliz (None, 8, 8, 216)     864         conv4_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation) (None, 8, 8, 216)     0           conv4_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)     (None, 8, 8, 64)      13824       conv4_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenate (None, 8, 8, 232)     0           conv4_block5_concat[0][0]        \n",
      "                                                                   conv4_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormaliz (None, 8, 8, 232)     928         conv4_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation) (None, 8, 8, 232)     0           conv4_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)     (None, 8, 8, 64)      14848       conv4_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenate (None, 8, 8, 248)     0           conv4_block6_concat[0][0]        \n",
      "                                                                   conv4_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormaliz (None, 8, 8, 248)     992         conv4_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation) (None, 8, 8, 248)     0           conv4_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)     (None, 8, 8, 64)      15872       conv4_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenate (None, 8, 8, 264)     0           conv4_block7_concat[0][0]        \n",
      "                                                                   conv4_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormaliz (None, 8, 8, 264)     1056        conv4_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation) (None, 8, 8, 264)     0           conv4_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)     (None, 8, 8, 64)      16896       conv4_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormaliz (None, 8, 8, 64)      256         conv4_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation) (None, 8, 8, 64)      0           conv4_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)     (None, 8, 8, 16)      9216        conv4_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenate (None, 8, 8, 280)     0           conv4_block8_concat[0][0]        \n",
      "                                                                   conv4_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormali (None, 8, 8, 280)     1120        conv4_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activation (None, 8, 8, 280)     0           conv4_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)    (None, 8, 8, 64)      17920       conv4_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activation (None, 8, 8, 64)      0           conv4_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatenat (None, 8, 8, 296)     0           conv4_block9_concat[0][0]        \n",
      "                                                                   conv4_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormali (None, 8, 8, 296)     1184        conv4_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activation (None, 8, 8, 296)     0           conv4_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)    (None, 8, 8, 64)      18944       conv4_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activation (None, 8, 8, 64)      0           conv4_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatenat (None, 8, 8, 312)     0           conv4_block10_concat[0][0]       \n",
      "                                                                   conv4_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormali (None, 8, 8, 312)     1248        conv4_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activation (None, 8, 8, 312)     0           conv4_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)    (None, 8, 8, 64)      19968       conv4_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activation (None, 8, 8, 64)      0           conv4_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatenat (None, 8, 8, 328)     0           conv4_block11_concat[0][0]       \n",
      "                                                                   conv4_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormali (None, 8, 8, 328)     1312        conv4_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activation (None, 8, 8, 328)     0           conv4_block13_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)    (None, 8, 8, 64)      20992       conv4_block13_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block13_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activation (None, 8, 8, 64)      0           conv4_block13_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block13_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatenat (None, 8, 8, 344)     0           conv4_block12_concat[0][0]       \n",
      "                                                                   conv4_block13_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormali (None, 8, 8, 344)     1376        conv4_block13_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activation (None, 8, 8, 344)     0           conv4_block14_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)    (None, 8, 8, 64)      22016       conv4_block14_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block14_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activation (None, 8, 8, 64)      0           conv4_block14_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block14_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatenat (None, 8, 8, 360)     0           conv4_block13_concat[0][0]       \n",
      "                                                                   conv4_block14_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormali (None, 8, 8, 360)     1440        conv4_block14_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activation (None, 8, 8, 360)     0           conv4_block15_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)    (None, 8, 8, 64)      23040       conv4_block15_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block15_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activation (None, 8, 8, 64)      0           conv4_block15_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block15_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatenat (None, 8, 8, 376)     0           conv4_block14_concat[0][0]       \n",
      "                                                                   conv4_block15_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormali (None, 8, 8, 376)     1504        conv4_block15_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activation (None, 8, 8, 376)     0           conv4_block16_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)    (None, 8, 8, 64)      24064       conv4_block16_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block16_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activation (None, 8, 8, 64)      0           conv4_block16_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block16_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatenat (None, 8, 8, 392)     0           conv4_block15_concat[0][0]       \n",
      "                                                                   conv4_block16_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormali (None, 8, 8, 392)     1568        conv4_block16_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activation (None, 8, 8, 392)     0           conv4_block17_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)    (None, 8, 8, 64)      25088       conv4_block17_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block17_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activation (None, 8, 8, 64)      0           conv4_block17_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block17_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatenat (None, 8, 8, 408)     0           conv4_block16_concat[0][0]       \n",
      "                                                                   conv4_block17_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormali (None, 8, 8, 408)     1632        conv4_block17_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activation (None, 8, 8, 408)     0           conv4_block18_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)    (None, 8, 8, 64)      26112       conv4_block18_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block18_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activation (None, 8, 8, 64)      0           conv4_block18_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block18_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatenat (None, 8, 8, 424)     0           conv4_block17_concat[0][0]       \n",
      "                                                                   conv4_block18_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormali (None, 8, 8, 424)     1696        conv4_block18_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activation (None, 8, 8, 424)     0           conv4_block19_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)    (None, 8, 8, 64)      27136       conv4_block19_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block19_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activation (None, 8, 8, 64)      0           conv4_block19_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block19_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatenat (None, 8, 8, 440)     0           conv4_block18_concat[0][0]       \n",
      "                                                                   conv4_block19_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormali (None, 8, 8, 440)     1760        conv4_block19_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activation (None, 8, 8, 440)     0           conv4_block20_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)    (None, 8, 8, 64)      28160       conv4_block20_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block20_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activation (None, 8, 8, 64)      0           conv4_block20_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block20_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatenat (None, 8, 8, 456)     0           conv4_block19_concat[0][0]       \n",
      "                                                                   conv4_block20_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormali (None, 8, 8, 456)     1824        conv4_block20_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activation (None, 8, 8, 456)     0           conv4_block21_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)    (None, 8, 8, 64)      29184       conv4_block21_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block21_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activation (None, 8, 8, 64)      0           conv4_block21_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block21_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatenat (None, 8, 8, 472)     0           conv4_block20_concat[0][0]       \n",
      "                                                                   conv4_block21_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormali (None, 8, 8, 472)     1888        conv4_block21_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activation (None, 8, 8, 472)     0           conv4_block22_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)    (None, 8, 8, 64)      30208       conv4_block22_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block22_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activation (None, 8, 8, 64)      0           conv4_block22_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block22_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatenat (None, 8, 8, 488)     0           conv4_block21_concat[0][0]       \n",
      "                                                                   conv4_block22_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormali (None, 8, 8, 488)     1952        conv4_block22_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activation (None, 8, 8, 488)     0           conv4_block23_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)    (None, 8, 8, 64)      31232       conv4_block23_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block23_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activation (None, 8, 8, 64)      0           conv4_block23_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block23_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatenat (None, 8, 8, 504)     0           conv4_block22_concat[0][0]       \n",
      "                                                                   conv4_block23_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormali (None, 8, 8, 504)     2016        conv4_block23_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activation (None, 8, 8, 504)     0           conv4_block24_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)    (None, 8, 8, 64)      32256       conv4_block24_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormali (None, 8, 8, 64)      256         conv4_block24_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activation (None, 8, 8, 64)      0           conv4_block24_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)    (None, 8, 8, 16)      9216        conv4_block24_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatenat (None, 8, 8, 520)     0           conv4_block23_concat[0][0]       \n",
      "                                                                   conv4_block24_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)    (None, 8, 8, 520)     2080        conv4_block24_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "pool4_relu (Activation)          (None, 8, 8, 520)     0           pool4_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)              (None, 8, 8, 260)     135200      pool4_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)    (None, 4, 4, 260)     0           pool4_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormaliz (None, 4, 4, 260)     1040        pool4_pool[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation) (None, 4, 4, 260)     0           conv5_block1_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)     (None, 4, 4, 64)      16640       conv5_block1_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block1_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block1_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block1_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenate (None, 4, 4, 276)     0           pool4_pool[0][0]                 \n",
      "                                                                   conv5_block1_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormaliz (None, 4, 4, 276)     1104        conv5_block1_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation) (None, 4, 4, 276)     0           conv5_block2_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)     (None, 4, 4, 64)      17664       conv5_block2_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block2_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block2_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block2_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenate (None, 4, 4, 292)     0           conv5_block1_concat[0][0]        \n",
      "                                                                   conv5_block2_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormaliz (None, 4, 4, 292)     1168        conv5_block2_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation) (None, 4, 4, 292)     0           conv5_block3_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)     (None, 4, 4, 64)      18688       conv5_block3_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block3_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block3_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block3_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenate (None, 4, 4, 308)     0           conv5_block2_concat[0][0]        \n",
      "                                                                   conv5_block3_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormaliz (None, 4, 4, 308)     1232        conv5_block3_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation) (None, 4, 4, 308)     0           conv5_block4_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)     (None, 4, 4, 64)      19712       conv5_block4_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block4_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block4_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block4_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenate (None, 4, 4, 324)     0           conv5_block3_concat[0][0]        \n",
      "                                                                   conv5_block4_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormaliz (None, 4, 4, 324)     1296        conv5_block4_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation) (None, 4, 4, 324)     0           conv5_block5_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)     (None, 4, 4, 64)      20736       conv5_block5_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block5_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block5_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block5_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenate (None, 4, 4, 340)     0           conv5_block4_concat[0][0]        \n",
      "                                                                   conv5_block5_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormaliz (None, 4, 4, 340)     1360        conv5_block5_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation) (None, 4, 4, 340)     0           conv5_block6_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)     (None, 4, 4, 64)      21760       conv5_block6_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block6_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block6_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block6_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenate (None, 4, 4, 356)     0           conv5_block5_concat[0][0]        \n",
      "                                                                   conv5_block6_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormaliz (None, 4, 4, 356)     1424        conv5_block6_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation) (None, 4, 4, 356)     0           conv5_block7_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)     (None, 4, 4, 64)      22784       conv5_block7_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block7_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block7_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block7_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenate (None, 4, 4, 372)     0           conv5_block6_concat[0][0]        \n",
      "                                                                   conv5_block7_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormaliz (None, 4, 4, 372)     1488        conv5_block7_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation) (None, 4, 4, 372)     0           conv5_block8_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)     (None, 4, 4, 64)      23808       conv5_block8_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block8_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block8_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block8_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenate (None, 4, 4, 388)     0           conv5_block7_concat[0][0]        \n",
      "                                                                   conv5_block8_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormaliz (None, 4, 4, 388)     1552        conv5_block8_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation) (None, 4, 4, 388)     0           conv5_block9_0_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)     (None, 4, 4, 64)      24832       conv5_block9_0_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormaliz (None, 4, 4, 64)      256         conv5_block9_1_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation) (None, 4, 4, 64)      0           conv5_block9_1_bn[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)     (None, 4, 4, 16)      9216        conv5_block9_1_relu[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenate (None, 4, 4, 404)     0           conv5_block8_concat[0][0]        \n",
      "                                                                   conv5_block9_2_conv[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormali (None, 4, 4, 404)     1616        conv5_block9_concat[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activation (None, 4, 4, 404)     0           conv5_block10_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)    (None, 4, 4, 64)      25856       conv5_block10_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block10_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activation (None, 4, 4, 64)      0           conv5_block10_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block10_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatenat (None, 4, 4, 420)     0           conv5_block9_concat[0][0]        \n",
      "                                                                   conv5_block10_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormali (None, 4, 4, 420)     1680        conv5_block10_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activation (None, 4, 4, 420)     0           conv5_block11_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)    (None, 4, 4, 64)      26880       conv5_block11_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block11_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activation (None, 4, 4, 64)      0           conv5_block11_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block11_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatenat (None, 4, 4, 436)     0           conv5_block10_concat[0][0]       \n",
      "                                                                   conv5_block11_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormali (None, 4, 4, 436)     1744        conv5_block11_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activation (None, 4, 4, 436)     0           conv5_block12_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)    (None, 4, 4, 64)      27904       conv5_block12_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block12_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activation (None, 4, 4, 64)      0           conv5_block12_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block12_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatenat (None, 4, 4, 452)     0           conv5_block11_concat[0][0]       \n",
      "                                                                   conv5_block12_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormali (None, 4, 4, 452)     1808        conv5_block12_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activation (None, 4, 4, 452)     0           conv5_block13_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)    (None, 4, 4, 64)      28928       conv5_block13_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block13_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activation (None, 4, 4, 64)      0           conv5_block13_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block13_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatenat (None, 4, 4, 468)     0           conv5_block12_concat[0][0]       \n",
      "                                                                   conv5_block13_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormali (None, 4, 4, 468)     1872        conv5_block13_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activation (None, 4, 4, 468)     0           conv5_block14_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)    (None, 4, 4, 64)      29952       conv5_block14_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block14_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activation (None, 4, 4, 64)      0           conv5_block14_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block14_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatenat (None, 4, 4, 484)     0           conv5_block13_concat[0][0]       \n",
      "                                                                   conv5_block14_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormali (None, 4, 4, 484)     1936        conv5_block14_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activation (None, 4, 4, 484)     0           conv5_block15_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)    (None, 4, 4, 64)      30976       conv5_block15_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block15_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activation (None, 4, 4, 64)      0           conv5_block15_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block15_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatenat (None, 4, 4, 500)     0           conv5_block14_concat[0][0]       \n",
      "                                                                   conv5_block15_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormali (None, 4, 4, 500)     2000        conv5_block15_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activation (None, 4, 4, 500)     0           conv5_block16_0_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)    (None, 4, 4, 64)      32000       conv5_block16_0_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormali (None, 4, 4, 64)      256         conv5_block16_1_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activation (None, 4, 4, 64)      0           conv5_block16_1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)    (None, 4, 4, 16)      9216        conv5_block16_1_relu[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatenat (None, 4, 4, 516)     0           conv5_block15_concat[0][0]       \n",
      "                                                                   conv5_block16_2_conv[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "bn (BatchNormalization)          (None, 4, 4, 516)     2064        conv5_block16_concat[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "relu (Activation)                (None, 4, 4, 516)     0           bn[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2D (None, 516)           0           relu[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "fc (Dense)                       (None, 171)           88407       avg_pool[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 1,945,831\n",
      "Trainable params: 1,902,543\n",
      "Non-trainable params: 43,288\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"av...)`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "img_model = DenseNet(blocks = [6, 12, 24, 16], \n",
    "                                cat_max = 171,\n",
    "                                weight_decay = 1e-4, \n",
    "                                kernel_initializer = 'glorot_normal',\n",
    "                                reduction = 0.5, \n",
    "                                init_filters = 64, \n",
    "                                growth_rate = 16).model\n",
    "img_model.load_weights(path + 'model_sub/6_12_24_16_ini64_growth16_02962/model_0_2018_09_13_08_23_48.h5')\n",
    "img_model_flat = Model(input = img_model.input, output = img_model.get_layer(name = 'avg_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = setB_test_data\n",
    "# # test_img = extract_array_from_series(test_data['img'])\n",
    "# # test_img = vgg16.preprocess_input(test_img)\n",
    "# # test_img_feature_map = img_model_flat.predict(test_img, verbose = 1)\n",
    "\n",
    "# with open(path + '/model_sub/6_12_24_16_ini64_growth32_inistride1_augdata_05010/flat_test_re_2018_09_22_19_24_38.pickle', 'rb') as handle:\n",
    "#     test_img_feature_map = pickle.load(handle)\n",
    "train_id = train_data['class_id'].unique()\n",
    "test_img_feature_map = extract_array_from_series(test_data['target'])\n",
    "# class_id_emb_attr.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_partial_model = Model(inputs = zs_model.inputs[2:], outputs = zs_model.outputs[0])\n",
    "test_class_id_emb_attr = class_id_emb_attr #[~class_id_emb_attr['class_id'].isin(train_id)]\n",
    "# pred_nearest_class_id = find_nearest_class(test_class_id_emb_attr, zs_partial_model, test_data, test_img_feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_05011 = models_eval(models = zs_model_list, eval_df = test_data, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[~class_id_emb_attr['class_id'].isin(train_id)], \\\n",
    "            img_feature_map = test_img_feature_map)\n",
    "preds_05011 = np.asarray(preds_05011).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ed5117811f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds_05077\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "preds_05077.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_102\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_102\" during training.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_108\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_108\" during training.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_114\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_114\" during training.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_120\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_120\" during training.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: Output \"dense_126\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_126\" during training.\n"
     ]
    }
   ],
   "source": [
    "with open(path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/flat_test_re_2018_09_24_03_07_15.pickle', 'rb') as handle:\n",
    "    flat_test_re = pickle.load(handle)\n",
    "zs_model_05077_list = []\n",
    "for i in range(5):\n",
    "    zs_model = create_dnn(kernel_initializer = 'he_normal', img_flat_len = 1032)\n",
    "    zs_model_name = path + '/model_sub/6_12_24_16_ini128_growth32_inistride1_augdata_05077/zs_model_' + str(i) +'_2018_09_25_01_01_31.txt'\n",
    "    zs_model.load_weights(zs_model_name)\n",
    "    zs_model_05077_list.append((zs_model, 'DEM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-086b73470e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreds_05077\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzs_model_05077_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mcand_class_id_emb_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_id_emb_attr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mclass_id_emb_attr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mimg_feature_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_test_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds_05011\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_05077\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "preds_05077 = models_eval(models = zs_model_05077_list, eval_df = test_data, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[~class_id_emb_attr['class_id'].isin(train_id)], \\\n",
    "            img_feature_map = flat_test_re)\n",
    "preds_05011 = np.asarray(preds_05011).T\n",
    "preds = np.c_[preds_05011, preds_05077]\n",
    "print (preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_preds = []\n",
    "for single_img_vote in preds:\n",
    "    uniq_val, counts = np.unique(single_img_vote, return_counts = True)\n",
    "    vote_preds.append(uniq_val[np.argmax(counts)])\n",
    "vote_preds = np.asarray(vote_preds)\n",
    "print (vote_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ZJL272' 'ZJL224' 'ZJL242' 'ZJL243' 'ZJL272']\n",
      " ['ZJL243' 'ZJL288' 'ZJL239' 'ZJL243' 'ZJL239']\n",
      " ['ZJL255' 'ZJL255' 'ZJL255' 'ZJL255' 'ZJL253']\n",
      " ...\n",
      " ['ZJL286' 'ZJL259' 'ZJL249' 'ZJL259' 'ZJL287']\n",
      " ['ZJL270' 'ZJL253' 'ZJL253' 'ZJL253' 'ZJL253']\n",
      " ['ZJL270' 'ZJL288' 'ZJL288' 'ZJL288' 'ZJL253']]\n",
      "['ZJL272' 'ZJL239' 'ZJL255' ... 'ZJL259' 'ZJL253' 'ZJL288']\n"
     ]
    }
   ],
   "source": [
    "pred_nearest_class_id = multi_models_vote(models = zs_model_list, eval_df = test_data, \\\n",
    "            cand_class_id_emb_attr = class_id_emb_attr[~class_id_emb_attr['class_id'].isin(train_id)], \\\n",
    "            img_feature_map = test_img_feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_label = time.strftime('_%Y_%m_%d_%H_%M_%S', time.gmtime())\n",
    "sub_name = path + '/model_sub/sub' + time_label + '.txt'\n",
    "sub = pd.DataFrame(pred_nearest_class_id, index = test_data['img_id'])\n",
    "sub.to_csv(sub_name, header = False, sep = '\\t')\n",
    "# zs_model.save(path + 'zs_model' + time_label + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.444444444444445"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "152 * 200 / 3600"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
